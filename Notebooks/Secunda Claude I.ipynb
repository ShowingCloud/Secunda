{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade --quiet langchain-community paramiko pydantic","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T18:10:02.443063Z","iopub.execute_input":"2025-02-24T18:10:02.443515Z","iopub.status.idle":"2025-02-24T18:10:08.067272Z","shell.execute_reply.started":"2025-02-24T18:10:02.443480Z","shell.execute_reply":"2025-02-24T18:10:08.065732Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import paramiko\nimport io\nfrom kaggle_secrets import UserSecretsClient\n\nssh_comment = UserSecretsClient().get_secret(\"SSH_COMMENT\")\n\ndef generate_rsa_key(bits=4096, comment=ssh_comment, passphrase=None):\n    \"\"\"Generate a new RSA key pair and return it as a string\"\"\"\n    # Create new private key\n    key = paramiko.RSAKey.generate(bits)\n    \n    # Add comment\n    key.comment = comment\n    \n    # Return as string (unencrypted or encrypted based on passphrase)\n    key_file = io.StringIO()\n    key.write_private_key(key_file, password=passphrase)\n    private_key_str = key_file.getvalue()\n    \n    # Get the public key as well\n    public_key_str = f\"ssh-rsa {key.get_base64()} {comment}\"\n    \n    return private_key_str, public_key_str\n    \ntry:\n    public_key = UserSecretsClient().get_secret(\"SSH_PUBLIC_KEY\")\n    encrypted_private_key = UserSecretsClient().get_secret(\"SSH_PRIVATE_KEY_ENCRYPTED\")\nexcept:\n    # Generate an unencrypted key\n    #private_key, public_key = generate_rsa_key()\n    #print(\"Private key:\\n\", private_key)\n    #print(\"\\nPublic key:\\n\", public_key)\n    \n    # Generate an encrypted key\n    encrypted_private_key, public_key = generate_rsa_key(passphrase=ssh_comment)\n    encrypted_private_key = encrypted_private_key.replace(\"\\n\", \"\\\\n\")\n    print(\"\\nEncrypted private key:\\n\", encrypted_private_key)\n    print(\"\\nPublic key:\\n\", public_key)\n\nssh_hostname = UserSecretsClient().get_secret(\"SSH_HOSTNAME\")\nssh_username = UserSecretsClient().get_secret(\"SSH_USERNAME\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T18:10:08.069103Z","iopub.execute_input":"2025-02-24T18:10:08.069434Z","iopub.status.idle":"2025-02-24T18:10:08.642463Z","shell.execute_reply.started":"2025-02-24T18:10:08.069402Z","shell.execute_reply":"2025-02-24T18:10:08.641213Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from langchain.agents import Tool, AgentExecutor\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema import HumanMessage, AIMessage\nfrom typing import List, Dict, Optional, AsyncGenerator\nimport asyncio\nimport paramiko\nimport json\nfrom pydantic import BaseModel, Field\nfrom datetime import datetime\nimport re\n\nclass OutputChunk(BaseModel):\n    \"\"\"Represents a chunk of command output\"\"\"\n    content: str\n    timestamp: datetime\n    type: str = \"stdout\"  # stdout, stderr, or system\n    requires_attention: bool = False\n    pattern_matches: Dict[str, str] = Field(default_factory=dict)\n\nclass PatternMatcher:\n    \"\"\"Matches important patterns in command output\"\"\"\n    def __init__(self):\n        self.patterns = {\n            'error': r'error|exception|failed|fatal',\n            'prompt': r'\\[y/N\\]|\\[Y/n\\]|password:|continue\\?',\n            'progress': r'\\d+%|\\d+/\\d+',\n            'completion': r'(done|completed|finished|ready).*$',\n        }\n        self.compiled_patterns = {\n            k: re.compile(v, re.IGNORECASE) for k, v in self.patterns.items()\n        }\n\n    def analyze_chunk(self, text: str) -> Dict[str, str]:\n        matches = {}\n        for pattern_name, pattern in self.compiled_patterns.items():\n            if found := pattern.search(text):\n                matches[pattern_name] = found.group(0)\n        return matches\n\nclass StreamProcessor:\n    \"\"\"Processes command output streams and chunks them intelligently\"\"\"\n    def __init__(self, pattern_matcher: PatternMatcher):\n        self.pattern_matcher = pattern_matcher\n        self.buffer = \"\"\n        self.chunk_size = 1024\n        self.min_chunk_size = 100  # Minimum size to process\n\n    def should_chunk(self, text: str) -> bool:\n        \"\"\"Determine if we should create a new chunk based on content\"\"\"\n        if len(text) >= self.chunk_size:\n            return True\n        \n        patterns = self.pattern_matcher.analyze_chunk(text)\n        return bool(patterns)  # Chunk if we find any important patterns\n\n    async def process_stream(self, stream: AsyncGenerator[str, None]) -> AsyncGenerator[OutputChunk, None]:\n        async for data in stream:\n            self.buffer += data\n            \n            while self.buffer:\n                if len(self.buffer) < self.min_chunk_size and not self.should_chunk(self.buffer):\n                    break\n                    \n                chunk_size = min(len(self.buffer), self.chunk_size)\n                chunk_text = self.buffer[:chunk_size]\n                self.buffer = self.buffer[chunk_size:]\n                \n                patterns = self.pattern_matcher.analyze_chunk(chunk_text)\n                requires_attention = bool({'error', 'prompt'} & patterns.keys())\n                \n                yield OutputChunk(\n                    content=chunk_text,\n                    timestamp=datetime.now(),\n                    requires_attention=requires_attention,\n                    pattern_matches=patterns\n                )\n\nclass SSHConnection:\n    def __init__(self):\n        self.client = paramiko.SSHClient()\n        self.client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n\n        key_string = encrypted_private_key.replace(\"\\\\n\", \"\\n\")\n        private_key = paramiko.RSAKey.from_private_key(\n            io.StringIO(key_string),\n            password = ssh_comment)\n        self.client.connect(\n            hostname = ssh_hostname,\n            username = ssh_username,\n            pkey = private_key\n        )\n\n    async def stream_output(self, shell) -> AsyncGenerator[str, None]:\n        \"\"\"Stream output from shell with backpressure control\"\"\"\n        while True:\n            if shell.recv_ready():\n                data = shell.recv(4096).decode('utf-8')\n                if data:\n                    yield data\n            await asyncio.sleep(0.1)\n\n    async def execute_interactive(self, command: str) -> AsyncGenerator[OutputChunk, None]:\n        \"\"\"Execute command and stream output chunks\"\"\"\n        shell = self.client.invoke_shell()\n        processor = StreamProcessor(PatternMatcher())\n        \n        # Send command\n        shell.send(command + '\\n')\n        \n        # Process output stream\n        async for chunk in processor.process_stream(self.stream_output(shell)):\n            yield chunk\n\nclass LinuxCommandTool(Tool):\n    def __init__(self, ssh_connection: SSHConnection):\n        self.ssh = ssh_connection\n        super().__init__(\n            name=\"linux_command\",\n            description=\"Execute Linux commands and handle interactive output\",\n            func=self.run\n        )\n    \n    async def run(self, command: str) -> AsyncGenerator[OutputChunk, None]:\n        async for chunk in self.ssh.execute_interactive(command):\n            yield chunk\n\nclass AgentResponse(BaseModel):\n    \"\"\"Structured response from the agent\"\"\"\n    action: str  # \"continue\", \"interact\", \"alert\", \"complete\"\n    response: str\n    reasoning: str\n    priority: int = 0\n\nclass StreamingAgent:\n    def __init__(self, llm, tools: List[Tool]):\n        self.llm = llm\n        self.tools = tools\n        self.memory = ConversationBufferMemory(max_history=10)\n        self.current_context = []\n        \n    def create_prompt(self, chunks: List[OutputChunk]) -> str:\n        \"\"\"Create prompt for LLM based on recent chunks\"\"\"\n        return f\"\"\"Analyze this command output stream and determine appropriate action:\n\nRecent output:\n{chunks[-5:]}  # Show last 5 chunks\n\nPatterns detected:\n{[chunk.pattern_matches for chunk in chunks[-5:]]}\n\nBased on this output:\n1. If you see a prompt/question, provide the appropriate response\n2. If you detect an error, provide guidance\n3. If the command is progressing normally, return \"continue\"\n4. If the command has completed, provide a summary\n\nRespond in JSON format:\n{{\n    \"action\": \"continue/interact/alert/complete\",\n    \"response\": \"your response or next action\",\n    \"reasoning\": \"your analysis\",\n    \"priority\": 0-10  # Urgency of response\n}}\"\"\"\n\n    async def process_chunks(self, chunks: List[OutputChunk]) -> Optional[AgentResponse]:\n        \"\"\"Process chunks and determine if LLM analysis is needed\"\"\"\n        # Quick pattern-based analysis first\n        if any(chunk.requires_attention for chunk in chunks):\n            # Important patterns detected, consult LLM\n            prompt = self.create_prompt(chunks)\n            response = await self.llm.apredict(prompt)\n            return AgentResponse(**json.loads(response))\n        \n        # For normal output, accumulate more before consulting LLM\n        if len(self.current_context) >= 5:  # Batch size\n            prompt = self.create_prompt(self.current_context)\n            response = await self.llm.apredict(prompt)\n            self.current_context.clear()\n            return AgentResponse(**json.loads(response))\n        \n        return None\n\n    async def run(self, command: str):\n        \"\"\"Run command and process output stream\"\"\"\n        tool = self.tools[0]  # Assume Linux command tool\n        \n        async for chunk in tool.run(command):\n            self.current_context.append(chunk)\n            \n            if response := await self.process_chunks([chunk]):\n                if response.action != \"continue\":\n                    # Handle interactive needs or alerts\n                    if response.action == \"interact\":\n                        # Send response back to command\n                        await tool.run(response.response)\n                    \n                    # Update memory\n                    self.memory.save_context(\n                        {\"input\": command},\n                        {\"output\": response.reasoning}\n                    )\n                    \n                    yield response\n\n# Example usage\nasync def main():\n    ssh = SSHConnection()\n    \n    tools = [LinuxCommandTool(ssh)]\n    llm = ChatOpenAI(temperature=0, openai_api_key=UserSecretsClient().get_secret(\"OPENAI_API_KEY\"))\n    agent = StreamingAgent(llm, tools)\n    \n    async for response in agent.run(\"sudo apt upgrade\"):\n        if response.action != \"continue\":\n            print(f\"Agent response: {response.dict()}\")\n\nif __name__ == \"__main__\":\n    await main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-24T18:12:29.044104Z","iopub.execute_input":"2025-02-24T18:12:29.044475Z","iopub.status.idle":"2025-02-24T18:12:30.090278Z","shell.execute_reply.started":"2025-02-24T18:12:29.044445Z","shell.execute_reply":"2025-02-24T18:12:30.088735Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-4ed5bdcdcda2>\u001b[0m in \u001b[0;36m<cell line: 218>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0;32mawait\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-13-4ed5bdcdcda2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStreamingAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sudo apt upgrade\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"continue\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Agent response: {response.dict()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-4ed5bdcdcda2>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mtool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Assume Linux command tool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-4ed5bdcdcda2>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAsyncGenerator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutputChunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ssh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_interactive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: type object 'object' has no attribute '__getattr__'"],"ename":"AttributeError","evalue":"type object 'object' has no attribute '__getattr__'","output_type":"error"}],"execution_count":13}]}