{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from typing import Dict, List, Optional, Set, Any\nfrom pydantic import BaseModel\nfrom enum import Enum\nimport asyncio\nfrom datetime import datetime\nimport json\nfrom collections import deque\n\nclass ThoughtType(Enum):\n    OBSERVATION = \"observation\"\n    ANALYSIS = \"analysis\"\n    DECISION = \"decision\"\n    LEARNING = \"learning\"\n    PLAN = \"plan\"\n    REFLECTION = \"reflection\"\n\nclass Thought(BaseModel):\n    \"\"\"Internal thought representation\"\"\"\n    type: ThoughtType\n    content: str\n    confidence: float\n    timestamp: datetime\n    context_hash: str  # Minimal context reference\n    related_thoughts: List[str] = []  # References to related thought IDs\n    \n    def to_context(self) -> Dict:\n        \"\"\"Convert thought to minimal context\"\"\"\n        return {\n            \"type\": self.type.value,\n            \"summary\": self._summarize_content(),\n            \"confidence\": self.confidence\n        }\n\nclass MemoryBank:\n    \"\"\"Efficient memory storage and retrieval system\"\"\"\n    def __init__(self, max_size: int = 10000):\n        self.thoughts: Dict[str, Thought] = {}\n        self.patterns: Dict[str, List[str]] = {}  # Pattern to thought ID mapping\n        self.recent_thoughts = deque(maxlen=100)\n        self.important_thoughts = set()\n        \n    async def add_thought(self, thought: Thought) -> str:\n        \"\"\"Store new thought and return its ID\"\"\"\n        thought_id = f\"{thought.type.value}_{datetime.now().timestamp()}\"\n        self.thoughts[thought_id] = thought\n        self.recent_thoughts.append(thought_id)\n        \n        # Extract and store patterns\n        patterns = self._extract_patterns(thought.content)\n        for pattern in patterns:\n            if pattern not in self.patterns:\n                self.patterns[pattern] = []\n            self.patterns[pattern].append(thought_id)\n            \n        return thought_id\n        \n    async def retrieve_relevant_thoughts(self, context: Dict) -> List[Thought]:\n        \"\"\"Get relevant thoughts based on context\"\"\"\n        patterns = self._extract_patterns(str(context))\n        relevant_thought_ids = set()\n        \n        for pattern in patterns:\n            if pattern in self.patterns:\n                relevant_thought_ids.update(self.patterns[pattern])\n                \n        return [self.thoughts[tid] for tid in relevant_thought_ids]\n\nclass ThoughtProcessor:\n    \"\"\"Processes and generates thoughts\"\"\"\n    def __init__(self):\n        self.current_focus: Optional[str] = None\n        self.active_patterns: Set[str] = set()\n        \n    async def generate_thought(self, \n                             observation: Dict,\n                             memory: MemoryBank) -> Thought:\n        \"\"\"Generate new thought based on observation and memory\"\"\"\n        # Get relevant past thoughts\n        relevant_thoughts = await memory.retrieve_relevant_thoughts(observation)\n        \n        # Analyze observation\n        analysis = await self._analyze_observation(observation, relevant_thoughts)\n        \n        # Generate new thought\n        return Thought(\n            type=self._determine_thought_type(analysis),\n            content=self._generate_thought_content(analysis),\n            confidence=self._calculate_confidence(analysis),\n            timestamp=datetime.now(),\n            context_hash=self._generate_context_hash(observation),\n            related_thoughts=[t.context_hash for t in relevant_thoughts]\n        )\n\nclass AutonomousProgrammer:\n    \"\"\"Autonomous programmer agent with internal thought process\"\"\"\n    def __init__(self, expertise: List[str]):\n        self.expertise = set(expertise)\n        self.memory = MemoryBank()\n        self.thought_processor = ThoughtProcessor()\n        self.execution_log = []\n        self.learning_progress = {}\n        \n    async def process_command_output(self, \n                                   command: str, \n                                   output: str,\n                                   system_state: Dict) -> Dict:\n        \"\"\"Process command output with internal thoughts\"\"\"\n        # Create minimal observation\n        observation = {\n            \"command\": command,\n            \"output_summary\": self._summarize_output(output),\n            \"state_changes\": self._detect_state_changes(system_state)\n        }\n        \n        # Generate thought\n        thought = await self.thought_processor.generate_thought(\n            observation,\n            self.memory\n        )\n        \n        # Store thought\n        thought_id = await self.memory.add_thought(thought)\n        \n        # Generate minimal context for sharing\n        return {\n            \"thought_summary\": thought.to_context(),\n            \"relevant_patterns\": list(self.thought_processor.active_patterns),\n            \"confidence\": thought.confidence\n        }\n    \n    async def learn_from_execution(self, execution_result: Dict) -> None:\n        \"\"\"Learn from execution with internal reflection\"\"\"\n        reflection = await self.thought_processor.generate_thought(\n            execution_result,\n            self.memory\n        )\n        await self.memory.add_thought(reflection)\n        \n        if reflection.confidence > 0.8:\n            self._update_learning_progress(reflection)\n\nclass AutoSpecialist:\n    \"\"\"Autonomous specialist with focused expertise\"\"\"\n    def __init__(self, specialty: str):\n        self.specialty = specialty\n        self.memory = MemoryBank()\n        self.thought_processor = ThoughtProcessor()\n        self.specialty_patterns = set()\n        \n    async def process_specialty_aspect(self, context: Dict) -> Dict:\n        \"\"\"Process specialty-specific aspect with internal thoughts\"\"\"\n        # Generate specialty-focused thought\n        thought = await self.thought_processor.generate_thought(\n            context,\n            self.memory\n        )\n        \n        # Store thought\n        await self.memory.add_thought(thought)\n        \n        # Update specialty patterns\n        self._update_patterns(thought)\n        \n        # Return minimal context\n        return {\n            \"specialty\": self.specialty,\n            \"insights\": thought.to_context(),\n            \"patterns\": list(self.specialty_patterns)\n        }\n\nclass LightweightContext(BaseModel):\n    \"\"\"Minimal context for inter-agent communication\"\"\"\n    source_agent: str\n    timestamp: datetime\n    summary: str\n    confidence: float\n    patterns: List[str]\n    \n    def to_dict(self) -> Dict:\n        \"\"\"Convert to dictionary for transmission\"\"\"\n        return {\n            \"agent\": self.source_agent,\n            \"summary\": self.summary,\n            \"confidence\": self.confidence,\n            \"patterns\": self.patterns\n        }\n\nclass ContextManager:\n    \"\"\"Manages lightweight context exchange\"\"\"\n    def __init__(self):\n        self.active_contexts: Dict[str, LightweightContext] = {}\n        self.pattern_subscriptions: Dict[str, List[str]] = {}\n        \n    async def share_context(self, \n                          context: LightweightContext,\n                          target_agents: List[str]) -> None:\n        \"\"\"Share minimal context with target agents\"\"\"\n        context_dict = context.to_dict()\n        \n        # Share only with relevant agents\n        for agent in target_agents:\n            if self._is_relevant_for_agent(context, agent):\n                await self._deliver_context(context_dict, agent)\n\nclass ExecutionMonitor:\n    \"\"\"Monitors command execution and system state\"\"\"\n    def __init__(self):\n        self.active_commands = {}\n        self.state_snapshots = deque(maxlen=100)\n        \n    async def monitor_execution(self, \n                              command: str,\n                              system_state: Dict) -> Dict:\n        \"\"\"Monitor command execution and return minimal context\"\"\"\n        # Track command execution\n        execution_id = f\"exec_{datetime.now().timestamp()}\"\n        self.active_commands[execution_id] = {\n            \"command\": command,\n            \"start_time\": datetime.now(),\n            \"state_snapshot\": system_state\n        }\n        \n        # Monitor and return minimal info\n        return {\n            \"execution_id\": execution_id,\n            \"state_changes\": self._detect_significant_changes(system_state)\n        }\n\n# Example usage\nasync def main():\n    # Create autonomous programmer\n    programmer = AutonomousProgrammer([\"linux\", \"python\"])\n    \n    # Create specialists\n    security_specialist = AutoSpecialist(\"security\")\n    performance_specialist = AutoSpecialist(\"performance\")\n    \n    # Create context manager\n    context_manager = ContextManager()\n    \n    # Monitor command execution\n    monitor = ExecutionMonitor()\n    \n    # Process command output\n    command = \"ls -la /var/log\"\n    output = \"... command output ...\"\n    system_state = {\"cpu\": 0.5, \"memory\": 0.7}\n    \n    # Get monitoring info\n    monitoring_context = await monitor.monitor_execution(command, system_state)\n    \n    # Process with programmer\n    programmer_context = await programmer.process_command_output(\n        command,\n        output,\n        system_state\n    )\n    \n    # Share minimal context\n    context = LightweightContext(\n        source_agent=\"programmer_1\",\n        timestamp=datetime.now(),\n        summary=programmer_context[\"thought_summary\"][\"summary\"],\n        confidence=programmer_context[\"thought_summary\"][\"confidence\"],\n        patterns=programmer_context[\"relevant_patterns\"]\n    )\n    \n    await context_manager.share_context(\n        context,\n        [\"security_specialist\", \"performance_specialist\"]\n    )","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}