{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom typing import Dict, Any, List, Optional, Union\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nimport asyncio\nfrom pathlib import Path\nimport logging\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(\"llm_deployment\")\n\nclass LLMManager:\n    \"\"\"Manages loading and inference with small LLMs for agents\"\"\"\n    \n    def __init__(self, cache_dir: Optional[str] = None):\n        \"\"\"Initialize the LLM manager\n        \n        Args:\n            cache_dir: Directory to cache downloaded models\n        \"\"\"\n        self.cache_dir = cache_dir\n        self.loaded_models = {}\n        self.model_info = {\n            \"phi-3-mini\": {\n                \"model_id\": \"microsoft/phi-3-mini\",\n                \"revision\": \"main\",\n                \"requires_gpu\": False,\n                \"quantization\": \"4bit\",  # Options: None, \"4bit\", \"8bit\"\n                \"context_length\": 4096\n            },\n            \"gemma-2b\": {\n                \"model_id\": \"google/gemma-2b\",\n                \"revision\": \"main\",\n                \"requires_gpu\": False,\n                \"quantization\": \"4bit\",\n                \"context_length\": 8192\n            },\n            \"tinyllama-1.1b\": {\n                \"model_id\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n                \"revision\": \"main\",\n                \"requires_gpu\": False,\n                \"quantization\": \"4bit\",\n                \"context_length\": 2048\n            },\n            \"mistral-7b\": {\n                \"model_id\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n                \"revision\": \"main\",\n                \"requires_gpu\": True,\n                \"quantization\": \"4bit\",\n                \"context_length\": 8192\n            }\n        }\n        \n        # Check if CUDA is available\n        self.cuda_available = torch.cuda.is_available()\n        if self.cuda_available:\n            logger.info(f\"CUDA is available: {torch.cuda.get_device_name(0)}\")\n        else:\n            logger.info(\"CUDA is not available, will use CPU\")\n    \n    async def load_model(self, model_name: str, device: Optional[str] = None) -> str:\n        \"\"\"Load a model asynchronously\n        \n        Args:\n            model_name: Name of the model to load (must be in model_info)\n            device: Device to load the model on (\"cuda\", \"cpu\", etc.)\n                    If None, will use CUDA if available and model requires it\n        \n        Returns:\n            model_key: Unique key for the loaded model\n        \"\"\"\n        if model_name not in self.model_info:\n            raise ValueError(f\"Unknown model: {model_name}. Available models: {list(self.model_info.keys())}\")\n        \n        # Determine device\n        if device is None:\n            if self.model_info[model_name][\"requires_gpu\"] and not self.cuda_available:\n                raise ValueError(f\"Model {model_name} requires GPU but CUDA is not available\")\n            device = \"cuda\" if self.cuda_available and self.model_info[model_name][\"requires_gpu\"] else \"cpu\"\n        \n        # Create unique model key\n        model_key = f\"{model_name}_{device}\"\n        \n        # Check if model is already loaded\n        if model_key in self.loaded_models:\n            logger.info(f\"Model {model_name} already loaded on {device}\")\n            return model_key\n        \n        # Get model info\n        model_info = self.model_info[model_name]\n        model_id = model_info[\"model_id\"]\n        revision = model_info[\"revision\"]\n        quantization = model_info[\"quantization\"]\n        \n        logger.info(f\"Loading model {model_name} on {device}...\")\n        \n        # Create a loading task\n        def load_model_task():\n            try:\n                # Load tokenizer\n                tokenizer = AutoTokenizer.from_pretrained(\n                    model_id,\n                    revision=revision,\n                    cache_dir=self.cache_dir\n                )\n                \n                # Load model with appropriate quantization\n                if quantization == \"4bit\":\n                    model = AutoModelForCausalLM.from_pretrained(\n                        model_id,\n                        revision=revision,\n                        cache_dir=self.cache_dir,\n                        device_map=device,\n                        load_in_4bit=True,\n                        trust_remote_code=True\n                    )\n                elif quantization == \"8bit\":\n                    model = AutoModelForCausalLM.from_pretrained(\n                        model_id,\n                        revision=revision,\n                        cache_dir=self.cache_dir,\n                        device_map=device,\n                        load_in_8bit=True,\n                        trust_remote_code=True\n                    )\n                else:\n                    model = AutoModelForCausalLM.from_pretrained(\n                        model_id,\n                        revision=revision,\n                        cache_dir=self.cache_dir,\n                        device_map=device,\n                        trust_remote_code=True\n                    )\n                \n                # Create pipeline\n                pipe = pipeline(\n                    \"text-generation\",\n                    model=model,\n                    tokenizer=tokenizer,\n                    device_map=device,\n                    max_new_tokens=512,\n                    trust_remote_code=True\n                )\n                \n                return {\n                    \"model\": model,\n                    \"tokenizer\": tokenizer,\n                    \"pipeline\": pipe,\n                    \"info\": model_info\n                }\n            except Exception as e:\n                logger.error(f\"Error loading model {model_name}: {e}\")\n                raise\n        \n        # Run the loading task in a separate thread to not block the event loop\n        loop = asyncio.get_event_loop()\n        model_data = await loop.run_in_executor(None, load_model_task)\n        \n        # Store the loaded model\n        self.loaded_models[model_key] = model_data\n        \n        logger.info(f\"Model {model_name} loaded successfully on {device}\")\n        return model_key\n    \n    async def unload_model(self, model_key: str) -> bool:\n        \"\"\"Unload a model to free memory\n        \n        Args:\n            model_key: Key of the model to unload\n            \n        Returns:\n            bool: True if model was unloaded, False if it wasn't loaded\n        \"\"\"\n        if model_key not in self.loaded_models:\n            logger.warning(f\"Model {model_key} is not loaded\")\n            return False\n        \n        # Get model data\n        model_data = self.loaded_models[model_key]\n        \n        # Unload model\n        model_data[\"model\"] = None\n        model_data[\"tokenizer\"] = None\n        model_data[\"pipeline\"] = None\n        \n        # Remove from loaded models\n        del self.loaded_models[model_key]\n        \n        # Force garbage collection\n        import gc\n        gc.collect()\n        \n        if self.cuda_available:\n            torch.cuda.empty_cache()\n        \n        logger.info(f\"Model {model_key} unloaded successfully\")\n        return True\n    \n    async def generate(\n        self, \n        model_key: str, \n        prompt: str, \n        max_new_tokens: int = 512,\n        temperature: float = 0.7,\n        top_p: float = 0.9,\n        top_k: int = 50,\n        repetition_penalty: float = 1.1,\n        **kwargs\n    ) -> str:\n        \"\"\"Generate text from the model\n        \n        Args:\n            model_key: Key of the model to use\n            prompt: Input prompt\n            max_new_tokens: Maximum number of tokens to generate\n            temperature: Sampling temperature\n            top_p: Nucleus sampling parameter\n            top_k: Top-k sampling parameter\n            repetition_penalty: Penalty for repeating tokens\n            **kwargs: Additional arguments to pass to the generation function\n            \n        Returns:\n            str: Generated text\n        \"\"\"\n        if model_key not in self.loaded_models:\n            raise ValueError(f\"Model {model_key} is not loaded\")\n        \n        # Get model data\n        model_data = self.loaded_models[model_key]\n        pipe = model_data[\"pipeline\"]\n        \n        # Create a generation task\n        def generate_task():\n            try:\n                outputs = pipe(\n                    prompt,\n                    max_new_tokens=max_new_tokens,\n                    temperature=temperature,\n                    top_p=top_p,\n                    top_k=top_k,\n                    repetition_penalty=repetition_penalty,\n                    do_sample=True,\n                    **kwargs\n                )\n                \n                # Extract generated text\n                generated_text = outputs[0][\"generated_text\"]\n                \n                # Remove the prompt from the generated text\n                if generated_text.startswith(prompt):\n                    generated_text = generated_text[len(prompt):]\n                \n                return generated_text.strip()\n            except Exception as e:\n                logger.error(f\"Error generating text: {e}\")\n                raise\n        \n        # Run the generation task in a separate thread to not block the event loop\n        loop = asyncio.get_event_loop()\n        result = await loop.run_in_executor(None, generate_task)\n        \n        return result\n    \n    def get_available_models(self) -> List[str]:\n        \"\"\"Get a list of available models\n        \n        Returns:\n            List[str]: List of available model names\n        \"\"\"\n        return list(self.model_info.keys())\n    \n    def get_loaded_models(self) -> Dict[str, str]:\n        \"\"\"Get","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}