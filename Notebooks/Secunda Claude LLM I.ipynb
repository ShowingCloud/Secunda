{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade --quiet transformers torch torchvision PyPDF2 bs4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T18:23:29.191149Z","iopub.execute_input":"2025-03-01T18:23:29.191437Z","iopub.status.idle":"2025-03-01T18:23:33.063825Z","shell.execute_reply.started":"2025-03-01T18:23:29.191416Z","shell.execute_reply":"2025-03-01T18:23:33.062626Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nos.environ[\"HF_TOKEN\"] = user_secrets.get_secret(\"HF_TOKEN\")\nos.environ[\"HF_USERNAME\"] = \"wgqme\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-01T18:23:33.065436Z","iopub.execute_input":"2025-03-01T18:23:33.065814Z","iopub.status.idle":"2025-03-01T18:23:33.180317Z","shell.execute_reply.started":"2025-03-01T18:23:33.065779Z","shell.execute_reply":"2025-03-01T18:23:33.179519Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"\"\"\"\nLocal LLM Deployment for Multi-Agent System\n- Implements lightweight agents using Phi-3-mini or similar locally deployed LLMs\n- Supports both Project Manager and Programmer agent roles\n- Integrates with knowledge bases (PDF manuals or web pages)\n\"\"\"\n\nimport os\nimport json\nimport asyncio\nimport logging\nfrom typing import Dict, List, Optional, Union, Any, Tuple\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nimport PyPDF2\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Agent types and roles\nclass AgentRole(str, Enum):\n    PROJECT_MANAGER = \"project_manager\"\n    PROGRAMMER = \"programmer\"\n    SPECIALIST = \"specialist\"\n\nclass ExpertiseArea(str, Enum):\n    PYTHON = \"python\"\n    JAVASCRIPT = \"javascript\"\n    DEVOPS = \"devops\"\n    DATABASE = \"database\"\n    FRONTEND = \"frontend\"\n    BACKEND = \"backend\"\n    \nclass MessageType(str, Enum):\n    TASK_ASSIGNMENT = \"task_assignment\"\n    STATUS_UPDATE = \"status_update\"\n    TECHNICAL_QUERY = \"technical_query\"\n    REVIEW_REQUEST = \"review_request\"\n    LEARNING_UPDATE = \"learning_update\"\n\n@dataclass\nclass Task:\n    id: str\n    title: str\n    description: str\n    required_expertise: List[ExpertiseArea]\n    priority: int = 1\n    status: str = \"pending\"\n    assigned_to: Optional[str] = None\n    \n@dataclass\nclass Message:\n    sender: str\n    recipient: str\n    message_type: MessageType\n    content: Dict[str, Any]\n    timestamp: float = field(default_factory=lambda: asyncio.get_event_loop().time())\n\n@dataclass\nclass Thought:\n    \"\"\"Internal thought representation for agent's reasoning process\"\"\"\n    thought_type: str  # Observation, Analysis, Decision, Learning\n    content: str\n    confidence: float\n    related_command: Optional[str] = None\n    timestamp: float = field(default_factory=lambda: asyncio.get_event_loop().time())\n    \n    def to_dict(self):\n        return {\n            \"thought_type\": self.thought_type,\n            \"content\": self.content,\n            \"confidence\": self.confidence,\n            \"related_command\": self.related_command,\n            \"timestamp\": self.timestamp\n        }\n\nclass LocalLLMManager:\n    \"\"\"Manages loading and inference for local LLMs\"\"\"\n    \n    def __init__(self, model_name: str = \"microsoft/Phi-4-mini-instruct\", device: str = None):\n        \"\"\"\n        Initialize the LLM manager\n        \n        Args:\n            model_name: HuggingFace model identifier\n            device: Device to run the model on (None for auto-detection)\n        \"\"\"\n        self.model_name = model_name\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        logger.info(f\"Initializing LocalLLMManager with {model_name} on {self.device}\")\n        \n        # Load model and tokenizer\n        self.tokenizer = None\n        self.model = None\n        self.pipe = None\n        \n    async def load_model(self):\n        \"\"\"Load the model and tokenizer asynchronously\"\"\"\n        # Run in a separate thread to avoid blocking\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, self._load_model_sync)\n        logger.info(f\"Model {self.model_name} loaded successfully\")\n    \n    def _load_model_sync(self):\n        \"\"\"Synchronous model loading function\"\"\"\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.model_name,\n            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n            device_map=self.device\n        )\n        self.pipe = pipeline(\n            \"text-generation\",\n            model=self.model,\n            tokenizer=self.tokenizer,\n            #device=0 if self.device == \"cuda\" else -1\n        )\n    \n    async def generate(self, prompt: str, max_tokens: int = 512, \n                      temperature: float = 0.7) -> str:\n        \"\"\"\n        Generate text using the loaded model\n        \n        Args:\n            prompt: Input prompt for the model\n            max_tokens: Maximum number of tokens to generate\n            temperature: Temperature for sampling\n            \n        Returns:\n            Generated text response\n        \"\"\"\n        if not self.model or not self.tokenizer:\n            await self.load_model()\n            \n        # Run in a separate thread to avoid blocking\n        loop = asyncio.get_event_loop()\n        response = await loop.run_in_executor(\n            None, \n            lambda: self.pipe(\n                prompt,\n                max_new_tokens=max_tokens,\n                temperature=temperature,\n                do_sample=True,\n                top_p=0.95,\n            )[0][\"generated_text\"]\n        )\n        \n        # Extract only the newly generated text\n        return response[len(prompt):].strip()\n\n    async def generate_thought(self, \n                             agent_role: AgentRole,\n                             thought_type: str,\n                             context: str,\n                             confidence_threshold: float = 0.7) -> Optional[Thought]:\n        \"\"\"\n        Generate an internal thought with confidence estimation\n        \n        Args:\n            agent_role: Role of the agent generating the thought\n            thought_type: Type of thought to generate\n            context: Context information for thought generation\n            confidence_threshold: Minimum confidence threshold\n            \n        Returns:\n            A Thought object or None if confidence is too low\n        \"\"\"\n        # Create a prompt for thought generation with confidence estimation\n        prompt = f\"\"\"\n        You are a {agent_role.value} agent generating internal thoughts.\n        \n        Context:\n        {context}\n        \n        Generate a {thought_type} thought about this situation.\n        Also estimate your confidence in this thought from 0.0 to 1.0.\n        \n        Format your response as:\n        Thought: [your thought here]\n        Confidence: [confidence score between 0.0 and 1.0]\n        \"\"\"\n        \n        response = await self.generate(prompt, max_tokens=200, temperature=0.7)\n        \n        # Parse the response\n        thought_content = \"\"\n        confidence = 0.0\n        \n        for line in response.split('\\n'):\n            if line.startswith(\"Thought:\"):\n                thought_content = line[len(\"Thought:\"):].strip()\n            elif line.startswith(\"Confidence:\"):\n                try:\n                    confidence = float(line[len(\"Confidence:\"):].strip())\n                except ValueError:\n                    confidence = 0.0\n        \n        # Create and return thought if confidence is above threshold\n        if confidence >= confidence_threshold and thought_content:\n            return Thought(\n                thought_type=thought_type,\n                content=thought_content,\n                confidence=confidence\n            )\n        return None\n\nclass KnowledgeBase:\n    \"\"\"Manages loading and retrieval of knowledge from manuals\"\"\"\n    \n    def __init__(self):\n        self.pdf_contents = {}\n        self.web_contents = {}\n    \n    async def load_pdf(self, name: str, file_path: str):\n        \"\"\"Load content from a PDF file\"\"\"\n        loop = asyncio.get_event_loop()\n        content = await loop.run_in_executor(None, self._read_pdf, file_path)\n        self.pdf_contents[name] = content\n        logger.info(f\"Loaded PDF knowledge base: {name}\")\n    \n    def _read_pdf(self, file_path: str) -> str:\n        \"\"\"Read content from a PDF file synchronously\"\"\"\n        text = \"\"\n        with open(file_path, 'rb') as file:\n            pdf_reader = PyPDF2.PdfReader(file)\n            for page in pdf_reader.pages:\n                text += page.extract_text() + \"\\n\"\n        return text\n    \n    async def load_webpage(self, name: str, url: str):\n        \"\"\"Load content from a webpage\"\"\"\n        loop = asyncio.get_event_loop()\n        content = await loop.run_in_executor(None, self._read_webpage, url)\n        self.web_contents[name] = content\n        logger.info(f\"Loaded web knowledge base: {name}\")\n    \n    def _read_webpage(self, url: str) -> str:\n        \"\"\"Read content from a webpage synchronously\"\"\"\n        response = requests.get(url)\n        soup = BeautifulSoup(response.text, 'html.parser')\n        \n        # Remove script and style elements\n        for script in soup([\"script\", \"style\"]):\n            script.extract()\n            \n        # Get text\n        text = soup.get_text()\n        \n        # Break into lines and remove leading and trailing space on each\n        lines = (line.strip() for line in text.splitlines())\n        # Break multi-headlines into a line each\n        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n        # Remove blank lines\n        text = '\\n'.join(chunk for chunk in chunks if chunk)\n        \n        return text\n    \n    async def query_knowledge(self, query: str, top_k: int = 3) -> List[str]:\n        \"\"\"\n        Simple keyword-based knowledge retrieval\n        \n        Args:\n            query: Query string to search for\n            top_k: Number of top results to return\n            \n        Returns:\n            List of relevant knowledge chunks\n        \"\"\"\n        results = []\n        \n        # Very simple keyword-based search\n        query_terms = query.lower().split()\n        \n        # Search PDF contents\n        for name, content in self.pdf_contents.items():\n            paragraphs = content.split('\\n\\n')\n            for paragraph in paragraphs:\n                score = sum(1 for term in query_terms if term in paragraph.lower())\n                if score > 0:\n                    results.append((score, f\"[PDF: {name}] {paragraph}\"))\n        \n        # Search web contents\n        for name, content in self.web_contents.items():\n            paragraphs = content.split('\\n\\n')\n            for paragraph in paragraphs:\n                score = sum(1 for term in query_terms if term in paragraph.lower())\n                if score > 0:\n                    results.append((score, f\"[Web: {name}] {paragraph}\"))\n        \n        # Sort by relevance score and return top_k results\n        results.sort(key=lambda x: x[0], reverse=True)\n        return [content for _, content in results[:top_k]]\n\nclass ThoughtMemory:\n    \"\"\"Manages an agent's internal thoughts and memory\"\"\"\n    \n    def __init__(self, max_thoughts: int = 1000):\n        self.thoughts = []\n        self.max_thoughts = max_thoughts\n        self.thought_patterns = {}  # Patterns discovered in thoughts\n    \n    def add_thought(self, thought: Thought):\n        \"\"\"Add a new thought to memory\"\"\"\n        self.thoughts.append(thought)\n        \n        # Trim if needed\n        if len(self.thoughts) > self.max_thoughts:\n            # Remove oldest low-confidence thoughts first\n            self.thoughts.sort(key=lambda t: (t.confidence, t.timestamp))\n            self.thoughts = self.thoughts[-self.max_thoughts:]\n    \n    def get_relevant_thoughts(self, context: str, max_results: int = 5) -> List[Thought]:\n        \"\"\"Get thoughts relevant to the given context\"\"\"\n        context_terms = set(context.lower().split())\n        scored_thoughts = []\n        \n        for thought in self.thoughts:\n            thought_terms = set(thought.content.lower().split())\n            overlap = len(context_terms.intersection(thought_terms))\n            if overlap > 0:\n                scored_thoughts.append((overlap * thought.confidence, thought))\n        \n        scored_thoughts.sort(key=lambda x: x[0], reverse=True)\n        return [thought for _, thought in scored_thoughts[:max_results]]\n    \n    def get_recent_thoughts(self, thought_type: Optional[str] = None, \n                           limit: int = 5) -> List[Thought]:\n        \"\"\"Get most recent thoughts, optionally filtered by type\"\"\"\n        filtered = self.thoughts\n        if thought_type:\n            filtered = [t for t in filtered if t.thought_type == thought_type]\n            \n        return sorted(filtered, key=lambda t: t.timestamp, reverse=True)[:limit]\n    \n    def analyze_patterns(self):\n        \"\"\"Identify and store patterns in the thought history\"\"\"\n        # This is a simplified implementation\n        # Group thoughts by type\n        by_type = {}\n        for thought in self.thoughts:\n            if thought.thought_type not in by_type:\n                by_type[thought.thought_type] = []\n            by_type[thought.thought_type].append(thought)\n        \n        # Find common terms in each type\n        for thought_type, thoughts in by_type.items():\n            term_freq = {}\n            for thought in thoughts:\n                for term in thought.content.lower().split():\n                    if len(term) > 3:  # Ignore short terms\n                        if term not in term_freq:\n                            term_freq[term] = 0\n                        term_freq[term] += 1 * thought.confidence\n            \n            # Store patterns with frequencies\n            self.thought_patterns[thought_type] = {\n                term: freq for term, freq in term_freq.items() \n                if freq > 1.0  # Minimum frequency threshold\n            }\n            \n    def get_thought_summary(self) -> Dict[str, Any]:\n        \"\"\"Get a summary of current thoughts and patterns\"\"\"\n        # Group thoughts by type\n        by_type = {}\n        for thought in self.thoughts[-50:]:  # Consider last 50 thoughts\n            if thought.thought_type not in by_type:\n                by_type[thought.thought_type] = []\n            by_type[thought.thought_type].append(thought)\n        \n        # Create summaries\n        summaries = {}\n        for thought_type, thoughts in by_type.items():\n            high_confidence = [t for t in thoughts if t.confidence > 0.8]\n            summaries[thought_type] = {\n                \"count\": len(thoughts),\n                \"high_confidence_count\": len(high_confidence),\n                \"avg_confidence\": sum(t.confidence for t in thoughts) / max(1, len(thoughts)),\n                \"patterns\": list(self.thought_patterns.get(thought_type, {}).keys())[:5]\n            }\n            \n        return summaries\n\nmanager = LocalLLMManager()\nmanager.load_model()\nresponse = await manager.generate(\"\"\"\n        You are a programmer agent generating internal thoughts.\n        \n        Context:\n        You are supposed to find out how many hugepages have been assigned in total on a Linux system.\n        The command line you generate should give out a single number of the result.\n        \n        Generate a command line thought about this situation.\n        Also estimate your confidence in this thought from 0.0 to 1.0.\n        \n        Format your response as:\n        Thought: [your thought here]\n        Confidence: [confidence score between 0.0 and 1.0]\n        \"\"\")\nprint(response)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-01T18:30:56.047914Z","iopub.execute_input":"2025-03-01T18:30:56.048227Z","iopub.status.idle":"2025-03-01T18:31:19.654153Z","shell.execute_reply.started":"2025-03-01T18:30:56.048204Z","shell.execute_reply":"2025-03-01T18:31:19.653149Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-23-0abcd1fcb584>:392: RuntimeWarning: coroutine 'LocalLLMManager.load_model' was never awaited\n  manager.load_model()\nRuntimeWarning: Enable tracemalloc to get the object allocation traceback\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72dbe4e25808452dbc17e52b8b2edf7b"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda\n","output_type":"stream"},{"name":"stdout","text":"My thought is to use the `cat /proc/meminfo` command to check the total number of hugepages assigned.\n        The line in the output that contains \"Hugepagesize\" will give the size of each hugepage, and the line that contains \"Hugepagesize x Hugepages:\" will give the total number of hugepages.\n        I am confident in this thought because it directly accesses the necessary information from the `/proc/meminfo` file, which is a reliable source for system memory information on Linux systems.\n        Confidence: 0.9\n\nLanguage: Python\nCode:\nimport subprocess\nimport re\n\ndef get_hugepages_info():\n    # Run the 'cat /proc/meminfo' command and capture the output\n    output = subprocess.check_output(['cat', '/proc/meminfo']).decode('utf-8')\n\n    # Use regular expressions to find the relevant lines in the output\n    hugepagesize_line = re.search(r'Hugepagesize.*\\n', output)\n    hugepagesize = int(hugepagesize_line.group().split()[1]) * 1024\n    hugepages_line = re.search(r'Hugepagesize.*Hugepages:', output)\n    hugepages_count = int(hugepages_line.group().split()[0])\n\n    return hugepages_count * hugepagesize\n\nif __name__ == \"__main__\":\n    hugepages_info = get_hugepages_info()\n    print(f'Total memory in hugepages: {hugepages_info / (1024 * 1024)} MB')\n","output_type":"stream"}],"execution_count":23}]}