{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# 1. Load pre-trained model and tokenizer\nmodel_name = \"gpt2\"  # Start with a smaller model for demonstration\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token  # Set pad token\n\n# 2. Define Reward Model for RLHF\nclass RewardModel(nn.Module):\n    def __init__(self, model_name):\n        super().__init__()\n        # Use the same architecture as the policy model but with a different head\n        self.backbone = AutoModelForCausalLM.from_pretrained(model_name).transformer\n        self.reward_head = nn.Linear(self.backbone.config.n_embd, 1)\n        \n    def forward(self, input_ids, attention_mask=None):\n        outputs = self.backbone(input_ids, attention_mask=attention_mask)\n        last_hidden_states = outputs.last_hidden_state\n        # Take the final token representation for reward prediction\n        final_state = last_hidden_states[:, -1, :]\n        return self.reward_head(final_state)\n\n# 3. PPO Implementation for the Minecraft Environment\nclass PPOTrainer:\n    def __init__(self, policy_model, reward_model, tokenizer, \n                 lr=1e-5, clip_ratio=0.2, value_coef=0.5, entropy_coef=0.01):\n        self.policy_model = policy_model\n        self.reward_model = reward_model\n        self.tokenizer = tokenizer\n        self.optimizer = optim.Adam(self.policy_model.parameters(), lr=lr)\n        self.clip_ratio = clip_ratio\n        self.value_coef = value_coef\n        self.entropy_coef = entropy_coef\n        \n        # Clone policy for old policy in PPO\n        self.old_policy = AutoModelForCausalLM.from_pretrained(model_name)\n        self.update_old_policy()\n        \n    def update_old_policy(self):\n        \"\"\"Copy current policy parameters to old policy\"\"\"\n        self.old_policy.load_state_dict(self.policy_model.state_dict())\n    \n    def compute_advantages(self, rewards, values, gamma=0.99, lambda_=0.95):\n        \"\"\"Compute GAE (Generalized Advantage Estimation)\"\"\"\n        advantages = torch.zeros_like(rewards)\n        returns = torch.zeros_like(rewards)\n        \n        last_gae_lam = 0\n        for t in reversed(range(len(rewards))):\n            # For simplicity, assuming all episodes end at the same time\n            next_value = values[t+1] if t < len(rewards)-1 else 0\n            delta = rewards[t] + gamma * next_value - values[t]\n            last_gae_lam = delta + gamma * lambda_ * last_gae_lam\n            advantages[t] = last_gae_lam\n            returns[t] = advantages[t] + values[t]\n            \n        return advantages, returns\n    \n    def train_step(self, states, actions, rewards, next_states, dones):\n        \"\"\"\n        Execute one PPO update step\n        - states: tokenized prompts from Minecraft environment\n        - actions: tokenized model responses\n        - rewards: rewards from the environment or reward model\n        \"\"\"\n        # Prepare batched inputs\n        states_tensor = torch.tensor(states, dtype=torch.long)\n        actions_tensor = torch.tensor(actions, dtype=torch.long)\n        \n        # Get value estimates from current policy\n        with torch.no_grad():\n            values = []\n            for state in states_tensor:\n                # Use a value head or reward model to estimate values\n                value = self.reward_model(state.unsqueeze(0)).item()\n                values.append(value)\n            values = torch.tensor(values)\n        \n        # Compute advantages and returns\n        rewards_tensor = torch.tensor(rewards)\n        advantages, returns = self.compute_advantages(rewards_tensor, values)\n        \n        # Normalize advantages (helps with training stability)\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n        \n        # Policy and value optimization\n        for _ in range(4):  # Multiple epochs of optimization on the same data\n            # Get log probs from current and old policy\n            curr_log_probs = self.get_log_probs(self.policy_model, states_tensor, actions_tensor)\n            old_log_probs = self.get_log_probs(self.old_policy, states_tensor, actions_tensor)\n            \n            # PPO ratio and clipped objective\n            ratio = torch.exp(curr_log_probs - old_log_probs.detach())\n            clip_adv = torch.clamp(ratio, 1-self.clip_ratio, 1+self.clip_ratio) * advantages\n            policy_loss = -torch.min(ratio * advantages, clip_adv).mean()\n            \n            # Value loss\n            value_loss = F.mse_loss(values, returns)\n            \n            # Entropy loss for exploration\n            entropy = self.compute_entropy(self.policy_model, states_tensor)\n            entropy_loss = -entropy.mean()\n            \n            # Total loss\n            loss = policy_loss + self.value_coef * value_loss + self.entropy_coef * entropy_loss\n            \n            # Optimize\n            self.optimizer.zero_grad()\n            loss.backward()\n            nn.utils.clip_grad_norm_(self.policy_model.parameters(), 0.5)  # Prevent exploding gradients\n            self.optimizer.step()\n        \n        # Update old policy\n        self.update_old_policy()\n        \n        return {\n            'policy_loss': policy_loss.item(),\n            'value_loss': value_loss.item(),\n            'entropy': entropy.mean().item(),\n            'total_loss': loss.item()\n        }\n    \n    def get_log_probs(self, model, states, actions):\n        \"\"\"Calculate log probabilities of actions under the policy\"\"\"\n        # Forward pass through model\n        outputs = model(states, labels=actions)\n        logits = outputs.logits\n        \n        # Get log probs for each action token\n        log_probs = F.log_softmax(logits[:, :-1, :], dim=-1)\n        \n        # Gather log probs of the actual actions\n        action_log_probs = torch.gather(\n            log_probs, \n            2, \n            actions[:, 1:].unsqueeze(-1)\n        ).squeeze(-1)\n        \n        # Return mean log prob per sequence\n        return action_log_probs.mean(dim=1)\n    \n    def compute_entropy(self, model, states):\n        \"\"\"Compute entropy of the policy for exploration\"\"\"\n        with torch.no_grad():\n            outputs = model(states)\n            logits = outputs.logits\n            probs = F.softmax(logits, dim=-1)\n            log_probs = F.log_softmax(logits, dim=-1)\n            entropy = -(probs * log_probs).sum(dim=-1).mean(dim=1)\n        return entropy\n\n# 4. Minecraft Environment Wrapper (simplified)\nclass MinecraftEnv:\n    def __init__(self, reward_model):\n        self.reward_model = reward_model\n        # This would integrate with the actual Minecraft API like MineRL\n        # (https://minerl.io/) or the Minecraft Python API\n        # For demonstration, we'll use a mock environment\n        \n        # Example tasks in Minecraft\n        self.tasks = [\n            \"Find and mine diamonds\",\n            \"Build a shelter before nightfall\",\n            \"Craft a diamond pickaxe\",\n            \"Defeat the Ender Dragon\",\n            \"Create an automated farm\",\n        ]\n        \n        # Example context templates\n        self.context_templates = [\n            \"You are in a forest biome. {}\",\n            \"You are in an underground cave. {}\",\n            \"You are in a village. {}\",\n            \"You are in the Nether. {}\",\n            \"You are on a mountain top. {}\"\n        ]\n    \n    def reset(self):\n        \"\"\"Reset environment and return initial observation\"\"\"\n        # Select random task and context\n        task = np.random.choice(self.tasks)\n        context = np.random.choice(self.context_templates).format(task)\n        self.current_task = task\n        self.current_context = context\n        return context\n    \n    def step(self, action):\n        \"\"\"\n        Takes an action (model's text response) and returns:\n        - next_state: next prompt/context\n        - reward: computed by reward model\n        - done: whether the episode is complete\n        - info: additional information\n        \"\"\"\n        # In a real implementation, this would execute the action in Minecraft\n        # and return the updated game state\n        \n        # For demonstration, we'll use the reward model to evaluate the action\n        context_tokens = tokenizer.encode(self.current_context, return_tensors=\"pt\")\n        action_tokens = tokenizer.encode(action, return_tensors=\"pt\")\n        \n        # Concatenate context and action for reward model\n        full_sequence = torch.cat([context_tokens, action_tokens[:, 1:]], dim=1)\n        \n        # Calculate reward using the reward model\n        with torch.no_grad():\n            reward = self.reward_model(full_sequence).item()\n        \n        # Simple termination condition\n        done = len(action.split()) > 50  # End episode if response is too long\n        \n        # Generate new context based on action (simplified)\n        if not done:\n            next_context = f\"After {action[:20]}..., {self.current_task} still needs to be completed.\"\n        else:\n            next_context = self.reset()  # New episode\n        \n        return next_context, reward, done, {}\n    \n    def close(self):\n        \"\"\"Close environment resources\"\"\"\n        pass\n\n# 5. Main training loop\ndef train(policy_model, reward_model, tokenizer, num_episodes=1000):\n    \"\"\"\n    Main RLHF training loop for Minecraft LLM agent\n    \n    Args:\n        policy_model: Language model being fine-tuned\n        reward_model: Trained reward model\n        tokenizer: Tokenizer for the language model\n        num_episodes: Number of training episodes\n    \n    Returns:\n        Trained policy model and tokenizer\n    \"\"\"\n    # Initialize trainer and environment\n    trainer = PPOTrainer(policy_model, reward_model, tokenizer)\n    env = MinecraftEnv(reward_model)\n    \n    # Training loop\n    for episode in tqdm(range(num_episodes)):\n        # Reset environment\n        context = env.reset()\n        done = False\n        episode_rewards = []\n        states = []\n        actions = []\n        rewards = []\n        next_states = []\n        dones = []\n        \n        # Episode loop\n        while not done:\n            # Tokenize state\n            state_tokens = tokenizer.encode(context, return_tensors=\"pt\")\n            states.append(state_tokens.squeeze().tolist())\n            \n            # Generate action using policy model\n            with torch.no_grad():\n                action_tokens = policy_model.generate(\n                    state_tokens,\n                    max_length=state_tokens.shape[1] + 50,\n                    temperature=0.7,\n                    do_sample=True\n                )\n            \n            # Convert tokens to text\n            action_text = tokenizer.decode(action_tokens[0][state_tokens.shape[1]:])\n            actions.append(action_tokens.squeeze().tolist())\n            \n            # Take step in environment\n            next_context, reward, done, _ = env.step(action_text)\n            rewards.append(reward)\n            next_states.append(tokenizer.encode(next_context, return_tensors=\"pt\").squeeze().tolist())\n            dones.append(done)\n            \n            # Update current context\n            context = next_context\n        \n        # Update policy with collected experience\n        train_stats = trainer.train_step(states, actions, rewards, next_states, dones)\n        \n        # Log training progress\n        if episode % 10 == 0:\n            print(f\"Episode {episode}: Avg reward: {np.mean(rewards):.4f}, Loss: {train_stats['total_loss']:.4f}\")\n            \n            # Generate sample response for a test prompt\n            test_prompt = \"You are in a dangerous cave with zombies. Find diamonds.\"\n            test_tokens = tokenizer.encode(test_prompt, return_tensors=\"pt\")\n            with torch.no_grad():\n                output = policy_model.generate(\n                    test_tokens, \n                    max_length=test_tokens.shape[1] + 100,\n                    temperature=0.7\n                )\n            print(f\"Sample response: {tokenizer.decode(output[0][test_tokens.shape[1]:])}\\n\")\n            \n    # Save the fine-tuned model\n    policy_model.save_pretrained(\"minecraft_rl_model\")\n    tokenizer.save_pretrained(\"minecraft_rl_model\")\n    \n    return policy_model, tokenizer\n\n# 6. Reward Model Training (before RLHF)\ndef train_reward_model(model_name, preference_dataset, num_epochs=3, batch_size=8, lr=1e-5):\n    # Initialize reward model\n    reward_model = RewardModel(model_name)\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    tokenizer.pad_token = tokenizer.eos_token\n    \n    # In real implementation, load your preference dataset\n    # dataset = load_dataset(preference_dataset)\n    \n    # For demonstration, create a simple mock dataset\n    preferred_responses = [\n        (\"You are in a forest. Find diamonds.\", \"I'll look for a cave entrance or dig down carefully to layer 12, where diamonds are most common.\"),\n        (\"You are in a cave. Build a shelter.\", \"I'll secure this area by placing torches and building a wall with a door to keep monsters out.\")\n    ]\n    \n    rejected_responses = [\n        (\"You are in a forest. Find diamonds.\", \"I'll just start digging straight down.\"),\n        (\"You are in a cave. Build a shelter.\", \"I'll explore deeper into the cave without securing the area first.\")\n    ]\n    \n    # Loss function and optimizer\n    optimizer = optim.Adam(reward_model.parameters(), lr=lr)\n    \n    # Training loop\n    for epoch in range(num_epochs):\n        reward_model.train()\n        total_loss = 0\n        \n        # In real implementation, use DataLoader\n        for i in range(len(preferred_responses)):\n            # Tokenize preferred and rejected responses\n            preferred_prompt, preferred_response = preferred_responses[i]\n            rejected_prompt, rejected_response = rejected_responses[i]\n            \n            preferred_tokens = tokenizer(preferred_prompt + preferred_response, return_tensors=\"pt\", truncation=True, max_length=512)\n            rejected_tokens = tokenizer(rejected_prompt + rejected_response, return_tensors=\"pt\", truncation=True, max_length=512)\n            \n            # Forward pass\n            preferred_reward = reward_model(preferred_tokens.input_ids)\n            rejected_reward = reward_model(rejected_tokens.input_ids)\n            \n            # Bradley-Terry loss for preference learning\n            loss = -F.logsigmoid(preferred_reward - rejected_reward).mean()\n            \n            # Backward pass and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(preferred_responses):.4f}\")\n    \n    # Save the trained reward model\n    torch.save(reward_model.state_dict(), \"minecraft_reward_model.pt\")\n    return reward_model","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:29:33.755215Z","iopub.execute_input":"2025-03-13T13:29:33.755534Z","iopub.status.idle":"2025-03-13T13:30:09.714625Z","shell.execute_reply.started":"2025-03-13T13:29:33.755499Z","shell.execute_reply":"2025-03-13T13:30:09.713978Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a94518d3c5443818fba24b21695230f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1b98ef69e8544f3a08e0fdf88040214"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b11590bff2c541fda2975383fab8b44c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9d9fb1b3fac48a88e063c698d281b42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cb90f1076c14c3bacd0dcd9643ea4f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e9ed484e49447f4b092cfc1adea310c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a81bf207a364b1b895eff59cb4d5822"}},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\nos.environ[\"HF_TOKEN\"] = UserSecretsClient().get_secret(\"HF_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:40:51.010138Z","iopub.execute_input":"2025-03-13T13:40:51.010511Z","iopub.status.idle":"2025-03-13T13:40:51.130903Z","shell.execute_reply.started":"2025-03-13T13:40:51.010481Z","shell.execute_reply":"2025-03-13T13:40:51.129904Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\"\"\"\nUse Case: Creating a Minecraft Assistant for New Players\n\nThis script demonstrates how to use the RLHF-trained model to create\nan in-game assistant that helps new Minecraft players survive their first night.\n\"\"\"\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport time\n\n# Load our RLHF fine-tuned model\nmodel_path = \"minecraft_rl_model\"\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\nclass MinecraftAssistant:\n    def __init__(self, model, tokenizer):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.context = \"\"\n        self.game_time = \"day\"  # Track in-game time\n        self.inventory = []     # Track player inventory\n        \n    def update_game_state(self, time_of_day, current_biome, inventory):\n        \"\"\"Update the assistant's knowledge of game state\"\"\"\n        self.game_time = time_of_day\n        self.current_biome = current_biome\n        self.inventory = inventory\n        \n    def get_advice(self, player_situation):\n        \"\"\"Generate contextual advice based on player's situation\"\"\"\n        # Construct a detailed prompt with game state\n        prompt = f\"\"\"\n        You are in a {self.current_biome} biome. \n        It is currently {self.game_time}.\n        Your inventory contains: {', '.join(self.inventory) if self.inventory else 'nothing'}.\n        \n        Situation: {player_situation}\n        \n        What should I do next to survive and progress?\n        \"\"\"\n        \n        # Generate response using our RLHF-trained model\n        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n        \n        # Set parameters to control generation\n        output = self.model.generate(\n            input_ids,\n            max_length=300,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True,\n            pad_token_id=self.tokenizer.eos_token_id\n        )\n        \n        # Extract only the generated response (not the prompt)\n        response = self.tokenizer.decode(output[0][input_ids.shape[1]:], skip_special_tokens=True)\n        return response\n\n# Demo scenario: New player's first day\ndef first_day_scenario():\n    \"\"\"Simulate a new player's first day experience\"\"\"\n    assistant = MinecraftAssistant(model, tokenizer)\n    \n    # Morning: Just spawned\n    assistant.update_game_state(\"morning\", \"forest\", [])\n    situation = \"I just spawned in a new world. I see trees around me and some sheep.\"\n    print(\"Player:\", situation)\n    advice = assistant.get_advice(situation)\n    print(\"Assistant:\", advice)\n    time.sleep(2)  # Simulate time passing\n    \n    # Midday: Collected some basic resources\n    assistant.update_game_state(\"midday\", \"forest\", [\"wooden axe\", \"12 oak logs\", \"4 wool\"])\n    situation = \"I've gathered some wood and wool. I can see a cave in the distance.\"\n    print(\"\\nPlayer:\", situation)\n    advice = assistant.get_advice(situation)\n    print(\"Assistant:\", advice)\n    time.sleep(2)\n    \n    # Late afternoon: Need to prepare for night\n    assistant.update_game_state(\"late afternoon\", \"forest\", \n                              [\"wooden axe\", \"8 oak logs\", \"4 wool\", \"wooden pickaxe\", \"12 cobblestone\", \"3 coal\"])\n    situation = \"The sun is going down and I don't have a shelter yet.\"\n    print(\"\\nPlayer:\", situation)\n    advice = assistant.get_advice(situation)\n    print(\"Assistant:\", advice)\n    time.sleep(2)\n    \n    # Night: Danger!\n    assistant.update_game_state(\"night\", \"forest edge\", \n                              [\"wooden axe\", \"3 oak logs\", \"wooden pickaxe\", \"crafting table\", \"8 cobblestone\", \"3 coal\"])\n    situation = \"It's dark and I hear zombies nearby! I didn't finish my shelter!\"\n    print(\"\\nPlayer:\", situation)\n    advice = assistant.get_advice(situation)\n    print(\"Assistant:\", advice)\n\nif __name__ == \"__main__\":\n    print(\"Running Minecraft Assistant Demo for First Day Survival\\n\")\n    first_day_scenario()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:40:57.501548Z","iopub.execute_input":"2025-03-13T13:40:57.501871Z","iopub.status.idle":"2025-03-13T13:40:57.646702Z","shell.execute_reply.started":"2025-03-13T13:40:57.501813Z","shell.execute_reply":"2025-03-13T13:40:57.645514Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/minecraft_rl_model/resolve/main/config.json","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    863\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1485\u001b[0m         \u001b[0;31m# Unauthorized => likely a token issue => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1487\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1377\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1295\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1297\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    281\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    457\u001b[0m             )\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-67d2e069-3e3aa0683c5dec2e6ab07e7c;ea94ae0f-6ff5-4cb3-8ca8-bb937447e3f3)\n\nRepository Not Found for url: https://huggingface.co/minecraft_rl_model/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-dda6665ef769>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Load our RLHF fine-tuned model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"minecraft_rl_model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;31m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    488\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m         ) from e\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    427\u001b[0m             \u001b[0;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to pass a token \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: minecraft_rl_model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"],"ename":"OSError","evalue":"minecraft_rl_model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`","output_type":"error"}],"execution_count":6}]}