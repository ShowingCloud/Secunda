{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"async def initialize_streaming_session(self, initial_prompt: str):\n    \"\"\"Initialize a streaming session with an initial prompt\"\"\"\n    if not self.model or not self.tokenizer:\n        await self.load_model()\n    \n    # Tokenize the initial prompt\n    input_ids = self.tokenizer(initial_prompt, return_tensors=\"pt\").input_ids.to(self.device)\n    \n    # Generate initial response and keep the past key values\n    with torch.no_grad():\n        outputs = self.model.generate(\n            input_ids, \n            max_new_tokens=0,  # Just initialize, don't generate yet\n            return_dict_in_generate=True,\n            output_scores=True,\n            past_key_values=None,  # Initial past is None\n        )\n    \n    # Store the past key values for future continuations\n    past_key_values = outputs.past_key_values\n    \n    # Return a session object with the current state\n    return {\n        \"past_key_values\": past_key_values,\n        \"input_ids\": input_ids,\n        \"generated_tokens\": []\n    }\n\nasync def continue_generation(self, session, new_data: str, max_tokens: int = 50):\n    \"\"\"Continue generation with new data fed into an existing session\"\"\"\n    if not self.model or not self.tokenizer:\n        await self.load_model()\n    \n    # Tokenize only the new data\n    new_tokens = self.tokenizer(new_data, return_tensors=\"pt\").input_ids.to(self.device)\n    \n    # Get the past state from the session\n    past_key_values = session[\"past_key_values\"]\n    \n    # Run generation with the cached state\n    loop = asyncio.get_event_loop()\n    \n    def generate_continuation():\n        with torch.no_grad():\n            outputs = self.model.generate(\n                new_tokens,\n                max_new_tokens=max_tokens,\n                do_sample=True,\n                temperature=0.7,\n                top_p=0.95,\n                return_dict_in_generate=True,\n                output_scores=True,\n                past_key_values=past_key_values,  # Use the cached state\n            )\n        \n        # Update the session with new past key values\n        session[\"past_key_values\"] = outputs.past_key_values\n        \n        # Get the generated text\n        generated_ids = outputs.sequences[0][-max_tokens:]\n        generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n        \n        # Update the session's generated tokens\n        session[\"generated_tokens\"].extend(generated_ids.tolist())\n        \n        return generated_text\n    \n    response = await loop.run_in_executor(None, generate_continuation)\n    return response","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"async def streaming_session(self, initial_prompt: str):\n    \"\"\"Create a streaming session that can accept new tokens\"\"\"\n    if not self.model or not self.tokenizer:\n        await self.load_model()\n    \n    # Initialize the context with the prompt\n    input_ids = self.tokenizer(initial_prompt, return_tensors=\"pt\").input_ids.to(self.device)\n    attention_mask = torch.ones_like(input_ids)\n    \n    # Do the initial forward pass to build the KV cache\n    with torch.no_grad():\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            use_cache=True\n        )\n    \n    # Store the context state\n    past_key_values = outputs.past_key_values\n    context_tokens = input_ids\n    \n    return {\n        \"past_key_values\": past_key_values,\n        \"context_tokens\": context_tokens,\n        \"attention_mask\": attention_mask\n    }\n\nasync def add_data_tokens(self, session, new_data: str):\n    \"\"\"Add new data tokens to the existing context\"\"\"\n    # Tokenize new data\n    new_tokens = self.tokenizer(new_data, return_tensors=\"pt\").input_ids.to(self.device)\n    \n    # Extend attention mask for new tokens\n    new_attention = torch.ones_like(new_tokens)\n    extended_attention = torch.cat([session[\"attention_mask\"], new_attention], dim=-1)\n    \n    # Do forward pass with just the new tokens, using the previous KV cache\n    with torch.no_grad():\n        outputs = self.model(\n            input_ids=new_tokens,\n            attention_mask=extended_attention,\n            past_key_values=session[\"past_key_values\"],\n            use_cache=True\n        )\n    \n    # Update the session state\n    session[\"past_key_values\"] = outputs.past_key_values\n    session[\"context_tokens\"] = torch.cat([session[\"context_tokens\"], new_tokens], dim=-1)\n    session[\"attention_mask\"] = extended_attention\n    \n    return session\n\nasync def generate_response(self, session, max_tokens: int = 50):\n    \"\"\"Generate a response based on the current context\"\"\"\n    # Get the last token to start generation\n    last_token = session[\"context_tokens\"][:, -1:]\n    \n    generated_ids = []\n    for _ in range(max_tokens):\n        # Predict next token using the cached KV state\n        with torch.no_grad():\n            outputs = self.model(\n                input_ids=last_token,\n                attention_mask=session[\"attention_mask\"],\n                past_key_values=session[\"past_key_values\"],\n                use_cache=True\n            )\n        \n        # Get the predicted token\n        next_token_logits = outputs.logits[:, -1, :]\n        next_token = self._sample_token(next_token_logits, temperature=0.7)\n        \n        # Append to results\n        generated_ids.append(next_token.item())\n        \n        # Break if we hit an end token\n        if next_token.item() == self.tokenizer.eos_token_id:\n            break\n        \n        # Update for next iteration\n        last_token = next_token.unsqueeze(0).unsqueeze(0)\n        session[\"past_key_values\"] = outputs.past_key_values\n        session[\"context_tokens\"] = torch.cat([session[\"context_tokens\"], last_token], dim=-1)\n        session[\"attention_mask\"] = torch.cat([session[\"attention_mask\"], torch.ones_like(last_token)], dim=-1)\n    \n    # Decode the generated text\n    generated_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n    return generated_text\n\ndef _sample_token(self, logits, temperature=0.7):\n    \"\"\"Sample a token from logits with temperature\"\"\"\n    if temperature == 0:\n        return torch.argmax(logits, dim=-1)\n    \n    probs = torch.nn.functional.softmax(logits / temperature, dim=-1)\n    next_token = torch.multinomial(probs, num_samples=1)\n    return next_token","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}