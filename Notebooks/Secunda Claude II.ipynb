{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade --quiet langchain-community langchain-openai paramiko pydantic","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T17:57:50.701875Z","iopub.execute_input":"2025-02-26T17:57:50.702157Z","iopub.status.idle":"2025-02-26T17:58:03.604922Z","shell.execute_reply.started":"2025-02-26T17:57:50.702131Z","shell.execute_reply":"2025-02-26T17:58:03.603954Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.3/414.3 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.3/472.3 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import paramiko\nimport io\nfrom kaggle_secrets import UserSecretsClient\n\nssh_comment = UserSecretsClient().get_secret(\"SSH_COMMENT\")\n\npublic_key = UserSecretsClient().get_secret(\"SSH_PUBLIC_KEY\")\nencrypted_private_key = UserSecretsClient().get_secret(\"SSH_PRIVATE_KEY_ENCRYPTED\")\nssh_hostname = UserSecretsClient().get_secret(\"SSH_HOSTNAME\")\nssh_username = UserSecretsClient().get_secret(\"SSH_USERNAME\")\n#ssh_jump_gateway = UserSecretsClient().get_secret(\"SSH_JUMP_GATEWAY\")\n#ssh_jump_dest = UserSecretsClient().get_secret(\"SSH_JUMP_DEST\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T17:58:09.069902Z","iopub.execute_input":"2025-02-26T17:58:09.070205Z","iopub.status.idle":"2025-02-26T17:58:10.067085Z","shell.execute_reply.started":"2025-02-26T17:58:09.070182Z","shell.execute_reply":"2025-02-26T17:58:10.065916Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from langchain_core.tools import BaseTool\nfrom langchain.agents import AgentExecutor, LLMSingleActionAgent\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import StringPromptTemplate, PromptTemplate\nfrom typing import List, Dict, Optional\nimport asyncio\nimport paramiko\nimport json\nfrom typing import ClassVar\nfrom pydantic import BaseModel, Field, PrivateAttr\n\nclass SSHConnection:\n    def __init__(self):\n        self.client = paramiko.SSHClient()\n        self.client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n\n        key_string = encrypted_private_key.replace(\"\\\\n\", \"\\n\")\n        private_key = paramiko.RSAKey.from_private_key(\n            io.StringIO(key_string),\n            password = ssh_comment)\n        self.client.connect(\n            hostname = ssh_hostname,\n            username = ssh_username,\n            pkey = private_key\n        )\n\n    async def execute_interactive(self, command: str, timeout: int = 30) -> asyncio.Queue:\n        \"\"\"\n        Execute an interactive command and return a queue for streaming output\n        \"\"\"\n        output_queue = asyncio.Queue()\n        \n        # Start interactive shell session\n        shell = self.client.invoke_shell()\n        \n        async def read_output():\n            while True:\n                if shell.recv_ready():\n                    output = shell.recv(4096).decode('utf-8')\n                    await output_queue.put(output)\n                await asyncio.sleep(0.1)\n                \n        # Start background task to read output\n        asyncio.create_task(read_output())\n        \n        # Send command\n        shell.send(command + '\\n')\n        \n        return output_queue\n\nclass LinuxCommandTool(BaseTool):\n    name: str = \"linux_command\"\n    description: str = \"Execute Linux commands and handle interactive output\"\n\n    _ssh: SSHConnection = PrivateAttr()\n    \n    def __init__(self, ssh_connection: SSHConnection):\n        super().__init__()\n        self._ssh = ssh_connection\n    \n    async def _run(self, command: str) -> str:\n        output_queue = await self.ssh.execute_interactive(command)\n        \n        # Collect output for a reasonable time\n        full_output = []\n        try:\n            while True:\n                output = await asyncio.wait_for(output_queue.get(), timeout=1.0)\n                full_output.append(output)\n                \n                # Check if command has completed (can be customized based on needs)\n                if output.strip().endswith('$ '):  # Basic prompt detection\n                    break\n        except asyncio.TimeoutError:\n            pass\n            \n        return ''.join(full_output)\n\nclass CommandState(BaseModel):\n    \"\"\"Track the state of command execution\"\"\"\n    command: str = Field(description=\"The command being executed\")\n    output: str = Field(description=\"Current accumulated output\")\n    status: str = Field(description=\"Current status: running/completed/error\")\n    next_action: Optional[str] = Field(description=\"Next action to take based on output\")\n\nclass InteractiveAgentPrompt(StringPromptTemplate):\n    input_variables: List[str] = Field(template=\"\"\"You are an expert Linux system administrator.\n    Current command state: {current_state}\n    \n    Based on the command output, determine the next action:\n    1. If the command is complete, analyze the output and summarize the results\n    2. If the command requires interaction, provide the next input\n    3. If there's an error, suggest how to resolve it\n    \n    Previous actions: {memory}\n    \n    Your response should be in JSON format:\n    {{\"action\": \"complete/interact/error\",\n      \"response\": \"your analysis or next input\",\n      \"reasoning\": \"your reasoning\"}}\n    \"\"\")\n    \n    def format(self, **kwargs) -> str:\n        return self.template.format(**kwargs)\n\nclass InteractiveAgent:\n    def __init__(self, llm, tools: List[BaseTool]):\n        self.llm = llm\n        self.tools = tools\n        self.memory = ConversationBufferMemory()\n        self.prompt = PromptTemplate.from_template(\"\"\"You are an expert Linux system administrator.\n    Current command state: {current_state}\n    \n    Based on the command output, determine the next action:\n    1. If the command is complete, analyze the output and summarize the results\n    2. If the command requires interaction, provide the next input\n    3. If there's an error, suggest how to resolve it\n    \n    Previous actions: {memory}\n    \n    Your response should be in JSON format:\n    {{\"action\": \"complete/interact/error\",\n      \"response\": \"your analysis or next input\",\n      \"reasoning\": \"your reasoning\"}}\n    \"\"\")\n        \n    async def run(self, command: str):\n        state = CommandState(\n            command=command,\n            output=\"\",\n            status=\"running\",\n            next_action=None\n        )\n        \n        while state.status == \"running\":\n            # Get next action from LLM\n            llm_response = await self.llm.apredict(\n                self.prompt.format(\n                    current_state=state.dict(),\n                    memory=self.memory.buffer\n                )\n            )\n            \n            action_data = json.loads(llm_response)\n            \n            if action_data[\"action\"] == \"interact\":\n                # Send next input to command\n                for tool in self.tools:\n                    if tool.name == \"linux_command\":\n                        new_output = await tool.run(action_data[\"response\"])\n                        state.output += new_output\n                        \n            elif action_data[\"action\"] == \"complete\":\n                state.status = \"completed\"\n                return action_data[\"response\"]\n                \n            elif action_data[\"action\"] == \"error\":\n                state.status = \"error\"\n                return action_data[\"response\"]\n            \n            # Update memory\n            self.memory.save_context(\n                {\"input\": state.command},\n                {\"output\": action_data[\"reasoning\"]}\n            )\n\n# Example usage\nasync def main():\n    # Initialize SSH connection\n    ssh = SSHConnection()\n    \n    # Create tools\n    tools = [LinuxCommandTool(ssh)]\n    \n    # Initialize LLM\n    llm = ChatOpenAI(temperature=0, openai_api_key=UserSecretsClient().get_secret(\"OPENAI_API_KEY\"))\n    \n    # Create agent\n    agent = InteractiveAgent(llm, tools)\n    \n    # Run interactive command\n    result = await agent.run(\"sudo apt update\")\n    print(f\"Final result: {result}\")\n\nif __name__ == \"__main__\":\n    await main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-26T18:19:06.629068Z","iopub.execute_input":"2025-02-26T18:19:06.629384Z","iopub.status.idle":"2025-02-26T18:19:09.221321Z","shell.execute_reply.started":"2025-02-26T18:19:06.629362Z","shell.execute_reply":"2025-02-26T18:19:09.220039Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-16-081f5c4253ff>:140: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  current_state=state.dict(),\n<ipython-input-16-081f5c4253ff>:138: LangChainDeprecationWarning: The method `BaseChatModel.apredict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~ainvoke` instead.\n  llm_response = await self.llm.apredict(\n","output_type":"stream"},{"name":"stdout","text":"Final result: The 'sudo apt update' command has completed successfully.\n","output_type":"stream"}],"execution_count":16}]}