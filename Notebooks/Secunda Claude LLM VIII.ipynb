{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import paramiko\nimport asyncio\nimport time\nfrom typing import AsyncGenerator, List, Dict, Optional, Tuple\nfrom dataclasses import dataclass\nimport torch\n\n@dataclass\nclass SSHState:\n    \"\"\"Represents the current state of an SSH session\"\"\"\n    buffer: str = \"\"                    # Current output buffer\n    last_interaction_time: float = 0    # Timestamp of last interaction\n    idle_time: float = 0                # Time with no new output\n    command_history: List[Dict] = None  # Previous commands and outputs\n    pending_decision: bool = False      # Whether LLM is currently making a decision\n    \n    def __post_init__(self):\n        if self.command_history is None:\n            self.command_history = []\n        self.last_interaction_time = time.time()\n\n\nclass SSHConnection_Paramiko:\n    def __init__(self, llm_manager: 'LocalLLMManager'):\n        self.client = paramiko.SSHClient()\n        self.client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n        self.client.connect(\n            hostname=ssh_jump_dest,\n            username=ssh_username,\n            pkey=private_key,\n            sock=channel)\n        \n        self.llm = llm_manager\n        self.max_history = 3  # Number of previous commands to include in context\n        \n    async def stream_output(self, shell) -> AsyncGenerator[Tuple[str, bool], None]:\n        \"\"\"\n        Stream output from shell with metadata\n        Returns: (data, has_data) tuple\n        \"\"\"\n        while True:\n            has_data = False\n            data = \"\"\n            \n            if shell.recv_ready():\n                data = shell.recv(4096).decode('utf-8', errors='replace')\n                has_data = bool(data)\n                \n            yield (data, has_data)\n            await asyncio.sleep(0.05)  # Shorter sleep for responsiveness\n    \n    def _create_llm_prompt(self, state: SSHState, current_command: str) -> str:\n        \"\"\"Create a prompt for the LLM with the current SSH state\"\"\"\n        # Format recent command history\n        history_text = \"\"\n        for cmd in state.command_history[-self.max_history:]:\n            history_text += f\"Command: {cmd['command']}\\nOutput: {cmd['output']}\\n\\n\"\n        \n        # Build the full prompt\n        prompt = f\"\"\"You are an intelligent SSH session agent. You're observing the output of a Linux terminal session.\n\nRecent command history:\n{history_text}\n\nCurrent command: {current_command}\nCurrent output buffer:\n{state.buffer}\n\nTime since last new output: {state.idle_time:.1f} seconds\n\nYour task is to analyze this SSH session and decide what to do next. Consider:\n1. Is there a prompt waiting for input? (e.g. y/n questions, password prompts, etc.)\n2. Has the command completed execution?\n3. Is the command running with continuous output that doesn't need interruption?\n4. Has output stalled, suggesting the command is waiting for input?\n\nProvide your reasoning and decision in the following format:\nTHOUGHT: Your analysis of the situation.\nACTION: [CONTINUE, RESPOND, or COMPLETE]\nRESPONSE: Your suggested input (if ACTION is RESPOND)\n\nCONTINUE means wait for more output.\nRESPOND means send a specific input.\nCOMPLETE means the command has finished and no further interaction is needed.\n\"\"\"\n        return prompt\n    \n    def _parse_llm_response(self, response: str) -> Dict:\n        \"\"\"Parse the LLM's response into structured data\"\"\"\n        result = {\n            \"thought\": \"\",\n            \"action\": \"CONTINUE\",\n            \"response\": \"\"\n        }\n        \n        # Extract the thought\n        if \"THOUGHT:\" in response:\n            thought_parts = response.split(\"THOUGHT:\", 1)[1].split(\"ACTION:\", 1)\n            result[\"thought\"] = thought_parts[0].strip()\n        \n        # Extract the action\n        if \"ACTION:\" in response:\n            action_parts = response.split(\"ACTION:\", 1)[1].split(\"RESPONSE:\", 1)\n            action = action_parts[0].strip()\n            if action in [\"CONTINUE\", \"RESPOND\", \"COMPLETE\"]:\n                result[\"action\"] = action\n        \n        # Extract the response (if any)\n        if \"RESPONSE:\" in response and result[\"action\"] == \"RESPOND\":\n            result[\"response\"] = response.split(\"RESPONSE:\", 1)[1].strip()\n        \n        return result\n    \n    async def execute_interactive_with_llm(self, command: str) -> AsyncGenerator[Dict, None]:\n        \"\"\"\n        Execute a command with LLM-driven interaction\n        Yields: Status updates containing the buffer and decisions\n        \"\"\"\n        shell = self.client.invoke_shell()\n        state = SSHState()\n        \n        # Send the initial command\n        shell.send(command + '\\n')\n        last_output_time = time.time()\n        \n        # Process the output stream\n        async for data, has_data in self.stream_output(shell):\n            # Update state if we received data\n            if has_data:\n                state.buffer += data\n                last_output_time = time.time()\n                \n                # Yield the new data\n                yield {\n                    \"type\": \"output\",\n                    \"data\": data,\n                    \"buffer\": state.buffer\n                }\n            \n            # Update idle time\n            current_time = time.time()\n            state.idle_time = current_time - last_output_time\n            \n            # Determine if we should make a decision\n            should_decide = (\n                not state.pending_decision and (\n                    # Decide after 2 seconds of no output\n                    state.idle_time > 2.0 or\n                    # Decide after accumulating substantial output\n                    len(state.buffer) > 2000 or\n                    # Check for obvious prompts more frequently\n                    (len(state.buffer) > 100 and \n                     any(pattern in state.buffer.lower() for pattern in \n                         ['[y/n]', 'password:', 'continue?', '$ ', '# ']))\n                )\n            )\n            \n            if should_decide:\n                state.pending_decision = True\n                \n                # Create a prompt and send to LLM\n                prompt = self._create_llm_prompt(state, command)\n                \n                # Notify that we're thinking\n                yield {\n                    \"type\": \"thinking\",\n                    \"buffer\": state.buffer\n                }\n                \n                # Generate response from LLM\n                llm_response = await self.llm.generate(prompt, max_tokens=256, temperature=0.3)\n                parsed = self._parse_llm_response(llm_response)\n                \n                # Update pending flag\n                state.pending_decision = False\n                \n                # Handle the decision\n                if parsed[\"action\"] == \"CONTINUE\":\n                    # Just clear buffer if it's large and we're continuing\n                    if len(state.buffer) > 5000:\n                        # Keep the last 1000 chars for context\n                        state.buffer = state.buffer[-1000:]\n                    \n                    # Yield the decision\n                    yield {\n                        \"type\": \"decision\",\n                        \"thought\": parsed[\"thought\"],\n                        \"action\": \"CONTINUE\",\n                        \"buffer\": state.buffer\n                    }\n                    \n                elif parsed[\"action\"] == \"RESPOND\":\n                    # Send the response\n                    response_text = parsed[\"response\"]\n                    shell.send(response_text + '\\n')\n                    \n                    # Update the buffer to show the response\n                    state.buffer += f\"\\n[AGENT INPUT: {response_text}]\\n\"\n                    \n                    # Yield the decision and response\n                    yield {\n                        \"type\": \"decision\",\n                        \"thought\": parsed[\"thought\"],\n                        \"action\": \"RESPOND\",\n                        \"response\": response_text,\n                        \"buffer\": state.buffer\n                    }\n                    \n                    # Reset idle time after responding\n                    last_output_time = time.time()\n                    \n                elif parsed[\"action\"] == \"COMPLETE\":\n                    # Add to command history\n                    state.command_history.append({\n                        \"command\": command,\n                        \"output\": state.buffer\n                    })\n                    \n                    # Yield the completion\n                    yield {\n                        \"type\": \"decision\",\n                        \"thought\": parsed[\"thought\"],\n                        \"action\": \"COMPLETE\",\n                        \"buffer\": state.buffer\n                    }\n                    \n                    # Exit the loop\n                    break","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LocalLLMManager:\n    \"\"\"Manages loading and inference for local LLMs with optimizations for SSH interaction\"\"\"\n    \n    def __init__(self, model_name: str = \"microsoft/phi-3-mini\", device: str = None):\n        \"\"\"\n        Initialize the LLM manager\n        \n        Args:\n            model_name: HuggingFace model identifier\n            device: Device to run the model on (None for auto-detection)\n        \"\"\"\n        self.model_name = model_name\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        logger.info(f\"Initializing LocalLLMManager with {model_name} on {self.device}\")\n        \n        # Load model and tokenizer\n        self.tokenizer = None\n        self.model = None\n        self.pipe = None\n        \n        # Cache for tokenized prompts to speed up repeated calls\n        self.cache = {}\n        self.max_cache_size = 10\n        \n    async def load_model(self):\n        \"\"\"Load the model and tokenizer asynchronously\"\"\"\n        # Run in a separate thread to avoid blocking\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, self._load_model_sync)\n        logger.info(f\"Model {self.model_name} loaded successfully\")\n    \n    def _load_model_sync(self):\n        \"\"\"Synchronous model loading function\"\"\"\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.model_name,\n            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n            device_map=self.device\n        )\n        self.pipe = pipeline(\n            \"text-generation\",\n            model=self.model,\n            tokenizer=self.tokenizer,\n            device=0 if self.device == \"cuda\" else -1\n        )\n    \n    async def generate(self, prompt: str, max_tokens: int = 512, \n                      temperature: float = 0.7) -> str:\n        \"\"\"\n        Generate text using the loaded model\n        \n        Args:\n            prompt: Input prompt for the model\n            max_tokens: Maximum number of tokens to generate\n            temperature: Temperature for sampling\n            \n        Returns:\n            Generated text response\n        \"\"\"\n        if not self.model or not self.tokenizer:\n            await self.load_model()\n            \n        # Run in a separate thread to avoid blocking\n        loop = asyncio.get_event_loop()\n        response = await loop.run_in_executor(\n            None, \n            lambda: self.pipe(\n                prompt,\n                max_new_tokens=max_tokens,\n                temperature=temperature,\n                do_sample=True,\n                top_p=0.95,\n            )[0][\"generated_text\"]\n        )\n        \n        # Extract only the newly generated text\n        return response[len(prompt):].strip()\n    \n    async def generate_for_ssh(self, state_buffer: str, command: str, \n                              history: List[Dict], idle_time: float) -> Dict:\n        \"\"\"\n        Specialized method for SSH decision making\n        \n        Args:\n            state_buffer: Current output buffer\n            command: Currently executing command\n            history: List of previous commands and outputs\n            idle_time: Time with no new output\n            \n        Returns:\n            Dictionary with decision data\n        \"\"\"\n        # Build a specialized prompt for SSH interaction\n        prompt = self._build_ssh_prompt(state_buffer, command, history, idle_time)\n        \n        # Generate with lower temperature for more consistent decisions\n        response = await self.generate(prompt, max_tokens=256, temperature=0.3)\n        \n        # Parse the response into structured data\n        return self._parse_ssh_response(response)\n    \n    def _build_ssh_prompt(self, buffer: str, command: str, \n                         history: List[Dict], idle_time: float) -> str:\n        \"\"\"Build a specialized prompt for SSH decision making\"\"\"\n        # Format recent command history (last 2 commands)\n        history_text = \"\"\n        for cmd in history[-2:]:\n            history_text += f\"Command: {cmd['command']}\\nOutput: {cmd['output'][:500]}...\\n\\n\"\n        \n        # Truncate buffer if too large\n        if len(buffer) > 3000:\n            display_buffer = f\"{buffer[:1000]}...[middle content omitted]...{buffer[-1000:]}\"\n        else:\n            display_buffer = buffer\n        \n        # Build the full prompt\n        prompt = f\"\"\"You are an intelligent SSH session agent. You're observing the output of a Linux terminal session.\n\nRecent command history:\n{history_text}\n\nCurrent command: {command}\nCurrent output buffer:\n{display_buffer}\n\nTime since last new output: {idle_time:.1f} seconds\n\nYour task is to analyze this SSH session and decide what to do next. Consider:\n1. Is there a prompt waiting for input? (e.g. y/n questions, password prompts, etc.)\n2. Has the command completed execution? Look for shell prompts ($ or #) at the end.\n3. Is the command running with continuous output that doesn't need interruption?\n4. Has output stalled, suggesting the command is waiting for input?\n\nProvide your reasoning and decision in the following format:\nTHOUGHT: Your analysis of the situation.\nACTION: [CONTINUE, RESPOND, or COMPLETE]\nRESPONSE: Your suggested input (if ACTION is RESPOND)\n\nCONTINUE means wait for more output.\nRESPOND means send a specific input.\nCOMPLETE means the command has finished and no further interaction is needed.\n\"\"\"\n        return prompt\n    \n    def _parse_ssh_response(self, response: str) -> Dict:\n        \"\"\"Parse the LLM's response into structured data\"\"\"\n        result = {\n            \"thought\": \"\",\n            \"action\": \"CONTINUE\",\n            \"response\": \"\"\n        }\n        \n        # Extract the thought\n        if \"THOUGHT:\" in response:\n            thought_parts = response.split(\"THOUGHT:\", 1)[1].split(\"ACTION:\", 1)\n            result[\"thought\"] = thought_parts[0].strip()\n        \n        # Extract the action\n        if \"ACTION:\" in response:\n            action_parts = response.split(\"ACTION:\", 1)[1].split(\"RESPONSE:\", 1)\n            action = action_parts[0].strip()\n            if action in [\"CONTINUE\", \"RESPOND\", \"COMPLETE\"]:\n                result[\"action\"] = action\n        \n        # Extract the response (if any)\n        if \"RESPONSE:\" in response and result[\"action\"] == \"RESPOND\":\n            result[\"response\"] = response.split(\"RESPONSE:\", 1)[1].strip()\n        \n        return result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"async def run_interactive_command(command: str, llm_manager: LocalLLMManager):\n    \"\"\"Run a command with LLM-driven interaction and display the results\"\"\"\n    ssh = SSHConnection_Paramiko(llm_manager)\n    \n    print(f\"Executing: {command}\")\n    \n    async for update in ssh.execute_interactive_with_llm(command):\n        if update[\"type\"] == \"output\":\n            # Print new output as it arrives\n            print(update[\"data\"], end='', flush=True)\n        \n        elif update[\"type\"] == \"thinking\":\n            # Show that the agent is thinking\n            print(\"\\n[Agent is analyzing the output...]\", end='', flush=True)\n        \n        elif update[\"type\"] == \"decision\":\n            if update[\"action\"] == \"CONTINUE\":\n                # Just show that we're continuing\n                print(\"\\n[Agent decided to wait for more output]\")\n                \n            elif update[\"action\"] == \"RESPOND\":\n                # Show the agent's response\n                print(f\"\\n[Agent is responding with: {update['response']}]\")\n                \n            elif update[\"action\"] == \"COMPLETE\":\n                # Show completion\n                print(\"\\n[Agent determined the command has completed]\")\n                print(f\"\\nThought process: {update['thought']}\")\n                break","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}