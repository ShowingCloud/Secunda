{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers peft torch datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import AutoModelForSequenceClassification\n\nclass SharedTaskLora(nn.Module):\n    def __init__(self, base_model, shared_r=8, task_r=4):\n        super().__init__()\n        self.base_model = base_model\n        self.shared_lora = LoraConfig(\n            r=shared_r,\n            lora_alpha=16,\n            target_modules=[\"query\", \"value\"],\n            lora_dropout=0.1,\n            # 共享参数部分\n        )\n        self.task_lora1 = LoraConfig(\n            r=task_r,\n            lora_alpha=8,\n            target_modules=[\"query\", \"value\"],\n            lora_dropout=0.1,\n            # 任务1特有参数\n        )\n        self.task_lora2 = LoraConfig(\n            r=task_r,\n            lora_alpha=8,\n            target_modules=[\"query\", \"value\"],\n            lora_dropout=0.1,\n            # 任务2特有参数\n        )\n        \n        # 创建共享基础模型\n        self.shared_model = get_peft_model(base_model, self.shared_lora)\n        # 添加任务特定适配器\n        self.shared_model.add_adapter(\"task1\", self.task_lora1)\n        self.shared_model.add_adapter(\"task2\", self.task_lora2)\n\n    def forward(self, input_ids, attention_mask, task_id):\n        # 动态切换适配器\n        if task_id == 0:\n            self.shared_model.set_adapter(\"task1\")\n        else:\n            self.shared_model.set_adapter(\"task2\")\n        return self.shared_model(input_ids, attention_mask=attention_mask)\n\n# 初始化模型\nbase_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\nmodel = SharedTaskLora(base_model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.optim import AdamW\nfrom datasets import load_dataset\n\n# 模拟两个任务的数据\ntask1_dataset = load_dataset(\"imdb\")[\"train\"].select(range(1000))\ntask2_dataset = load_dataset(\"hate_speech\")[\"train\"].select(range(1000))\n\n# 优化器分组：共享参数低学习率，任务参数高学习率\noptimizer = AdamW([\n    {\"params\": model.shared_model.base_model.parameters(), \"lr\": 1e-5},\n    {\"params\": model.shared_model.get_adapter(\"task1\").parameters(), \"lr\": 1e-4},\n    {\"params\": model.shared_model.get_adapter(\"task2\").parameters(), \"lr\": 1e-4},\n])\n\n# 交替训练函数\ndef alternate_train(model, task1_data, task2_data, epochs=3):\n    for epoch in range(epochs):\n        # 混合数据并打乱顺序\n        mixed_batches = interleave_batches(task1_data, task2_data, batch_size=8)\n        \n        for batch in mixed_batches:\n            inputs = tokenizer(batch[\"text\"], padding=True, truncation=True, return_tensors=\"pt\")\n            task_id = batch[\"task_id\"]  # 假设数据中包含任务ID\n            \n            # 前向传播（自动选择适配器）\n            outputs = model(**inputs, task_id=task_id)\n            loss = outputs.loss\n            \n            # L2正则化约束任务参数与共享参数的相似性\n            l2_loss = 0.0\n            for task_param in model.shared_model.get_adapter_parameters():\n                l2_loss += torch.norm(task_param - model.shared_model.base_model.parameters())\n            loss += 0.1 * l2_loss  # λ=0.1\n            \n            # 反向传播\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"l2_loss = torch.norm(task_param - shared_param)  # 计算L2距离\nloss += 0.1 * l2_loss  # λ=0.1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 数据预处理函数\ndef preprocess_function(examples, task_id):\n    tokenized = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n    tokenized[\"task_id\"] = task_id\n    return tokenized\n\n# 处理两个任务的数据\ntask1_data = task1_dataset.map(preprocess_function, fn_kwargs={\"task_id\": 0})\ntask2_data = task2_dataset.map(preprocess_function, fn_kwargs={\"task_id\": 1})\n\n# 开始训练\nalternate_train(model, task1_data, task2_data, epochs=3)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}