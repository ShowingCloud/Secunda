{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport json\nimport logging\nimport hashlib\nfrom typing import Dict, List, Optional, Tuple, Union, Set\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\nimport asyncio\nfrom collections import defaultdict\nimport numpy as np\n\n# For PDF processing\nimport PyPDF2\nimport fitz  # PyMuPDF\n\n# For web scraping\nimport requests\nfrom bs4 import BeautifulSoup\n\n# For text processing and embeddings\nfrom sentence_transformers import SentenceTransformer\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nnltk.download('punkt', quiet=True)\n\n# Initialize logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass KnowledgeChunk:\n    \"\"\"A chunk of knowledge extracted from a document.\"\"\"\n    content: str\n    source: str\n    page_num: Optional[int] = None\n    section: Optional[str] = None\n    url: Optional[str] = None\n    embedding: Optional[np.ndarray] = None\n    metadata: Dict = field(default_factory=dict)\n    chunk_id: str = field(init=False)\n    \n    def __post_init__(self):\n        # Generate a unique ID based on content\n        self.chunk_id = hashlib.md5(self.content.encode()).hexdigest()\n    \n    def to_dict(self) -> Dict:\n        \"\"\"Convert to dictionary for storage.\"\"\"\n        result = {\n            \"chunk_id\": self.chunk_id,\n            \"content\": self.content,\n            \"source\": self.source,\n            \"metadata\": self.metadata\n        }\n        \n        if self.page_num is not None:\n            result[\"page_num\"] = self.page_num\n        if self.section is not None:\n            result[\"section\"] = self.section\n        if self.url is not None:\n            result[\"url\"] = self.url\n        if self.embedding is not None:\n            result[\"embedding\"] = self.embedding.tolist()\n            \n        return result\n    \n    @classmethod\n    def from_dict(cls, data: Dict) -> 'KnowledgeChunk':\n        \"\"\"Create from dictionary storage.\"\"\"\n        embedding = data.get(\"embedding\")\n        if embedding is not None:\n            embedding = np.array(embedding)\n        \n        chunk = cls(\n            content=data[\"content\"],\n            source=data[\"source\"],\n            page_num=data.get(\"page_num\"),\n            section=data.get(\"section\"),\n            url=data.get(\"url\"),\n            embedding=embedding,\n            metadata=data.get(\"metadata\", {})\n        )\n        chunk.chunk_id = data[\"chunk_id\"]\n        return chunk\n\n\nclass KnowledgeBase:\n    \"\"\"Manages a collection of knowledge chunks with semantic search capabilities.\"\"\"\n    \n    def __init__(self, embedding_model: str = 'all-MiniLM-L6-v2', cache_dir: Optional[str] = None):\n        \"\"\"Initialize the knowledge base.\n        \n        Args:\n            embedding_model: The SentenceTransformer model to use for embeddings\n            cache_dir: Directory to cache extracted text and embeddings\n        \"\"\"\n        self.chunks: Dict[str, KnowledgeChunk] = {}\n        self.source_to_chunks: Dict[str, List[str]] = defaultdict(list)\n        self.embedding_model = SentenceTransformer(embedding_model)\n        self.cache_dir = cache_dir\n        \n        if cache_dir:\n            os.makedirs(cache_dir, exist_ok=True)\n    \n    def add_chunk(self, chunk: KnowledgeChunk) -> None:\n        \"\"\"Add a knowledge chunk to the base.\"\"\"\n        if chunk.embedding is None:\n            chunk.embedding = self.embedding_model.encode(chunk.content)\n        \n        self.chunks[chunk.chunk_id] = chunk\n        self.source_to_chunks[chunk.source].append(chunk.chunk_id)\n    \n    def search(self, query: str, top_k: int = 5) -> List[Tuple[KnowledgeChunk, float]]:\n        \"\"\"Search for chunks relevant to the query.\"\"\"\n        if not self.chunks:\n            return []\n        \n        query_embedding = self.embedding_model.encode(query)\n        \n        results = []\n        for chunk_id, chunk in self.chunks.items():\n            if chunk.embedding is not None:\n                similarity = cosine_similarity(query_embedding, chunk.embedding)\n                results.append((chunk, similarity))\n        \n        results.sort(key=lambda x: x[1], reverse=True)\n        return results[:top_k]\n    \n    def get_chunks_by_source(self, source: str) -> List[KnowledgeChunk]:\n        \"\"\"Get all chunks from a specific source.\"\"\"\n        chunk_ids = self.source_to_chunks.get(source, [])\n        return [self.chunks[chunk_id] for chunk_id in chunk_ids]\n    \n    def save_to_disk(self, filepath: str) -> None:\n        \"\"\"Save the knowledge base to disk.\"\"\"\n        data = {\n            \"chunks\": [chunk.to_dict() for chunk in self.chunks.values()]\n        }\n        \n        with open(filepath, 'w') as f:\n            json.dump(data, f)\n    \n    def load_from_disk(self, filepath: str) -> None:\n        \"\"\"Load the knowledge base from disk.\"\"\"\n        with open(filepath, 'r') as f:\n            data = json.load(f)\n        \n        self.chunks = {}\n        self.source_to_chunks = defaultdict(list)\n        \n        for chunk_data in data[\"chunks\"]:\n            chunk = KnowledgeChunk.from_dict(chunk_data)\n            self.chunks[chunk.chunk_id] = chunk\n            self.source_to_chunks[chunk.source].append(chunk.chunk_id)\n\n\ndef cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\n\nclass DocumentProcessor:\n    \"\"\"Processes documents (PDF, web pages) into knowledge chunks.\"\"\"\n    \n    def __init__(self, knowledge_base: KnowledgeBase, chunk_size: int = 1000, chunk_overlap: int = 200):\n        \"\"\"Initialize the document processor.\n        \n        Args:\n            knowledge_base: KnowledgeBase to store processed chunks\n            chunk_size: Maximum character length for each chunk\n            chunk_overlap: Character overlap between consecutive chunks\n        \"\"\"\n        self.knowledge_base = knowledge_base\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n    \n    def process_pdf(self, pdf_path: str, extract_sections: bool = True) -> List[str]:\n        \"\"\"Process a PDF document into the knowledge base.\n        \n        Args:\n            pdf_path: Path to the PDF file\n            extract_sections: Whether to try to extract section titles\n            \n        Returns:\n            List of chunk IDs added to the knowledge base\n        \"\"\"\n        logger.info(f\"Processing PDF: {pdf_path}\")\n        \n        # Check if we have a cached version\n        if self.knowledge_base.cache_dir:\n            pdf_hash = hashlib.md5(open(pdf_path, 'rb').read()).hexdigest()\n            cache_file = os.path.join(self.knowledge_base.cache_dir, f\"{pdf_hash}.json\")\n            \n            if os.path.exists(cache_file):\n                logger.info(f\"Loading cached PDF processing for {pdf_path}\")\n                with open(cache_file, 'r') as f:\n                    cached_data = json.load(f)\n                \n                chunk_ids = []\n                for chunk_data in cached_data:\n                    chunk = KnowledgeChunk.from_dict(chunk_data)\n                    self.knowledge_base.add_chunk(chunk)\n                    chunk_ids.append(chunk.chunk_id)\n                \n                return chunk_ids\n        \n        # Extract text using PyMuPDF (fitz)\n        doc = fitz.open(pdf_path)\n        \n        # Extract sections and content\n        sections = []\n        current_section = \"Introduction\"\n        \n        filename = os.path.basename(pdf_path)\n        chunk_ids = []\n        \n        for page_num in range(len(doc)):\n            page = doc[page_num]\n            text = page.get_text()\n            \n            # Try to identify section headers if requested\n            if extract_sections:\n                lines = text.split('\\n')\n                for line in lines:\n                    line = line.strip()\n                    # Simple heuristic for section headers: short, all caps or title case\n                    if 3 < len(line) < 100 and (line.isupper() or line.istitle()):\n                        if not any(c.isdigit() for c in line) or re.match(r'^\\d+\\.\\s+', line):\n                            current_section = line\n            \n            # Create chunks from the page text\n            chunks = self._chunk_text(text)\n            \n            for chunk_text in chunks:\n                chunk = KnowledgeChunk(\n                    content=chunk_text,\n                    source=filename,\n                    page_num=page_num + 1,\n                    section=current_section\n                )\n                \n                self.knowledge_base.add_chunk(chunk)\n                chunk_ids.append(chunk.chunk_id)\n        \n        # Cache the results if caching is enabled\n        if self.knowledge_base.cache_dir:\n            chunks_data = [self.knowledge_base.chunks[chunk_id].to_dict() for chunk_id in chunk_ids]\n            with open(cache_file, 'w') as f:\n                json.dump(chunks_data, f)\n        \n        return chunk_ids\n    \n    def process_webpage(self, url: str, selector: Optional[str] = None) -> List[str]:\n        \"\"\"Process a webpage into the knowledge base.\n        \n        Args:\n            url: URL of the webpage\n            selector: Optional CSS selector to extract specific content\n            \n        Returns:\n            List of chunk IDs added to the knowledge base\n        \"\"\"\n        logger.info(f\"Processing webpage: {url}\")\n        \n        # Check if we have a cached version\n        if self.knowledge_base.cache_dir:\n            url_hash = hashlib.md5(url.encode()).hexdigest()\n            cache_file = os.path.join(self.knowledge_base.cache_dir, f\"{url_hash}.json\")\n            \n            if os.path.exists(cache_file):\n                logger.info(f\"Loading cached webpage processing for {url}\")\n                with open(cache_file, 'r') as f:\n                    cached_data = json.load(f)\n                \n                chunk_ids = []\n                for chunk_data in cached_data:\n                    chunk = KnowledgeChunk.from_dict(chunk_data)\n                    self.knowledge_base.add_chunk(chunk)\n                    chunk_ids.append(chunk.chunk_id)\n                \n                return chunk_ids\n        \n        # Fetch the webpage\n        try:\n            response = requests.get(url)\n            response.raise_for_status()\n        except requests.RequestException as e:\n            logger.error(f\"Failed to fetch webpage {url}: {e}\")\n            return []\n        \n        # Parse HTML\n        soup = BeautifulSoup(response.text, 'html.parser')\n        \n        # Remove unwanted elements\n        for element in soup(['script', 'style', 'nav', 'footer', 'head']):\n            element.decompose()\n        \n        # Extract content based on selector if provided\n        if selector:\n            content_elements = soup.select(selector)\n            content = ' '.join(element.get_text(strip=True) for element in content_elements)\n        else:\n            # Extract main content\n            main_content = soup.find('main') or soup.find('article') or soup.find('body')\n            content = main_content.get_text(strip=True) if main_content else soup.get_text(strip=True)\n        \n        # Extract title\n        title_element = soup.find('title')\n        title = title_element.get_text() if title_element else url\n        \n        # Create chunks\n        chunks = self._chunk_text(content)\n        chunk_ids = []\n        \n        for chunk_text in chunks:\n            chunk = KnowledgeChunk(\n                content=chunk_text,\n                source=title,\n                url=url\n            )\n            \n            self.knowledge_base.add_chunk(chunk)\n            chunk_ids.append(chunk.chunk_id)\n        \n        # Cache the results if caching is enabled\n        if self.knowledge_base.cache_dir:\n            chunks_data = [self.knowledge_base.chunks[chunk_id].to_dict() for chunk_id in chunk_ids]\n            with open(cache_file, 'w') as f:\n                json.dump(chunks_data, f)\n        \n        return chunk_ids\n    \n    def _chunk_text(self, text: str) -> List[str]:\n        \"\"\"Split text into overlapping chunks of approximately even size.\"\"\"\n        if len(text) <= self.chunk_size:\n            return [text]\n        \n        # Split text into sentences\n        sentences = sent_tokenize(text)\n        \n        chunks = []\n        current_chunk = []\n        current_length = 0\n        \n        for sentence in sentences:\n            sentence_length = len(sentence)\n            \n            # If adding this sentence would exceed chunk size, save current chunk and start a new one\n            if current_length + sentence_length > self.chunk_size and current_length > 0:\n                chunks.append(' '.join(current_chunk))\n                \n                # Include overlap by keeping some sentences from the previous chunk\n                overlap_length = 0\n                overlap_sentences = []\n                \n                # Add sentences from the end of the previous chunk until we reach desired overlap\n                for i in range(len(current_chunk) - 1, -1, -1):\n                    sent = current_chunk[i]\n                    if overlap_length + len(sent) <= self.chunk_overlap:\n                        overlap_sentences.insert(0, sent)\n                        overlap_length += len(sent)\n                    else:\n                        break\n                \n                current_chunk = overlap_sentences\n                current_length = overlap_length\n            \n            current_chunk.append(sentence)\n            current_length += sentence_length\n        \n        # Add the last chunk if there's anything left\n        if current_chunk:\n            chunks.append(' '.join(current_chunk))\n        \n        return chunks\n\n\nclass ManualIntegration:\n    \"\"\"Integrates manuals and documentation into agent knowledge.\"\"\"\n    \n    def __init__(self, knowledge_base: KnowledgeBase = None, cache_dir: str = \"./knowledge_cache\"):\n        \"\"\"Initialize the manual integration system.\n        \n        Args:\n            knowledge_base: Optional existing knowledge base to use\n            cache_dir: Directory to cache extracted text and embeddings\n        \"\"\"\n        self.knowledge_base = knowledge_base or KnowledgeBase(cache_dir=cache_dir)\n        self.document_processor = DocumentProcessor(self.knowledge_base)\n    \n    async def load_manuals(self, manual_sources: Dict[str, Union[str, List[str]]]) -> None:\n        \"\"\"Load manuals from various sources.\n        \n        Args:\n            manual_sources: Dictionary mapping manual types to file paths or URLs\n                Example: {\"linux_commands\": \"/path/to/manual.pdf\", \"web_docs\": [\"https://example.com/docs\"]}\n        \"\"\"\n        tasks = []\n        \n        for manual_type, sources in manual_sources.items():\n            if isinstance(sources, str):\n                sources = [sources]\n            \n            for source in sources:\n                if source.startswith(('http://', 'https://')):\n                    tasks.append(self._process_web_source(manual_type, source))\n                else:\n                    tasks.append(self._process_file_source(manual_type, source))\n        \n        await asyncio.gather(*tasks)\n        logger.info(f\"Loaded {len(self.knowledge_base.chunks)} knowledge chunks from manuals\")\n    \n    async def _process_web_source(self, manual_type: str, url: str) -> None:\n        \"\"\"Process a web source in the background.\"\"\"\n        self.document_processor.process_webpage(url)\n    \n    async def _process_file_source(self, manual_type: str, file_path: str) -> None:\n        \"\"\"Process a file source in the background.\"\"\"\n        if file_path.lower().endswith('.pdf'):\n            self.document_processor.process_pdf(file_path)\n        else:\n            logger.warning(f\"Unsupported file type: {file_path}\")\n    \n    def query_knowledge(self, query: str, top_k: int = 5) -> List[Dict]:\n        \"\"\"Query the knowledge base for relevant information.\n        \n        Args:\n            query: The query string\n            top_k: Number of top results to return\n            \n        Returns:\n            List of relevant knowledge chunks with similarity scores\n        \"\"\"\n        results = self.knowledge_base.search(query, top_k=top_k)\n        return [\n            {\n                \"content\": chunk.content,\n                \"source\": chunk.source,\n                \"page\": chunk.page_num,\n                \"section\": chunk.section,\n                \"similarity\": score,\n                \"url\": chunk.url\n            }\n            for chunk, score in results\n        ]\n    \n    def save_knowledge_base(self, filepath: str) -> None:\n        \"\"\"Save the knowledge base to disk.\"\"\"\n        self.knowledge_base.save_to_disk(filepath)\n    \n    def load_knowledge_base(self, filepath: str) -> None:\n        \"\"\"Load the knowledge base from disk.\"\"\"\n        self.knowledge_base.load_from_disk(filepath)\n\n\nclass AgentKnowledgeProvider:\n    \"\"\"Provides knowledge to agents based on their role and current task.\"\"\"\n    \n    def __init__(self, manual_integration: ManualIntegration):\n        \"\"\"Initialize the agent knowledge provider.\n        \n        Args:\n            manual_integration: The manual integration system\n        \"\"\"\n        self.manual_integration = manual_integration\n        self.role_knowledge_mapping = {}\n    \n    def configure_role_knowledge(self, role: str, knowledge_topics: List[str]) -> None:\n        \"\"\"Configure which knowledge topics are relevant for each agent role.\n        \n        Args:\n            role: Agent role (e.g., \"programmer\", \"project_manager\")\n            knowledge_topics: List of knowledge topics relevant for this role\n        \"\"\"\n        self.role_knowledge_mapping[role] = knowledge_topics\n    \n    def get_knowledge_for_task(self, role: str, task_description: str, max_chunks: int = 5) -> Dict:\n        \"\"\"Get relevant knowledge for a specific task and agent role.\n        \n        Args:\n            role: Agent role (e.g., \"programmer\", \"project_manager\")\n            task_description: Description of the current task\n            max_chunks: Maximum number of knowledge chunks to return\n            \n        Returns:\n            Dictionary with relevant knowledge organized by source\n        \"\"\"\n        # Get all knowledge relevant to the role and task\n        results = self.manual_integration.query_knowledge(task_description, top_k=max_chunks)\n        \n        # Organize by source for better context\n        organized_knowledge = defaultdict(list)\n        for result in results:\n            source_name = result[\"source\"]\n            organized_knowledge[source_name].append({\n                \"content\": result[\"content\"],\n                \"page\": result[\"page\"],\n                \"section\": result[\"section\"],\n                \"similarity\": result[\"similarity\"]\n            })\n        \n        return dict(organized_knowledge)\n    \n    def create_knowledge_prompt(self, role: str, task_description: str) -> str:\n        \"\"\"Create a prompt with relevant knowledge for the agent.\n        \n        Args:\n            role: Agent role\n            task_description: Description of the current task\n            \n        Returns:\n            Formatted prompt with relevant knowledge\n        \"\"\"\n        knowledge = self.get_knowledge_for_task(role, task_description)\n        \n        prompt_parts = [\"### Relevant Knowledge\\n\"]\n        \n        for source, chunks in knowledge.items():\n            prompt_parts.append(f\"\\n## From {source}:\\n\")\n            \n            for chunk in chunks:\n                section = f\" - Section: {chunk['section']}\" if chunk.get('section') else \"\"\n                page = f\" - Page: {chunk['page']}\" if chunk.get('page') is not None else \"\"\n                \n                prompt_parts.append(f\"{section}{page}\\n{chunk['content']}\\n\")\n        \n        return \"\\n\".join(prompt_parts)\n\n\n# Example usage\nasync def main():\n    # Initialize the manual integration system\n    manual_integration = ManualIntegration(cache_dir=\"./knowledge_cache\")\n    \n    # Load manuals\n    await manual_integration.load_manuals({\n        \"linux_commands\": \"/path/to/linux_manual.pdf\",\n        \"programming_guides\": [\n            \"/path/to/python_guide.pdf\",\n            \"https://docs.python.org/3/\"\n        ]\n    })\n    \n    # Save knowledge base for future use\n    manual_integration.save_knowledge_base(\"./knowledge_base.json\")\n    \n    # Create knowledge provider for agents\n    knowledge_provider = AgentKnowledgeProvider(manual_integration)\n    \n    # Configure knowledge for different roles\n    knowledge_provider.configure_role_knowledge(\n        \"programmer\", \n        [\"linux_commands\", \"programming_guides\"]\n    )\n    \n    knowledge_provider.configure_role_knowledge(\n        \"project_manager\", \n        [\"project_management\", \"team_coordination\"]\n    )\n    \n    # Example: Get knowledge for a programmer task\n    task = \"Need to optimize the performance of a Python script that processes large log files\"\n    knowledge_prompt = knowledge_provider.create_knowledge_prompt(\"programmer\", task)\n    \n    print(knowledge_prompt)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}