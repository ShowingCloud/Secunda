{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 环境安装 (需先执行)\n# !pip install transformers datasets peft accelerate torch\n\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\nfrom peft import LoraConfig, TaskType, get_peft_model\nfrom datasets import load_dataset\nimport torch\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\n# 步骤1: 加载模型和tokenizer\nmodel_name = \"distilbert-base-uncased\"  # 使用较小的模型示例\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# 步骤2: 配置LoRA\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,  # 任务类型\n    r=8,                         # LoRA秩\n    lora_alpha=32,               # 缩放因子\n    lora_dropout=0.1,            # Dropout概率\n    bias=\"none\",                 # 是否训练偏置项\n    target_modules=[\"q_lin\", \"v_lin\"]  # 对Transformer的query和value层应用LoRA\n)\n\n# 将基础模型转换为PEFT模型（仅添加0.7%的可训练参数！）\npeft_model = get_peft_model(model, lora_config)\npeft_model.print_trainable_parameters()  # 输出: trainable params: 884,736 || all params: 66,951,936 || 1.32%\n\n# 步骤3: 准备数据集\ndataset = load_dataset(\"imdb\")\ndef tokenize_fn(examples):\n    return tokenizer(\n        examples[\"text\"], \n        padding=\"max_length\", \n        truncation=True, \n        max_length=256\n    )\n\ntokenized_dataset = dataset.map(tokenize_fn, batched=True)\ntrain_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(1000))  # 小样本示例\neval_dataset = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(200))\n\n# 步骤4: 定义评估指标\ndef compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=1)\n    return {\"accuracy\": accuracy_score(p.label_ids, preds)}\n\n# 步骤5: 配置训练参数\ntraining_args = TrainingArguments(\n    output_dir=\"./lora_imdb\",\n    learning_rate=3e-4,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    evaluation_strategy=\"epoch\",\n    logging_dir=\"./logs\",\n    save_strategy=\"no\",  # 小样本训练不需要保存检查点\n    fp16=True,           # 启用混合精度训练\n)\n\n# 步骤6: 创建Trainer\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics,\n)\n\n# 步骤7: 训练模型\ntrainer.train()\n\n# 步骤8: 测试推理示例\nsample_text = \"This movie was absolutely fantastic! The acting was superb.\"\ninputs = tokenizer(sample_text, return_tensors=\"pt\", padding=True, truncation=True)\nwith torch.no_grad():\n    logits = peft_model(**inputs).logits\nprediction = torch.argmax(logits, dim=1).item()\nprint(f\"Prediction: {'Positive' if predction == 1 else 'Negative'}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 对每个任务创建独立的LoRA配置\ntask1_lora = LoraConfig(...)\ntask2_lora = LoraConfig(...)\n\n# 训练时动态切换适配器\npeft_model.set_adapter(\"task1\")  # 切换至任务1的适配器\npeft_model.set_adapter(\"task2\")  # 切换至任务2的适配器","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 常规全参数微调的参数量：\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\nsum(p.numel() for p in model.parameters() if p.requires_grad)  # 约6700万可训练参数","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_model = peft_model.merge_and_unload()  # 将LoRA权重合并到基础模型\nmerged_model.save_pretrained(\"./merged_model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}