{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade --quiet langgraph langchain-community langchain-openai paramiko fabric pydantic","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nssh_comment = UserSecretsClient().get_secret(\"SSH_COMMENT\")\npublic_key = UserSecretsClient().get_secret(\"SSH_PUBLIC_KEY\")\nencrypted_private_key = UserSecretsClient().get_secret(\"SSH_PRIVATE_KEY_ENCRYPTED\")\n\nssh_hostname = UserSecretsClient().get_secret(\"SSH_HOSTNAME\")\nssh_username = UserSecretsClient().get_secret(\"SSH_USERNAME\")\n\nssh_jump_gateway = UserSecretsClient().get_secret(\"SSH_JUMP_GATEWAY\")\nssh_jump_dest = UserSecretsClient().get_secret(\"SSH_JUMP_DEST\")\n\nos.environ[\"HF_TOKEN\"] = user_secrets.get_secret(\"HF_TOKEN\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import paramiko, fabric\nimport io\nimport asyncio\n\nclass SSHConnection_Paramiko:\n    def __init__(self):\n        self.jump_client = paramiko.SSHClient()\n        self.jump_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n\n        key_string = encrypted_private_key.replace(\"\\\\n\", \"\\n\")\n        private_key = paramiko.RSAKey.from_private_key(\n            io.StringIO(key_string),\n            password = ssh_comment)\n        self.jump_client.connect(\n            hostname = ssh_hostname,\n            username = ssh_username,\n            pkey = private_key\n        )\n\n        transport = self.jump_client.get_transport()\n        dest_addr = (ssh_jump_dest, 22)\n        local_addr = (ssh_jump_gateway, 22)\n        channel = transport.open_channel(\"direct-tcpip\", dest_addr, local_addr)\n        \n        self.client = paramiko.SSHClient()\n        self.client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n        self.client.connect(\n            hostname = ssh_jump_dest,\n            username = ssh_username,\n            pkey = private_key,\n            sock = channel)\n\n    async def stream_output(self, shell) -> AsyncGenerator[str, None]:\n        \"\"\"Stream output from shell with backpressure control\"\"\"\n        while True:\n            if shell.recv_ready():\n                data = shell.recv(4096).decode('utf-8')\n                if data:\n                    yield data\n            await asyncio.sleep(0.1)\n\n    async def execute_interactive(self, command: str) -> AsyncGenerator[OutputChunk, None]:\n        \"\"\"Execute command and stream output chunks\"\"\"\n        shell = self.client.invoke_shell()\n        processor = StreamProcessor(PatternMatcher())\n        \n        # Send command\n        shell.send(command + '\\n')\n        \n        # Process output stream\n        async for chunk in processor.process_stream(self.stream_output(shell)):\n            yield chunk\n\nclass SSHConnection_Fabric:\n    def __init__(self):\n        key_string = encrypted_private_key.replace(\"\\\\n\", \"\\n\")\n        private_key = paramiko.RSAKey.from_private_key(\n            io.StringIO(key_string),\n            password = ssh_comment)\n        self.gateway_client = fabric.Connection(\n            ssh_hostname,\n            user = ssh_username,\n            connect_kwargs = { 'pkey': private_key })\n        self.client = fabric.Connection(\n            ssh_jump_dest,\n            user = ssh_username,\n            connect_kwargs = { 'pkey': private_key },\n            gateway = self.gateway_client)\n\n    async def stream_output(self, shell) -> AsyncGenerator[str, None]:\n        pass\n\n    async def execute_interactive(self, command: str) -> AsyncGenerator[OutputChunk, None]:\n        pass\n\nclass SSHConnection:\n    def __init__(self, use_paramiko=True):\n        if use_paramiko:\n            self._impl = SSHConnection_Paramiko()\n            self.client = self._impl.client\n            stdin, stdout, stderr = self.client.exec_command('uname -a')\n            for line in stdout:\n                print(line.strip())\n        else:\n            self._impl = SSHConnection_Fabric()\n            self.client = self._impl.client\n            self.client.run('uname -a')\n\n    async def stream_output(self, shell) -> AsyncGenerator[str, None]:\n        async for chunk in self._impl.stream_output(shell):\n            yield chunk\n\n    async def execute_interactive(self, command: str) -> AsyncGenerator[OutputChunk, None]:\n        async for chunk in self._impl.execute_interactive(command):\n            yield chunk","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom enum import Enum\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\n# Agent types and roles\nclass AgentRole(str, Enum):\n    PROJECT_MANAGER = \"project_manager\"\n    PROGRAMMER = \"programmer\"\n    SPECIALIST = \"specialist\"\n\n\nclass LocalLLMManager:\n    \"\"\"Manages loading and inference for local LLMs\"\"\"\n    \n    def __init__(self, model_name: str = \"microsoft/Phi-4-mini-instruct\", device: str = None):\n        \"\"\"\n        Initialize the LLM manager\n        \n        Args:\n            model_name: HuggingFace model identifier\n            device: Device to run the model on (None for auto-detection)\n        \"\"\"\n        self.model_name = model_name\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        logger.info(f\"Initializing LocalLLMManager with {model_name} on {self.device}\")\n        \n        # Load model and tokenizer\n        self.tokenizer = None\n        self.model = None\n        self.pipe = None\n        \n    async def load_model(self):\n        \"\"\"Load the model and tokenizer asynchronously\"\"\"\n        # Run in a separate thread to avoid blocking\n        loop = asyncio.get_event_loop()\n        await loop.run_in_executor(None, self._load_model_sync)\n        logger.info(f\"Model {self.model_name} loaded successfully\")\n    \n    def _load_model_sync(self):\n        \"\"\"Synchronous model loading function\"\"\"\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.model_name,\n            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n            device_map=self.device\n        )\n        self.pipe = pipeline(\n            \"text-generation\",\n            model=self.model,\n            tokenizer=self.tokenizer,\n            #device=0 if self.device == \"cuda\" else -1\n        )\n    \n    async def generate(self, prompt: str, max_tokens: int = 512, \n                      temperature: float = 0.7) -> str:\n        \"\"\"\n        Generate text using the loaded model\n        \n        Args:\n            prompt: Input prompt for the model\n            max_tokens: Maximum number of tokens to generate\n            temperature: Temperature for sampling\n            \n        Returns:\n            Generated text response\n        \"\"\"\n        if not self.model or not self.tokenizer:\n            await self.load_model()\n            \n        # Run in a separate thread to avoid blocking\n        loop = asyncio.get_event_loop()\n        response = await loop.run_in_executor(\n            None, \n            lambda: self.pipe(\n                prompt,\n                max_new_tokens=max_tokens,\n                temperature=temperature,\n                do_sample=True,\n                top_p=0.95,\n            )[0][\"generated_text\"]\n        )\n        \n        # Extract only the newly generated text\n        return response[len(prompt):].strip()\n\n    async def generate_thought(self, \n                             agent_role: AgentRole,\n                             thought_type: str,\n                             context: str,\n                             confidence_threshold: float = 0.7) -> Optional[Thought]:\n        \"\"\"\n        Generate an internal thought with confidence estimation\n        \n        Args:\n            agent_role: Role of the agent generating the thought\n            thought_type: Type of thought to generate\n            context: Context information for thought generation\n            confidence_threshold: Minimum confidence threshold\n            \n        Returns:\n            A Thought object or None if confidence is too low\n        \"\"\"\n        # Create a prompt for thought generation with confidence estimation\n        prompt = f\"\"\"\n        You are a {agent_role.value} agent generating internal thoughts.\n        \n        Context:\n        {context}\n        \n        Generate a {thought_type} thought about this situation.\n        Also estimate your confidence in this thought from 0.0 to 1.0.\n        \n        Format your response as:\n        Thought: [your thought here]\n        Confidence: [confidence score between 0.0 and 1.0]\n        \"\"\"\n        \n        response = await self.generate(prompt, max_tokens=200, temperature=0.7)\n        \n        # Parse the response\n        thought_content = \"\"\n        confidence = 0.0\n        \n        for line in response.split('\\n'):\n            if line.startswith(\"Thought:\"):\n                thought_content = line[len(\"Thought:\"):].strip()\n            elif line.startswith(\"Confidence:\"):\n                try:\n                    confidence = float(line[len(\"Confidence:\"):].strip())\n                except ValueError:\n                    confidence = 0.0\n        \n        # Create and return thought if confidence is above threshold\n        if confidence >= confidence_threshold and thought_content:\n            return Thought(\n                thought_type=thought_type,\n                content=thought_content,\n                confidence=confidence\n            )\n        return None\n        \nmanager = LocalLLMManager()\nmanager.load_model()\nresponse = await manager.generate(\"\"\"\n        You are a programmer agent generating internal thoughts.\n        \n        Context:\n        You are supposed to find out how many hugepages have been assigned in total on a Linux system.\n        The command line you generate should give out a single number of the result.\n        \n        Generate a observation thought about this situation.\n        Also estimate your confidence in this thought from 0.0 to 1.0.\n        \n        Format your response as:\n        Thought: [your thought here]\n        Confidence: [confidence score between 0.0 and 1.0]\n        \"\"\")\nprint(response)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain_core.tools import BaseTool\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langchain_openai import ChatOpenAI\nfrom langchain.schema import HumanMessage, AIMessage\nfrom typing import List, Dict, Optional, AsyncGenerator\nimport json\nfrom pydantic import BaseModel, Field, PrivateAttr\nfrom datetime import datetime\nimport re\n\n\n\nclass OutputChunk(BaseModel):\n    \"\"\"Represents a chunk of command output\"\"\"\n    content: str\n    timestamp: datetime\n    type: str = \"stdout\"  # stdout, stderr, or system\n    requires_attention: bool = False\n    pattern_matches: Dict[str, str] = Field(default_factory=dict)\n\nclass PatternMatcher:\n    \"\"\"Matches important patterns in command output\"\"\"\n    def __init__(self):\n        self.patterns = {\n            'error': r'error|exception|failed|fatal',\n            'prompt': r'\\[y/N\\]|\\[Y/n\\]|password:|continue\\?',\n            'progress': r'\\d+%|\\d+/\\d+',\n            'completion': r'(done|completed|finished|ready|upgraded).*$',\n        }\n        self.compiled_patterns = {\n            k: re.compile(v, re.IGNORECASE) for k, v in self.patterns.items()\n        }\n\n    def analyze_chunk(self, text: str) -> Dict[str, str]:\n        matches = {}\n        for pattern_name, pattern in self.compiled_patterns.items():\n            if found := pattern.search(text):\n                matches[pattern_name] = found.group(0)\n        return matches\n\nclass StreamProcessor:\n    \"\"\"Processes command output streams and chunks them intelligently\"\"\"\n    def __init__(self, pattern_matcher: PatternMatcher):\n        self.pattern_matcher = pattern_matcher\n        self.buffer = \"\"\n        self.chunk_size = 1024\n        self.min_chunk_size = 100  # Minimum size to process\n\n    def should_chunk(self, text: str) -> bool:\n        \"\"\"Determine if we should create a new chunk based on content\"\"\"\n        if len(text) >= self.chunk_size:\n            return True\n        \n        patterns = self.pattern_matcher.analyze_chunk(text)\n        return bool(patterns)  # Chunk if we find any important patterns\n\n    async def process_stream(self, stream: AsyncGenerator[str, None]) -> AsyncGenerator[OutputChunk, None]:\n        async for data in stream:\n            self.buffer += data\n            \n            while self.buffer:\n                if len(self.buffer) < self.min_chunk_size and not self.should_chunk(self.buffer):\n                    break\n                    \n                chunk_size = min(len(self.buffer), self.chunk_size)\n                chunk_text = self.buffer[:chunk_size]\n                self.buffer = self.buffer[chunk_size:]\n                \n                patterns = self.pattern_matcher.analyze_chunk(chunk_text)\n                requires_attention = bool({'error', 'prompt', 'completion'} & patterns.keys())\n                \n                yield OutputChunk(\n                    content=chunk_text,\n                    timestamp=datetime.now(),\n                    requires_attention=requires_attention,\n                    pattern_matches=patterns\n                )\n\n\n\n\n\n\nclass LinuxCommandTool(BaseTool):\n    name: str = \"linux_command\"\n    description: str = \"Execute Linux commands and handle interactive output\"\n\n    _ssh: SSHConnection = PrivateAttr()\n    \n    def __init__(self, ssh_connection: SSHConnection):\n        super().__init__()\n        self._ssh = ssh_connection\n    \n    async def _run(self, command: str) -> AsyncGenerator[OutputChunk, None]:\n        async for chunk in self._ssh.execute_interactive(command):\n            yield chunk\n    \nclass AgentResponse(BaseModel):\n    \"\"\"Structured response from the agent\"\"\"\n    action: str  # \"continue\", \"interact\", \"alert\", \"complete\"\n    response: str\n    reasoning: str\n    priority: int = 0\n\nclass StreamingAgent:\n    def __init__(self, llm, tools: List[BaseTool]):\n        self.llm = llm\n        self.tools = tools\n        self.memory = MemorySaver() #(max_history=10)\n        self.current_context = []\n        \n    def create_prompt(self, chunks: List[OutputChunk]) -> str:\n        \"\"\"Create prompt for LLM based on recent chunks\"\"\"\n        return f\"\"\"Analyze this command output stream and determine appropriate action:\n\nRecent output:\n{chunks[-5:]}  # Show last 5 chunks\n\nPatterns detected:\n{[chunk.pattern_matches for chunk in chunks[-5:]]}\n\nBased on this output:\n1. If you see a prompt/question, provide the appropriate response\n2. If you detect an error, provide guidance\n3. If the command is progressing normally, return \"continue\"\n4. If the command has completed, provide a summary\n\nRespond in JSON format:\n{{\n    \"action\": \"continue/interact/alert/complete\",\n    \"response\": \"your response or next action\",\n    \"reasoning\": \"your analysis\",\n    \"priority\": 0-10  # Urgency of response\n}}\"\"\"\n\n    async def process_chunks(self, chunks: List[OutputChunk]) -> Optional[AgentResponse]:\n        \"\"\"Process chunks and determine if LLM analysis is needed\"\"\"\n        # Quick pattern-based analysis first\n        if any(chunk.requires_attention for chunk in chunks):\n            # Important patterns detected, consult LLM\n            prompt = self.create_prompt(chunks)\n            response = await self.llm.ainvoke(prompt)\n            print(response)\n            return AgentResponse(**json.loads(response.content))\n\n        # For normal output, accumulate more before consulting LLM\n        if len(self.current_context) >= 5:  # Batch size\n            prompt = self.create_prompt(self.current_context)\n            response = await self.llm.ainvoke(prompt)\n            self.current_context.clear()\n            return AgentResponse(**json.loads(response.content))\n\n        return None\n\n    async def run(self, command: str):\n        \"\"\"Run command and process output stream\"\"\"\n        tool = self.tools[0]  # Assume Linux command tool\n        \n        async for chunk in tool.run(command):\n            self.current_context.append(chunk)\n            \n            if response := await self.process_chunks([chunk]):\n                if response.action != \"continue\":\n                    # Handle interactive needs or alerts\n                    if response.action == \"interact\":\n                        # Send response back to command\n                        await tool.run(response.response)\n                    \n                    # Update memory\n                    self.memory.save_context(\n                        {\"input\": command},\n                        {\"output\": response.reasoning}\n                    )\n                    \n                    yield response\n\n# Example usage\nasync def main():\n    ssh = SSHConnection(use_paramiko = True)\n    \n    tools = [LinuxCommandTool(ssh)]\n    llm = ChatOpenAI(temperature=0, openai_api_key=UserSecretsClient().get_secret(\"OPENAI_API_KEY\"))\n    agent = StreamingAgent(llm, tools)\n    \n    async for response in agent.run(\"sudo apt upgrade\"):\n        if response.action != \"continue\":\n            print(f\"Agent response: {response.model_dump()}\")\n\nif __name__ == \"__main__\":\n    await main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}