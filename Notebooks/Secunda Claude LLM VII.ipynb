{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import paramiko\nimport asyncio\nfrom typing import AsyncGenerator, List, Dict, Optional\nfrom dataclasses import dataclass\nimport re\n\n@dataclass\nclass OutputChunk:\n    text: str\n    is_complete: bool = False\n\n@dataclass\nclass CommandContext:\n    command: str\n    output: str = \"\"\n    thoughts: List[str] = None\n    is_complete: bool = False\n    \n    def __post_init__(self):\n        if self.thoughts is None:\n            self.thoughts = []\n\nclass ContextManager:\n    def __init__(self, max_history: int = 5):\n        self.history: List[CommandContext] = []\n        self.max_history = max_history\n        self.current_context: Optional[CommandContext] = None\n    \n    def start_command(self, command: str) -> CommandContext:\n        self.current_context = CommandContext(command=command)\n        return self.current_context\n    \n    def add_output(self, output: str):\n        if self.current_context:\n            self.current_context.output += output\n    \n    def add_thought(self, thought: str):\n        if self.current_context:\n            self.current_context.thoughts.append(thought)\n    \n    def complete_command(self):\n        if self.current_context:\n            self.current_context.is_complete = True\n            self.history.append(self.current_context)\n            \n            # Maintain history size\n            if len(self.history) > self.max_history:\n                self.history = self.history[-self.max_history:]\n            \n            self.current_context = None\n    \n    def get_recent_context(self, count: int = 2) -> List[CommandContext]:\n        return self.history[-count:] if self.history else []\n\n\nclass PatternMatcher:\n    \"\"\"Detects patterns in SSH output that might indicate when to interrupt or continue\"\"\"\n    \n    def __init__(self):\n        # Common patterns that indicate command completion or prompt for input\n        self.prompt_patterns = [\n            re.compile(r'[$#>]\\s*$'),  # Common shell prompts\n            re.compile(r'password:', re.IGNORECASE),  # Password prompts\n            re.compile(r'\\[y/n\\]', re.IGNORECASE),  # Yes/no confirmation\n            re.compile(r'continue\\?', re.IGNORECASE),  # Continue prompts\n        ]\n        \n        # Patterns that indicate ongoing processes\n        self.ongoing_patterns = [\n            re.compile(r'^\\s*\\d+%', re.MULTILINE),  # Progress indicators\n            re.compile(r'loading', re.IGNORECASE),\n            re.compile(r'please wait', re.IGNORECASE),\n        ]\n    \n    def should_interact(self, text: str) -> bool:\n        \"\"\"Determine if we should interact based on the output\"\"\"\n        for pattern in self.prompt_patterns:\n            if pattern.search(text):\n                return True\n        return False\n    \n    def is_ongoing_process(self, text: str) -> bool:\n        \"\"\"Check if the output indicates an ongoing process\"\"\"\n        for pattern in self.ongoing_patterns:\n            if pattern.search(text):\n                return True\n        return False\n\n\nclass StreamProcessor:\n    def __init__(self, matcher: PatternMatcher):\n        self.matcher = matcher\n        self.buffer = \"\"\n        self.output_line_count = 0\n        \n    async def process_stream(self, stream_generator) -> AsyncGenerator[OutputChunk, None]:\n        async for data in stream_generator:\n            self.buffer += data\n            \n            # Check if we should yield this chunk\n            should_interact = self.matcher.should_interact(self.buffer)\n            is_ongoing = self.matcher.is_ongoing_process(self.buffer)\n            \n            # Count output lines for line-based decision making\n            new_line_count = self.buffer.count('\\n')\n            \n            # Decision logic for when to yield chunks\n            should_yield = (\n                should_interact or \n                (new_line_count - self.output_line_count >= 10 and not is_ongoing) or\n                len(self.buffer) > 1000  # Yield after a certain amount of content\n            )\n            \n            if should_yield:\n                yield OutputChunk(\n                    text=self.buffer,\n                    is_complete=should_interact\n                )\n                self.output_line_count = new_line_count\n                self.buffer = \"\"\n\n\nclass SSHConnection_Paramiko:\n    def __init__(self):\n        self.client = paramiko.SSHClient()\n        self.client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n        self.client.connect(\n            hostname=ssh_jump_dest,\n            username=ssh_username,\n            pkey=private_key,\n            sock=channel)\n        \n        # Initialize the context manager\n        self.context_manager = ContextManager()\n        \n    async def stream_output(self, shell) -> AsyncGenerator[str, None]:\n        \"\"\"Stream output from shell with backpressure control\"\"\"\n        while True:\n            if shell.recv_ready():\n                data = shell.recv(4096).decode('utf-8')\n                if data:\n                    yield data\n            await asyncio.sleep(0.1)\n            \n    async def execute_interactive(self, command: str) -> AsyncGenerator[OutputChunk, None]:\n        \"\"\"Execute command and stream output chunks\"\"\"\n        shell = self.client.invoke_shell()\n        processor = StreamProcessor(PatternMatcher())\n        \n        # Start a new command context\n        context = self.context_manager.start_command(command)\n        \n        # Send command\n        shell.send(command + '\\n')\n        \n        # Process output stream\n        async for chunk in processor.process_stream(self.stream_output(shell)):\n            # Update the context with the output\n            self.context_manager.add_output(chunk.text)\n            \n            # If the command appears to be complete, mark it accordingly\n            if chunk.is_complete:\n                self.context_manager.complete_command()\n                \n            yield chunk\n            \n    async def execute_with_llm(self, command: str, llm_handler):\n        \"\"\"Execute a command and use LLM to decide on further interactions\"\"\"\n        shell = self.client.invoke_shell()\n        processor = StreamProcessor(PatternMatcher())\n        \n        # Start a new command context\n        context = self.context_manager.start_command(command)\n        \n        # Send initial command\n        shell.send(command + '\\n')\n        \n        # Get recent context to provide to the LLM\n        recent_context = self.context_manager.get_recent_context()\n        \n        # Continuously process output\n        buffer = \"\"\n        async for data in self.stream_output(shell):\n            buffer += data\n            self.context_manager.add_output(data)\n            \n            # Check if we need LLM to make a decision\n            if processor.matcher.should_interact(buffer):\n                # Prepare context for LLM\n                llm_input = {\n                    \"current_command\": command,\n                    \"current_output\": buffer,\n                    \"recent_history\": [\n                        {\"command\": ctx.command, \"output\": ctx.output} \n                        for ctx in recent_context\n                    ],\n                    \"thoughts\": context.thoughts\n                }\n                \n                # Get LLM decision\n                llm_response = await llm_handler(llm_input)\n                \n                # Store LLM reasoning as thought\n                if \"thought\" in llm_response:\n                    self.context_manager.add_thought(llm_response[\"thought\"])\n                \n                # Handle LLM decision\n                if \"action\" in llm_response:\n                    action = llm_response[\"action\"]\n                    \n                    if action[\"type\"] == \"continue\":\n                        # Just continue monitoring\n                        buffer = \"\"\n                    elif action[\"type\"] == \"respond\":\n                        # Send a response\n                        response_text = action[\"text\"]\n                        shell.send(response_text + '\\n')\n                        buffer = \"\"\n                    elif action[\"type\"] == \"complete\":\n                        # Mark command as complete and exit\n                        self.context_manager.complete_command()\n                        break\n            \n            # Also yield after accumulating significant output\n            if len(buffer) > 1000:\n                buffer = \"\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}