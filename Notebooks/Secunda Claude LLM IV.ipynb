{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import asyncio\nfrom typing import List, Dict, Optional, Any, Set\nfrom enum import Enum\nfrom pydantic import BaseModel, Field\nfrom datetime import datetime\nfrom collections import deque\nimport json\nimport os\nfrom pathlib import Path\n\n# Import the base agent framework (assumed to be implemented)\nfrom base_agent import BaseAgent, MessageType, Message\n\n# Define programmer expertise areas\nclass ExpertiseArea(str, Enum):\n    PYTHON = \"python\"\n    JAVASCRIPT = \"javascript\"\n    DOCKER = \"docker\"\n    KUBERNETES = \"kubernetes\"\n    DATABASE = \"database\"\n    NETWORKING = \"networking\"\n    LINUX = \"linux\"\n    SECURITY = \"security\"\n    CLOUD = \"cloud\"\n\n# Define task models\nclass Task(BaseModel):\n    id: str\n    title: str\n    description: str\n    required_expertise: List[ExpertiseArea]\n    priority: int = 1\n    deadline: Optional[datetime] = None\n    assigned_at: Optional[datetime] = None\n    completed_at: Optional[datetime] = None\n    status: str = \"pending\"  # pending, in_progress, completed, failed\n\n# Command execution record for learning\nclass CommandExecutionRecord(BaseModel):\n    command: str\n    stdout: str\n    stderr: str\n    syslog: Optional[str] = None\n    exit_code: int\n    system_state_before: Dict[str, Any]\n    system_state_after: Dict[str, Any]\n    execution_time: float\n    timestamp: datetime = Field(default_factory=datetime.now)\n    interaction_history: List[Dict[str, str]] = []  # For interactive commands\n    agent_thoughts: List[Dict[str, Any]] = []\n\n# Thought types for internal processing\nclass ThoughtType(str, Enum):\n    OBSERVATION = \"observation\"\n    ANALYSIS = \"analysis\"\n    DECISION = \"decision\"\n    LEARNING = \"learning\"\n    REFLECTION = \"reflection\"\n    PLANNING = \"planning\"\n\n# Thought structure\nclass Thought(BaseModel):\n    type: ThoughtType\n    content: str\n    confidence: float = 1.0\n    related_command: Optional[str] = None\n    related_output: Optional[str] = None\n    timestamp: datetime = Field(default_factory=datetime.now)\n    tags: List[str] = []\n    \n# Programmer metrics\nclass ProgrammerMetrics(BaseModel):\n    tasks_completed: int = 0\n    tasks_failed: int = 0\n    commands_executed: int = 0\n    command_success_rate: float = 1.0\n    average_task_completion_time: float = 0.0\n    learning_activities: List[str] = []\n    skill_improvements: Dict[str, float] = {}\n\nclass ProgrammerAgent(BaseAgent):\n    def __init__(\n        self,\n        name: str,\n        model_path: str,\n        expertise: List[ExpertiseArea],\n        knowledge_base_paths: List[str],\n        max_memory_size: int = 1000,\n        max_context_size: int = 2048\n    ):\n        super().__init__(name=name, agent_type=\"programmer\")\n        self.model_path = model_path\n        self.expertise = expertise\n        self.knowledge_base_paths = knowledge_base_paths\n        self.current_tasks: List[Task] = []\n        self.completed_tasks: List[Task] = []\n        self.test_results: List[Dict[str, Any]] = []\n        self.command_history: List[CommandExecutionRecord] = []\n        self.metrics = ProgrammerMetrics()\n        \n        # Internal thought process\n        self.thoughts = deque(maxlen=max_memory_size)\n        self.thought_patterns = {}  # Patterns discovered in thoughts\n        self.thought_clusters = {}  # Clusters of related thoughts\n        \n        # Context management\n        self.max_context_size = max_context_size\n        self.current_context = []\n        \n        # LLM initialization will be handled in initialize()\n        self.llm = None\n        \n    async def initialize(self):\n        \"\"\"Initialize the agent, load the LLM, and prepare knowledge base\"\"\"\n        # LLM initialization will be implemented with the LLM deployment part\n        # For now, we'll use a placeholder\n        self.llm = await self._load_model(self.model_path)\n        \n        # Load knowledge from manuals\n        self.knowledge_base = await self._load_knowledge_base()\n        \n        print(f\"Programmer Agent {self.name} initialized with expertise: {', '.join([e.value for e in self.expertise])}\")\n    \n    async def _load_model(self, model_path: str):\n        \"\"\"Load the small LLM model - implementation will vary based on framework choice\"\"\"\n        # Placeholder for actual model loading\n        # This will be replaced with actual implementation in the LLM deployment section\n        return {\"name\": \"placeholder_model\", \"path\": model_path}\n    \n    async def _load_knowledge_base(self) -> Dict[str, Any]:\n        \"\"\"Load knowledge from PDFs or web pages\"\"\"\n        # Placeholder for knowledge base loading\n        knowledge = {}\n        for path in self.knowledge_base_paths:\n            if path.endswith('.pdf'):\n                # Load PDF knowledge\n                knowledge[os.path.basename(path)] = {\"type\": \"pdf\", \"content\": \"PDF content placeholder\"}\n            elif path.startswith('http'):\n                # Load web page knowledge\n                knowledge[path] = {\"type\": \"webpage\", \"content\": \"Web content placeholder\"}\n        return knowledge\n    \n    async def process_message(self, message: Message) -> Optional[Message]:\n        \"\"\"Process incoming messages from other agents\"\"\"\n        # Generate thoughts about the message\n        thought = Thought(\n            type=ThoughtType.OBSERVATION,\n            content=f\"Received message of type {message.type} from {message.sender}\",\n            related_command=message.content.get(\"command\") if isinstance(message.content, dict) else None\n        )\n        self.add_thought(thought)\n        \n        # Process different message types\n        if message.type == MessageType.TASK_ASSIGNMENT:\n            return await self._handle_task_assignment(message)\n        elif message.type == MessageType.COMMAND_REQUEST:\n            return await self._handle_command_request(message)\n        elif message.type == MessageType.FEEDBACK:\n            return await self._handle_feedback(message)\n        \n        # Default acknowledgment\n        return Message(\n            sender=self.name,\n            recipient=message.sender,\n            type=MessageType.ACKNOWLEDGMENT,\n            content={\"status\": \"received\", \"message_id\": message.id}\n        )\n    \n    async def _handle_task_assignment(self, message: Message) -> Message:\n        \"\"\"Handle task assignment from project manager\"\"\"\n        task_data = message.content.get(\"task\", {})\n        task = Task(**task_data)\n        \n        # Check if we have the expertise for this task\n        has_expertise = any(exp in self.expertise for exp in task.required_expertise)\n        \n        if has_expertise:\n            # Accept the task\n            task.status = \"in_progress\"\n            task.assigned_at = datetime.now()\n            self.current_tasks.append(task)\n            \n            # Generate planning thought\n            planning_thought = Thought(\n                type=ThoughtType.PLANNING,\n                content=f\"Planning approach for task: {task.title}\",\n                tags=[exp.value for exp in task.required_expertise]\n            )\n            self.add_thought(planning_thought)\n            \n            return Message(\n                sender=self.name,\n                recipient=message.sender,\n                type=MessageType.TASK_ACCEPTED,\n                content={\"task_id\": task.id, \"status\": \"in_progress\"}\n            )\n        else:\n            # Reject the task due to lack of expertise\n            return Message(\n                sender=self.name,\n                recipient=message.sender,\n                type=MessageType.TASK_REJECTED,\n                content={\n                    \"task_id\": task.id, \n                    \"reason\": \"Lack of required expertise\",\n                    \"available_expertise\": [exp.value for exp in self.expertise]\n                }\n            )\n    \n    async def _handle_command_request(self, message: Message) -> Message:\n        \"\"\"Handle request to execute a command\"\"\"\n        command = message.content.get(\"command\", \"\")\n        server = message.content.get(\"server\", \"\")\n        \n        # Generate analysis thought\n        analysis_thought = Thought(\n            type=ThoughtType.ANALYSIS,\n            content=f\"Analyzing command: {command} for server {server}\",\n            related_command=command,\n            confidence=0.9\n        )\n        self.add_thought(analysis_thought)\n        \n        # For now, we'll just acknowledge the command\n        # In a real implementation, this would execute the command and process results\n        return Message(\n            sender=self.name,\n            recipient=message.sender,\n            type=MessageType.COMMAND_RESPONSE,\n            content={\"command\": command, \"status\": \"acknowledged\"}\n        )\n    \n    async def _handle_feedback(self, message: Message) -> Message:\n        \"\"\"Handle feedback from project manager or other agents\"\"\"\n        feedback = message.content.get(\"feedback\", \"\")\n        task_id = message.content.get(\"task_id\", None)\n        \n        # Generate learning thought\n        learning_thought = Thought(\n            type=ThoughtType.LEARNING,\n            content=f\"Learning from feedback: {feedback}\",\n            confidence=0.85,\n            tags=[\"feedback\", f\"task_{task_id}\"] if task_id else [\"feedback\"]\n        )\n        self.add_thought(learning_thought)\n        \n        # Update metrics based on feedback\n        if \"positive\" in feedback.lower():\n            self.metrics.skill_improvements[\"feedback_response\"] = self.metrics.skill_improvements.get(\"feedback_response\", 0) + 0.1\n        \n        return Message(\n            sender=self.name,\n            recipient=message.sender,\n            type=MessageType.ACKNOWLEDGMENT,\n            content={\"status\": \"feedback_received\", \"message\": \"Thank you for the feedback\"}\n        )\n    \n    def add_thought(self, thought: Thought):\n        \"\"\"Add a thought to the agent's thought process\"\"\"\n        self.thoughts.append(thought)\n        \n        # Process the thought to identify patterns\n        self._process_thought(thought)\n    \n    def _process_thought(self, thought: Thought):\n        \"\"\"Process a thought to identify patterns and update internal knowledge\"\"\"\n        # Extract keywords from thought\n        keywords = set(thought.content.lower().split())\n        \n        # Update thought patterns\n        for keyword in keywords:\n            if keyword not in self.thought_patterns:\n                self.thought_patterns[keyword] = []\n            self.thought_patterns[keyword].append(thought)\n        \n        # Cluster related thoughts (simplified version)\n        for tag in thought.tags:\n            if tag not in self.thought_clusters:\n                self.thought_clusters[tag] = []\n            self.thought_clusters[tag].append(thought)\n    \n    async def execute_command(self, command: str, server: str) -> CommandExecutionRecord:\n        \"\"\"Execute a command on a server and record the results\"\"\"\n        # In a real implementation, this would connect to the server and execute the command\n        # For now, we'll simulate command execution\n        \n        # Generate decision thought\n        decision_thought = Thought(\n            type=ThoughtType.DECISION,\n            content=f\"Decided to execute command: {command} on server {server}\",\n            related_command=command,\n            confidence=0.9\n        )\n        self.add_thought(decision_thought)\n        \n        # Simulate command execution\n        stdout = f\"Simulated output for {command}\"\n        stderr = \"\"\n        exit_code = 0\n        execution_time = 0.5\n        \n        # Create execution record\n        record = CommandExecutionRecord(\n            command=command,\n            stdout=stdout,\n            stderr=stderr,\n            exit_code=exit_code,\n            system_state_before={\"status\": \"before\"},\n            system_state_after={\"status\": \"after\"},\n            execution_time=execution_time,\n            agent_thoughts=[\n                decision_thought.dict(),\n                Thought(\n                    type=ThoughtType.OBSERVATION,\n                    content=f\"Observed command output: {stdout}\",\n                    related_command=command,\n                    related_output=stdout,\n                    confidence=1.0\n                ).dict()\n            ]\n        )\n        \n        # Update metrics\n        self.metrics.commands_executed += 1\n        self.command_history.append(record)\n        \n        return record\n    \n    async def complete_task(self, task_id: str, status: str = \"completed\", results: Dict[str, Any] = None):\n        \"\"\"Mark a task as completed and report results\"\"\"\n        for i, task in enumerate(self.current_tasks):\n            if task.id == task_id:\n                task.status = status\n                task.completed_at = datetime.now()\n                \n                # Generate reflection thought\n                reflection_thought = Thought(\n                    type=ThoughtType.REFLECTION,\n                    content=f\"Reflected on completed task {task.title} with status {status}\",\n                    confidence=0.9,\n                    tags=[f\"task_{task_id}\", status]\n                )\n                self.add_thought(reflection_thought)\n                \n                # Update metrics\n                if status == \"completed\":\n                    self.metrics.tasks_completed += 1\n                else:\n                    self.metrics.tasks_failed += 1\n                \n                # Calculate task completion time\n                if task.assigned_at:\n                    completion_time = (datetime.now() - task.assigned_at).total_seconds()\n                    # Update average completion time\n                    total_completed = self.metrics.tasks_completed + self.metrics.tasks_failed\n                    self.metrics.average_task_completion_time = (\n                        (self.metrics.average_task_completion_time * (total_completed - 1) + completion_time) \n                        / total_completed\n                    )\n                \n                # Move to completed tasks\n                self.completed_tasks.append(task)\n                self.current_tasks.pop(i)\n                \n                # Notify project manager\n                await self.send_message(\n                    \"project_manager\",\n                    MessageType.TASK_COMPLETED,\n                    {\n                        \"task_id\": task_id,\n                        \"status\": status,\n                        \"results\": results or {},\n                        \"completion_time\": task.completed_at - task.assigned_at if task.assigned_at else None\n                    }\n                )\n                return True\n        \n        return False\n    \n    async def run_tests(self):\n        \"\"\"Run tests to improve skills when not working on tasks\"\"\"\n        if not self.current_tasks:\n            # Generate planning thought for self-improvement\n            planning_thought = Thought(\n                type=ThoughtType.PLANNING,\n                content=\"Planning self-improvement through testing\",\n                confidence=0.8,\n                tags=[\"self_improvement\", \"testing\"]\n            )\n            self.add_thought(planning_thought)\n            \n            # Simulate running tests\n            test_result = {\n                \"coverage\": 85.5,\n                \"passed\": 42,\n                \"failed\": 3,\n                \"learning_points\": [\"Point 1\", \"Point 2\"]\n            }\n            \n            self.test_results.append(test_result)\n            \n            # Update learning metrics\n            self.metrics.learning_activities.append(\"Unit Testing\")\n            self.metrics.skill_improvements[\"testing\"] = self.metrics.skill_improvements.get(\"testing\", 0) + 0.1\n            \n            # Generate learning thought from test results\n            learning_thought = Thought(\n                type=ThoughtType.LEARNING,\n                content=f\"Learned from test results: {test_result['passed']} passed, {test_result['failed']} failed\",\n                confidence=0.9,\n                tags=[\"testing\", \"self_improvement\"]\n            )\n            self.add_thought(learning_thought)\n            \n            return test_result\n        \n        return None\n    \n    async def request_help(self, problem: Dict[str, Any], recipient: str = \"architect\"):\n        \"\"\"Request help from architect or other specialists\"\"\"\n        # Generate analysis thought about the problem\n        analysis_thought = Thought(\n            type=ThoughtType.ANALYSIS,\n            content=f\"Analyzed problem requiring help: {problem.get('description', '')}\",\n            confidence=0.7,\n            tags=[\"help_request\"]\n        )\n        self.add_thought(analysis_thought)\n        \n        # Send help request\n        return await self.send_message(\n            recipient,\n            MessageType.HELP_REQUEST,\n            {\n                \"problem\": problem,\n                \"context\": {\n                    \"expertise\": [e.value for e in self.expertise],\n                    \"current_task\": self.current_tasks[0].dict() if self.current_tasks else None,\n                    \"relevant_thoughts\": [\n                        t.dict() for t in list(self.thoughts)[-5:] \n                        if any(tag in t.tags for tag in problem.get(\"tags\", []))\n                    ]\n                }\n            }\n        )\n    \n    async def get_relevant_thoughts(self, query: str, max_thoughts: int = 5) -> List[Thought]:\n        \"\"\"Retrieve relevant thoughts based on a query\"\"\"\n        # Simple keyword matching for now\n        keywords = set(query.lower().split())\n        relevant_thoughts = []\n        \n        # Find thoughts related to the keywords\n        for keyword in keywords:\n            if keyword in self.thought_patterns:\n                relevant_thoughts.extend(self.thought_patterns[keyword])\n        \n        # Remove duplicates and sort by timestamp (newest first)\n        unique_thoughts = {}\n        for thought in relevant_thoughts:\n            if thought not in unique_thoughts:\n                unique_thoughts[thought] = thought\n        \n        sorted_thoughts = sorted(\n            unique_thoughts.values(), \n            key=lambda t: t.timestamp, \n            reverse=True\n        )\n        \n        return sorted_thoughts[:max_thoughts]\n    \n    async def create_context_for_llm(self, query: str, task: Optional[Task] = None) -> Dict[str, Any]:\n        \"\"\"Create a context for the LLM based on the current state and query\"\"\"\n        # Get relevant thoughts\n        relevant_thoughts = await self.get_relevant_thoughts(query)\n        \n        # Create a lightweight context\n        context = {\n            \"query\": query,\n            \"expertise\": [e.value for e in self.expertise],\n            \"current_task\": task.dict() if task else None,\n            \"relevant_thoughts\": [t.dict() for t in relevant_thoughts],\n            \"knowledge_summary\": self._summarize_knowledge_for_query(query)\n        }\n        \n        return context\n    \n    def _summarize_knowledge_for_query(self, query: str) -> Dict[str, str]:\n        \"\"\"Summarize relevant knowledge base entries for a query\"\"\"\n        # Simple placeholder implementation\n        # In a real system, this would use the LLM to find and summarize relevant knowledge\n        summaries = {}\n        keywords = set(query.lower().split())\n        \n        for key, entry in self.knowledge_base.items():\n            content = entry.get(\"content\", \"\")\n            # Very simple matching - would be more sophisticated in real implementation\n            if any(keyword in content.lower() for keyword in keywords):\n                summaries[key] = f\"Summary of {key}\"\n        \n        return summaries\n    \n    async def get_command_recommendation(self, task: Task) -> str:\n        \"\"\"Get a command recommendation for a task from the LLM\"\"\"\n        # Create context for the LLM\n        context = await self.create_context_for_llm(task.description, task)\n        \n        # In a real implementation, this would use the LLM to generate a command\n        # For now, return a placeholder\n        return f\"echo 'This is a placeholder command for task {task.id}'\"\n    \n    async def run(self):\n        \"\"\"Main agent loop\"\"\"\n        await self.initialize()\n        \n        while True:\n            # Process messages\n            messages = await self.check_messages()\n            for message in messages:\n                await self.process_message(message)\n            \n            # Work on current tasks\n            for task in self.current_tasks:\n                # Generate thought about working on the task\n                working_thought = Thought(\n                    type=ThoughtType.OBSERVATION,\n                    content=f\"Working on task {task.id}: {task.title}\",\n                    confidence=1.0,\n                    tags=[f\"task_{task.id}\"]\n                )\n                self.add_thought(working_thought)\n                \n                # Get command recommendation\n                command = await self.get_command_recommendation(task)\n                \n                # Execute command\n                record = await self.execute_command(command, \"target_server\")\n                \n                # Check if task is complete\n                if record.exit_code == 0:\n                    # Simple condition for now\n                    # In a real implementation, this would be more sophisticated\n                    if \"success\" in record.stdout.lower():\n                        await self.complete_task(task.id, \"completed\", {\"output\": record.stdout})\n                else:\n                    # Command failed\n                    # Request help if needed\n                    await self.request_help({\n                        \"description\": f\"Command failed: {command}\",\n                        \"error\": record.stderr,\n                        \"tags\": [f\"task_{task.id}\", \"error\"]\n                    })\n            \n            # Run tests if no current tasks\n            if not self.current_tasks:\n                await self.run_tests()\n            \n            # Sleep to avoid busy waiting\n            await asyncio.sleep(1)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}