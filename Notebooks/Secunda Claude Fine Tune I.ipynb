{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport logging\nimport asyncio\nfrom typing import List, Dict, Union, Optional\n\nimport torch\nimport torch.nn as nn\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    TrainingArguments, \n    Trainer, \n    DataCollatorForLanguageModeling\n)\nfrom peft import (\n    LoraConfig, \n    get_peft_model, \n    prepare_model_for_kbit_training,\n    PeftModel,\n    PeftConfig\n)\nfrom datasets import Dataset, load_dataset\n\nclass LocalLLMFineTuner:\n    \"\"\"\n    Comprehensive fine-tuning manager for local language models\n    Supports multiple fine-tuning strategies:\n    1. Full parameter fine-tuning\n    2. LoRA (Low-Rank Adaptation)\n    3. Quantized fine-tuning\n    \"\"\"\n    \n    def __init__(\n        self, \n        model_name: str = \"microsoft/Phi-4-mini-instruct\", \n        device: Optional[str] = None,\n        logging_dir: str = \"./logs\"\n    ):\n        \"\"\"\n        Initialize the fine-tuning manager\n        \n        Args:\n            model_name: HuggingFace model identifier\n            device: Device to run the model on (None for auto-detection)\n            logging_dir: Directory for storing training logs\n        \"\"\"\n        self.model_name = model_name\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.logging_dir = logging_dir\n        \n        # Setup logging\n        os.makedirs(logging_dir, exist_ok=True)\n        logging.basicConfig(\n            level=logging.INFO,\n            format='%(asctime)s - %(levelname)s - %(message)s',\n            handlers=[\n                logging.FileHandler(os.path.join(logging_dir, \"finetuning.log\")),\n                logging.StreamHandler()\n            ]\n        )\n        self.logger = logging.getLogger(__name__)\n        \n        # Model and tokenizer placeholders\n        self.original_model = None\n        self.tokenizer = None\n        self.fine_tuned_model = None\n    \n    def load_model(self, quantize: bool = False):\n        \"\"\"\n        Load the base model and tokenizer\n        \n        Args:\n            quantize: Whether to load the model in a quantized format\n        \"\"\"\n        self.logger.info(f\"Loading model {self.model_name}\")\n        \n        # Load tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        # Load model\n        load_kwargs = {\n            \"torch_dtype\": torch.float16 if self.device == \"cuda\" else torch.float32,\n            \"device_map\": self.device\n        }\n        \n        if quantize:\n            load_kwargs.update({\n                \"load_in_4bit\": True,\n                \"quantization_config\": BitsAndBytesConfig(\n                    load_in_4bit=True,\n                    bnb_4bit_compute_dtype=torch.float16,\n                    bnb_4bit_quant_type=\"nf4\",\n                    bnb_4bit_use_double_quant=True\n                )\n            })\n        \n        self.original_model = AutoModelForCausalLM.from_pretrained(\n            self.model_name, \n            **load_kwargs\n        )\n        \n        self.logger.info(\"Model loaded successfully\")\n    \n    def prepare_dataset(\n        self, \n        train_data: Union[str, List[Dict], Dataset], \n        text_column: str = \"text\"\n    ) -> Dataset:\n        \"\"\"\n        Prepare dataset for fine-tuning\n        \n        Args:\n            train_data: Training data source (file path, list of dicts, or existing dataset)\n            text_column: Column name containing text data\n        \n        Returns:\n            Processed dataset\n        \"\"\"\n        # Handle different input types\n        if isinstance(train_data, str):\n            # Attempt to load from file or HuggingFace dataset\n            try:\n                dataset = load_dataset(train_data)\n            except:\n                # Assume it's a local file path\n                dataset = load_dataset(\"text\", data_files=train_data)\n        elif isinstance(train_data, list):\n            dataset = Dataset.from_list(train_data)\n        elif isinstance(train_data, Dataset):\n            dataset = train_data\n        else:\n            raise ValueError(\"Unsupported dataset type\")\n        \n        # Tokenize dataset\n        def tokenize_function(examples):\n            return self.tokenizer(\n                examples[text_column], \n                truncation=True, \n                padding=\"max_length\", \n                max_length=512\n            )\n        \n        tokenized_dataset = dataset.map(\n            tokenize_function, \n            batched=True, \n            remove_columns=dataset.column_names\n        )\n        \n        return tokenized_dataset\n    \n    def setup_lora(\n        self, \n        r: int = 16, \n        lora_alpha: int = 32, \n        lora_dropout: float = 0.1\n    ):\n        \"\"\"\n        Setup LoRA configuration for efficient fine-tuning\n        \n        Args:\n            r: Rank of the low-rank adaptation\n            lora_alpha: Scaling factor for LoRA\n            lora_dropout: Dropout probability\n        \"\"\"\n        if not self.original_model:\n            self.load_model()\n        \n        # Prepare model for LoRA training\n        self.original_model = prepare_model_for_kbit_training(self.original_model)\n        \n        # LoRA configuration\n        lora_config = LoraConfig(\n            r=r,\n            lora_alpha=lora_alpha,\n            target_modules=[\"q_proj\", \"v_proj\"],\n            lora_dropout=lora_dropout,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\"\n        )\n        \n        # Create LoRA model\n        self.fine_tuned_model = get_peft_model(self.original_model, lora_config)\n        \n        self.logger.info(\"LoRA configuration completed\")\n    \n    def fine_tune(\n        self, \n        train_data: Union[str, List[Dict], Dataset],\n        method: str = \"lora\",\n        epochs: int = 3,\n        batch_size: int = 4,\n        learning_rate: float = 2e-5\n    ):\n        \"\"\"\n        Fine-tune the model using specified method\n        \n        Args:\n            train_data: Training data\n            method: Fine-tuning method ('lora', 'full', 'quantized')\n            epochs: Number of training epochs\n            batch_size: Training batch size\n            learning_rate: Learning rate for training\n        \"\"\"\n        # Prepare dataset\n        tokenized_dataset = self.prepare_dataset(train_data)\n        \n        # Split dataset\n        train_dataset = tokenized_dataset.shuffle(seed=42).select(range(int(len(tokenized_dataset)*0.8)))\n        eval_dataset = tokenized_dataset.shuffle(seed=42).select(range(int(len(tokenized_dataset)*0.2)))\n        \n        # Prepare model based on method\n        if method == \"lora\":\n            self.setup_lora()\n            model_to_train = self.fine_tuned_model\n        elif method == \"quantized\":\n            self.load_model(quantize=True)\n            model_to_train = self.original_model\n        else:  # Full fine-tuning\n            if not self.original_model:\n                self.load_model()\n            model_to_train = self.original_model\n        \n        # Training arguments\n        training_args = TrainingArguments(\n            output_dir=os.path.join(self.logging_dir, \"checkpoints\"),\n            num_train_epochs=epochs,\n            per_device_train_batch_size=batch_size,\n            per_device_eval_batch_size=batch_size,\n            learning_rate=learning_rate,\n            warmup_steps=100,\n            logging_dir=self.logging_dir,\n            logging_steps=10,\n            evaluation_strategy=\"epoch\",\n            save_strategy=\"epoch\",\n            load_best_model_at_end=True\n        )\n        \n        # Data collator\n        data_collator = DataCollatorForLanguageModeling(\n            tokenizer=self.tokenizer, \n            mlm=False\n        )\n        \n        # Trainer\n        trainer = Trainer(\n            model=model_to_train,\n            args=training_args,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            data_collator=data_collator\n        )\n        \n        # Train\n        trainer.train()\n        \n        # Save model\n        if method == \"lora\":\n            self.fine_tuned_model.save_pretrained(\n                os.path.join(self.logging_dir, \"lora_model\")\n            )\n        else:\n            model_to_train.save_pretrained(\n                os.path.join(self.logging_dir, \"fine_tuned_model\")\n            )\n        \n        self.logger.info(f\"Fine-tuning completed using {method} method\")\n    \n    def log_training_metrics(self, trainer):\n        \"\"\"\n        Log training metrics and performance\n        \n        Args:\n            trainer: Trainer object from fine-tuning process\n        \"\"\"\n        # Extract and log key metrics\n        metrics = trainer.evaluate()\n        loss = metrics.get('eval_loss', None)\n        perplexity = math.exp(loss) if loss else None\n        \n        log_data = {\n            \"method\": \"lora\",  # or other method\n            \"loss\": loss,\n            \"perplexity\": perplexity,\n            \"training_time\": metrics.get('train_runtime', None),\n            \"samples_per_second\": metrics.get('train_samples_per_second', None)\n        }\n        \n        # Write to log file\n        with open(os.path.join(self.logging_dir, \"training_metrics.json\"), \"w\") as f:\n            json.dump(log_data, f, indent=4)\n        \n        self.logger.info(\"Training metrics logged successfully\")\n    \n    def load_fine_tuned_model(self, model_path: str, method: str = \"lora\"):\n        \"\"\"\n        Load a previously fine-tuned model\n        \n        Args:\n            model_path: Path to the fine-tuned model\n            method: Fine-tuning method used\n        \"\"\"\n        if not self.original_model:\n            self.load_model()\n        \n        if method == \"lora\":\n            # Load LoRA model\n            self.fine_tuned_model = PeftModel.from_pretrained(\n                self.original_model, \n                model_path\n            )\n        else:\n            # Load full fine-tuned model\n            self.fine_tuned_model = AutoModelForCausalLM.from_pretrained(\n                model_path,\n                device_map=self.device\n            )\n        \n        self.logger.info(f\"Fine-tuned model loaded from {model_path}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    # Initialize fine-tuner\n    fine_tuner = LocalLLMFineTuner(\n        model_name=\"microsoft/Phi-4-mini-instruct\"\n    )\n    \n    # Prepare your training data\n    training_data = [\n        {\"text\": \"Your first training example\"},\n        {\"text\": \"Another training example\"}\n    ]\n    \n    # Fine-tune using LoRA\n    fine_tuner.fine_tune(\n        train_data=training_data, \n        method=\"lora\", \n        epochs=3\n    )\n    \n    # Alternatively, full fine-tuning\n    fine_tuner.fine_tune(\n        train_data=training_data, \n        method=\"full\", \n        epochs=3\n    )","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}