{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nTesting & Evaluation System for Multi-Agent Architecture\nThis module provides comprehensive testing and evaluation capabilities for measuring\nagent performance and improvement over time.\n\"\"\"\n\nimport json\nimport time\nimport asyncio\nimport logging\nfrom enum import Enum\nfrom typing import Dict, List, Any, Optional, Set, Tuple\nfrom datetime import datetime\nfrom pydantic import BaseModel, Field\nfrom collections import defaultdict\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(\"testing_evaluation\")\n\n# ============================================================================\n# Data Models\n# ============================================================================\n\nclass MetricType(str, Enum):\n    ACCURACY = \"accuracy\"\n    EFFICIENCY = \"efficiency\"\n    LEARNING_RATE = \"learning_rate\"\n    ERROR_RECOVERY = \"error_recovery\"\n    CONTEXT_UTILIZATION = \"context_utilization\"\n    PATTERN_RECOGNITION = \"pattern_recognition\"\n    COLLABORATION = \"collaboration\"\n\n\nclass TestResult(BaseModel):\n    \"\"\"Result of a single test case execution\"\"\"\n    test_id: str\n    agent_id: str\n    timestamp: datetime = Field(default_factory=datetime.now)\n    success: bool\n    execution_time: float\n    metrics: Dict[str, float]\n    command: str\n    output: str\n    thoughts: List[Dict[str, Any]] = []\n    context_size: int\n    errors: List[str] = []\n\n\nclass AgentMetricSnapshot(BaseModel):\n    \"\"\"Snapshot of all metrics for an agent at a point in time\"\"\"\n    agent_id: str\n    timestamp: datetime = Field(default_factory=datetime.now)\n    metrics: Dict[str, float]\n    recent_tests: List[str] = []\n    improvement_rate: Dict[str, float] = {}\n    \n\nclass TestScenario(BaseModel):\n    \"\"\"A scenario for testing agents with specific conditions\"\"\"\n    scenario_id: str\n    name: str\n    description: str\n    difficulty_level: int = 1\n    required_skills: Set[str] = set()\n    test_cases: List[Dict[str, Any]] = []\n    evaluation_criteria: Dict[str, float] = {}\n\n\nclass LearningAssessment(BaseModel):\n    \"\"\"Assessment of an agent's learning progress over time\"\"\"\n    agent_id: str\n    start_date: datetime\n    end_date: datetime\n    metrics_trend: Dict[str, List[Tuple[datetime, float]]] = {}\n    skill_improvements: Dict[str, float] = {}\n    identified_patterns: int\n    learning_efficiency: float\n    strengths: List[str] = []\n    improvement_areas: List[str] = []\n\n\n# ============================================================================\n# Test Runner\n# ============================================================================\n\nclass TestRunner:\n    \"\"\"Executes test scenarios and captures results\"\"\"\n    \n    def __init__(self, db_connector=None):\n        self.scenarios = {}\n        self.results = []\n        self.db = db_connector\n        self.baseline_metrics = defaultdict(dict)\n        \n    async def register_scenario(self, scenario: TestScenario) -> None:\n        \"\"\"Register a new test scenario\"\"\"\n        self.scenarios[scenario.scenario_id] = scenario\n        logger.info(f\"Registered scenario: {scenario.name}\")\n        \n    async def run_test(self, scenario_id: str, agent, context=None) -> List[TestResult]:\n        \"\"\"Run all test cases in a scenario against an agent\"\"\"\n        if scenario_id not in self.scenarios:\n            raise ValueError(f\"Unknown scenario ID: {scenario_id}\")\n            \n        scenario = self.scenarios[scenario_id]\n        results = []\n        \n        logger.info(f\"Running scenario '{scenario.name}' on agent {agent.agent_id}\")\n        \n        for test_case in scenario.test_cases:\n            start_time = time.time()\n            \n            # Capture initial context size\n            initial_context_size = len(json.dumps(agent.context)) if hasattr(agent, \"context\") else 0\n            \n            try:\n                # Execute test against agent\n                success, output, thoughts = await agent.execute_command(\n                    test_case[\"command\"], \n                    context=context\n                )\n                \n                # Calculate metrics\n                metrics = self._calculate_metrics(agent, test_case, success, output, thoughts)\n                execution_time = time.time() - start_time\n                \n                # Create result\n                result = TestResult(\n                    test_id=test_case[\"id\"],\n                    agent_id=agent.agent_id,\n                    success=success,\n                    execution_time=execution_time,\n                    metrics=metrics,\n                    command=test_case[\"command\"],\n                    output=output,\n                    thoughts=thoughts,\n                    context_size=initial_context_size,\n                    errors=[]\n                )\n                \n            except Exception as e:\n                execution_time = time.time() - start_time\n                result = TestResult(\n                    test_id=test_case[\"id\"],\n                    agent_id=agent.agent_id,\n                    success=False,\n                    execution_time=execution_time,\n                    metrics={},\n                    command=test_case[\"command\"],\n                    output=\"\",\n                    context_size=initial_context_size,\n                    errors=[str(e)]\n                )\n                logger.error(f\"Test failed: {str(e)}\")\n            \n            results.append(result)\n            self.results.append(result)\n            \n            # Store result if db connection available\n            if self.db:\n                await self.db.store_test_result(result)\n                \n        return results\n    \n    def _calculate_metrics(self, agent, test_case, success, output, thoughts) -> Dict[str, float]:\n        \"\"\"Calculate various performance metrics based on test execution\"\"\"\n        metrics = {}\n        \n        # Basic success metric\n        metrics[MetricType.ACCURACY] = 1.0 if success else 0.0\n        \n        # Efficiency - compare to expected execution time if available\n        if \"expected_time\" in test_case:\n            efficiency = test_case[\"expected_time\"] / (time.time() - agent.start_time)\n            metrics[MetricType.EFFICIENCY] = min(1.0, efficiency)  # Cap at 1.0\n            \n        # Context utilization - measure how effectively the agent uses context\n        if hasattr(agent, \"context\") and agent.context:\n            # Ratio of relevant context items used vs total context size\n            relevant_items = sum(1 for thought in thoughts if thought.get(\"context_items_used\", 0) > 0)\n            total_items = len(agent.context)\n            metrics[MetricType.CONTEXT_UTILIZATION] = relevant_items / max(1, total_items)\n            \n        # Pattern recognition - based on thought patterns identified\n        pattern_count = sum(1 for thought in thoughts if \"pattern\" in thought)\n        metrics[MetricType.PATTERN_RECOGNITION] = min(1.0, pattern_count / 5.0)  # Normalize to 0-1\n        \n        # Add more metric calculations as needed\n        \n        return metrics\n\n\n# ============================================================================\n# Evaluation System\n# ============================================================================\n\nclass EvaluationSystem:\n    \"\"\"Analyzes test results and generates improvement recommendations\"\"\"\n    \n    def __init__(self, db_connector=None):\n        self.db = db_connector\n        self.baseline_metrics = {}\n        self.snapshots = []\n        \n    async def create_agent_snapshot(self, agent_id: str, test_results: List[TestResult]) -> AgentMetricSnapshot:\n        \"\"\"Create a performance snapshot for an agent based on recent test results\"\"\"\n        if not test_results:\n            raise ValueError(\"No test results provided for snapshot creation\")\n            \n        # Aggregate metrics across test results\n        aggregated_metrics = defaultdict(list)\n        for result in test_results:\n            for metric_name, value in result.metrics.items():\n                aggregated_metrics[metric_name].append(value)\n        \n        # Calculate average for each metric\n        metrics = {\n            metric_name: sum(values) / len(values) \n            for metric_name, values in aggregated_metrics.items()\n        }\n        \n        # Get previous snapshot for improvement calculation\n        previous_snapshot = None\n        if self.db:\n            previous_snapshot = await self.db.get_latest_snapshot(agent_id)\n        \n        # Calculate improvement rates if previous snapshot exists\n        improvement_rate = {}\n        if previous_snapshot:\n            for metric_name, current_value in metrics.items():\n                if metric_name in previous_snapshot.metrics:\n                    previous_value = previous_snapshot.metrics[metric_name]\n                    if previous_value > 0:\n                        improvement_rate[metric_name] = (current_value - previous_value) / previous_value\n        \n        # Create snapshot\n        snapshot = AgentMetricSnapshot(\n            agent_id=agent_id,\n            metrics=metrics,\n            recent_tests=[result.test_id for result in test_results],\n            improvement_rate=improvement_rate\n        )\n        \n        # Store snapshot\n        self.snapshots.append(snapshot)\n        if self.db:\n            await self.db.store_agent_snapshot(snapshot)\n            \n        return snapshot\n        \n    async def assess_learning(self, agent_id: str, start_date: datetime, end_date: datetime) -> LearningAssessment:\n        \"\"\"Assess learning progress for an agent over a time period\"\"\"\n        if not self.db:\n            raise ValueError(\"Database connection required for learning assessment\")\n            \n        # Fetch all snapshots for the specified period\n        snapshots = await self.db.get_agent_snapshots(\n            agent_id, \n            start_date=start_date, \n            end_date=end_date\n        )\n        \n        if not snapshots:\n            raise ValueError(f\"No snapshots found for agent {agent_id} in the specified period\")\n            \n        # Calculate metric trends\n        metrics_trend = defaultdict(list)\n        for snapshot in snapshots:\n            for metric_name, value in snapshot.metrics.items():\n                metrics_trend[metric_name].append((snapshot.timestamp, value))\n        \n        # Calculate skill improvements\n        first_snapshot = snapshots[0]\n        last_snapshot = snapshots[-1]\n        \n        skill_improvements = {}\n        for metric_name, end_value in last_snapshot.metrics.items():\n            if metric_name in first_snapshot.metrics:\n                start_value = first_snapshot.metrics[metric_name]\n                if start_value > 0:\n                    skill_improvements[metric_name] = end_value - start_value\n        \n        # Get pattern recognition data\n        pattern_data = await self.db.get_agent_pattern_data(agent_id, start_date, end_date)\n        identified_patterns = pattern_data.get(\"identified_patterns\", 0)\n        \n        # Calculate learning efficiency\n        # (Improvement per test case over time)\n        total_tests = await self.db.count_agent_tests(agent_id, start_date, end_date)\n        avg_improvement = sum(skill_improvements.values()) / max(1, len(skill_improvements))\n        learning_efficiency = avg_improvement / max(1, total_tests)\n        \n        # Identify strengths and improvement areas\n        strengths = []\n        improvement_areas = []\n        \n        for metric_name, improvement in skill_improvements.items():\n            if improvement > 0.1:  # Significant improvement\n                strengths.append(metric_name)\n            elif improvement < 0.05:  # Little improvement\n                improvement_areas.append(metric_name)\n        \n        # Create assessment\n        assessment = LearningAssessment(\n            agent_id=agent_id,\n            start_date=start_date,\n            end_date=end_date,\n            metrics_trend=dict(metrics_trend),\n            skill_improvements=skill_improvements,\n            identified_patterns=identified_patterns,\n            learning_efficiency=learning_efficiency,\n            strengths=strengths,\n            improvement_areas=improvement_areas\n        )\n        \n        return assessment\n    \n    async def generate_recommendations(self, assessment: LearningAssessment) -> Dict[str, Any]:\n        \"\"\"Generate improvement recommendations based on learning assessment\"\"\"\n        recommendations = {\n            \"priority_skills\": [],\n            \"learning_exercises\": [],\n            \"pattern_focus\": [],\n            \"collaboration_suggestions\": []\n        }\n        \n        # Prioritize improvement areas\n        for area in assessment.improvement_areas:\n            recommendations[\"priority_skills\"].append({\n                \"skill\": area,\n                \"current_level\": assessment.skill_improvements.get(area, 0),\n                \"target_improvement\": 0.2\n            })\n        \n        # Generate learning exercises based on current skills\n        for area in assessment.improvement_areas[:2]:  # Focus on top 2 areas\n            exercises = await self._generate_exercises_for_skill(area)\n            recommendations[\"learning_exercises\"].extend(exercises)\n        \n        # Pattern recognition recommendations\n        if MetricType.PATTERN_RECOGNITION in assessment.improvement_areas:\n            recommendations[\"pattern_focus\"] = [\n                \"Increase focus on identifying command output patterns\",\n                \"Practice with more varied command sequences\",\n                \"Implement pattern matching for interactive commands\"\n            ]\n        \n        # Collaboration recommendations\n        if MetricType.COLLABORATION in assessment.improvement_areas:\n            recommendations[\"collaboration_suggestions\"] = [\n                \"Increase interaction frequency with specialists\",\n                \"Implement proactive collaboration mode\",\n                \"Practice information sharing with other programmer agents\"\n            ]\n        \n        return recommendations\n    \n    async def _generate_exercises_for_skill(self, skill_area: str) -> List[Dict[str, Any]]:\n        \"\"\"Generate specific learning exercises for a skill area\"\"\"\n        exercises = []\n        \n        if skill_area == MetricType.PATTERN_RECOGNITION:\n            exercises = [\n                {\n                    \"name\": \"Pattern Identification Exercise\",\n                    \"description\": \"Analyze 20 command outputs and identify recurring patterns\",\n                    \"difficulty\": \"Medium\",\n                    \"expected_improvement\": 0.15\n                },\n                {\n                    \"name\": \"Pattern Categorization\",\n                    \"description\": \"Categorize identified patterns into system states\",\n                    \"difficulty\": \"Hard\",\n                    \"expected_improvement\": 0.2\n                }\n            ]\n        elif skill_area == MetricType.ERROR_RECOVERY:\n            exercises = [\n                {\n                    \"name\": \"Error Recovery Simulation\",\n                    \"description\": \"Practice recovering from 10 common error scenarios\",\n                    \"difficulty\": \"Medium\",\n                    \"expected_improvement\": 0.18\n                }\n            ]\n        # Add more skill-specific exercises\n        \n        return exercises\n\n\n# ============================================================================\n# Skill Development Tracking\n# ============================================================================\n\nclass SkillTracker:\n    \"\"\"Tracks the development of specific skills over time\"\"\"\n    \n    def __init__(self, db_connector=None):\n        self.db = db_connector\n        self.skill_definitions = {}\n        \n    async def define_skill(self, skill_id: str, name: str, metrics: List[str], \n                           thresholds: Dict[str, float]) -> None:\n        \"\"\"Define a skill to track based on metrics and thresholds\"\"\"\n        self.skill_definitions[skill_id] = {\n            \"name\": name,\n            \"metrics\": metrics,\n            \"thresholds\": thresholds\n        }\n        \n        if self.db:\n            await self.db.store_skill_definition(skill_id, name, metrics, thresholds)\n    \n    async def assess_skill_level(self, agent_id: str, skill_id: str, \n                                 recent_results: List[TestResult]) -> float:\n        \"\"\"Assess current skill level for an agent\"\"\"\n        if skill_id not in self.skill_definitions:\n            raise ValueError(f\"Unknown skill ID: {skill_id}\")\n            \n        skill_def = self.skill_definitions[skill_id]\n        \n        # Extract relevant metrics from test results\n        relevant_metrics = defaultdict(list)\n        for result in recent_results:\n            for metric in skill_def[\"metrics\"]:\n                if metric in result.metrics:\n                    relevant_metrics[metric].append(result.metrics[metric])\n        \n        # Calculate average for each metric\n        avg_metrics = {\n            metric: sum(values) / len(values) \n            for metric, values in relevant_metrics.items() \n            if values\n        }\n        \n        # Calculate overall skill level (0.0 to 1.0)\n        # Weight each metric based on thresholds\n        skill_level = 0.0\n        total_weight = 0.0\n        \n        for metric, threshold in skill_def[\"thresholds\"].items():\n            if metric in avg_metrics:\n                weight = 1.0\n                total_weight += weight\n                normalized_value = min(1.0, avg_metrics[metric] / threshold)\n                skill_level += normalized_value * weight\n        \n        if total_weight > 0:\n            skill_level /= total_weight\n        \n        # Store skill assessment\n        if self.db:\n            await self.db.store_skill_assessment(\n                agent_id, skill_id, skill_level, datetime.now()\n            )\n            \n        return skill_level\n    \n    async def get_skill_history(self, agent_id: str, skill_id: str) -> List[Tuple[datetime, float]]:\n        \"\"\"Get historical skill levels for an agent\"\"\"\n        if not self.db:\n            raise ValueError(\"Database connection required for skill history\")\n            \n        return await self.db.get_skill_history(agent_id, skill_id)\n\n\n# ============================================================================\n# Progression System\n# ============================================================================\n\nclass ProgressionSystem:\n    \"\"\"Manages agent progression through increasingly difficult test scenarios\"\"\"\n    \n    def __init__(self, test_runner: TestRunner, evaluation: EvaluationSystem, \n                 skill_tracker: SkillTracker):\n        self.test_runner = test_runner\n        self.evaluation = evaluation\n        self.skill_tracker = skill_tracker\n        self.progression_paths = {}\n        self.agent_levels = defaultdict(int)\n        \n    async def define_progression_path(self, path_id: str, name: str, \n                                      levels: List[Dict[str, Any]]) -> None:\n        \"\"\"Define a progression path with multiple difficulty levels\"\"\"\n        self.progression_paths[path_id] = {\n            \"name\": name,\n            \"levels\": levels\n        }\n        \n    async def get_next_scenario(self, agent_id: str, path_id: str) -> Optional[str]:\n        \"\"\"Get the next appropriate test scenario for an agent based on current level\"\"\"\n        if path_id not in self.progression_paths:\n            raise ValueError(f\"Unknown progression path: {path_id}\")\n            \n        current_level = self.agent_levels.get(agent_id, 0)\n        path = self.progression_paths[path_id]\n        \n        if current_level >= len(path[\"levels\"]):\n            return None  # Agent has completed all levels\n            \n        level_info = path[\"levels\"][current_level]\n        return level_info[\"scenario_id\"]\n    \n    async def evaluate_advancement(self, agent_id: str, path_id: str, \n                                   test_results: List[TestResult]) -> bool:\n        \"\"\"Evaluate if an agent should advance to the next level\"\"\"\n        if path_id not in self.progression_paths:\n            raise ValueError(f\"Unknown progression path: {path_id}\")\n            \n        current_level = self.agent_levels.get(agent_id, 0)\n        path = self.progression_paths[path_id]\n        \n        if current_level >= len(path[\"levels\"]):\n            return False  # Already at max level\n            \n        level_info = path[\"levels\"][current_level]\n        \n        # Check advancement criteria\n        success_rate = sum(1 for r in test_results if r.success) / max(1, len(test_results))\n        \n        # Check required skill levels\n        skills_met = True\n        for skill_id, required_level in level_info.get(\"required_skills\", {}).items():\n            current_skill = await self.skill_tracker.assess_skill_level(\n                agent_id, skill_id, test_results\n            )\n            if current_skill < required_level:\n                skills_met = False\n                break\n        \n        # Determine if agent should advance\n        should_advance = (\n            success_rate >= level_info.get(\"required_success_rate\", 0.8) and\n            skills_met\n        )\n        \n        if should_advance:\n            self.agent_levels[agent_id] = current_level + 1\n            \n        return should_advance\n\n\n# ============================================================================\n# Example Usage\n# ============================================================================\n\nasync def example_usage():\n    \"\"\"Example of how to use the testing and evaluation system\"\"\"\n    \n    # Create components\n    test_runner = TestRunner()\n    evaluation = EvaluationSystem()\n    skill_tracker = SkillTracker()\n    progression = ProgressionSystem(test_runner, evaluation, skill_tracker)\n    \n    # Define a test scenario\n    basic_commands = TestScenario(\n        scenario_id=\"basic_linux_commands\",\n        name=\"Basic Linux Commands\",\n        description=\"Tests basic Linux command execution and output processing\",\n        difficulty_level=1,\n        required_skills={\"command_execution\", \"output_processing\"},\n        test_cases=[\n            {\n                \"id\": \"ls_test\",\n                \"command\": \"ls -la\",\n                \"expected_success\": True,\n                \"expected_time\": 0.5\n            },\n            {\n                \"id\": \"grep_test\",\n                \"command\": \"grep 'error' /var/log/syslog\",\n                \"expected_success\": True,\n                \"expected_time\": 1.0\n            }\n        ],\n        evaluation_criteria={\n            MetricType.ACCURACY: 0.8,\n            MetricType.EFFICIENCY: 0.7\n        }\n    )\n    \n    # Register scenario\n    await test_runner.register_scenario(basic_commands)\n    \n    # Define skills\n    await skill_tracker.define_skill(\n        \"command_execution\",\n        \"Linux Command Execution\",\n        [MetricType.ACCURACY, MetricType.EFFICIENCY],\n        {MetricType.ACCURACY: 0.8, MetricType.EFFICIENCY: 0.7}\n    )\n    \n    # Define progression path\n    await progression.define_progression_path(\n        \"linux_mastery\",\n        \"Linux Command Mastery\",\n        [\n            {\n                \"level\": 1,\n                \"scenario_id\": \"basic_linux_commands\",\n                \"required_success_rate\": 0.8,\n                \"required_skills\": {\"command_execution\": 0.7}\n            },\n            {\n                \"level\": 2,\n                \"scenario_id\": \"intermediate_linux_commands\",\n                \"required_success_rate\": 0.75,\n                \"required_skills\": {\"command_execution\": 0.8, \"output_processing\": 0.7}\n            }\n        ]\n    )\n    \n    # The rest would be implemented with actual agent instances\n\n\nif __name__ == \"__main__\":\n    asyncio.run(example_usage())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}