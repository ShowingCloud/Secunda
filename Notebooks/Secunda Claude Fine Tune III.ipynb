{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Data structures\nclass Task:\n    id: int\n    primary_lora: str                  # Main LoRA needed for this task\n    preprocessing_lora: Optional[str]  # LoRA needed for preprocessing (if any)\n    postprocessing_lora: Optional[str] # LoRA needed for postprocessing (if any)\n    status: Literal[\"pending\", \"preprocessing\", \"processing\", \"postprocessing\", \"completed\"]\n    priority: int                      # Higher number = higher priority\n    estimated_duration: int            # Estimated time units for main processing\n    io_bound: bool                     # Whether task involves I/O waiting periods\n\nclass LoRAModel:\n    name: str\n    loaded: bool        # Whether currently loaded in VRAM\n    last_used: datetime # Last time this LoRA was used\n    task_count: int     # Number of tasks currently assigned\n    compatible_tasks: List[str]  # Task types this LoRA can handle\n    \n# Global state\nbase_model = None       # The base model M\nlora_models = {}        # Map of LoRA name to LoRAModel\nloaded_loras = []       # List of currently loaded LoRA models\ntask_queue = []         # Priority queue of pending tasks\nactive_tasks = {}       # Map of task ID to Task object for in-progress tasks\ncompleted_tasks = []    # List of completed tasks\nvram_capacity = 8       # GB of VRAM available\nbase_model_size = 3     # GB of VRAM for base model\nlora_model_size = 0.2   # GB of VRAM for each LoRA adapter\n\n# Main scheduler loop\ndef scheduler_loop():\n    initialize_system()\n    \n    while True:\n        # Check for completed tasks\n        update_completed_tasks()\n        \n        # Schedule new tasks based on available resources\n        schedule_tasks()\n        \n        # Sleep a bit to avoid CPU spinning\n        time.sleep(0.1)\n\ndef initialize_system():\n    # Load base model\n    load_base_model()\n    \n    # Initialize LoRA models\n    for lora_name in [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]:\n        lora_models[lora_name] = LoRAModel(\n            name=lora_name,\n            loaded=False,\n            last_used=None,\n            task_count=0,\n            compatible_tasks=get_compatible_tasks(lora_name)\n        )\n\ndef update_completed_tasks():\n    # Check if any active tasks have completed\n    for task_id, task in list(active_tasks.items()):\n        if is_task_completed(task_id):\n            task.status = \"completed\"\n            completed_tasks.append(task)\n            del active_tasks[task_id]\n            \n            # Update the LoRA models' task counts\n            if task.primary_lora:\n                lora_models[task.primary_lora].task_count -= 1\n            \n            # Free up LoRAs that are no longer needed\n            manage_loaded_loras()\n\ndef schedule_tasks():\n    # Calculate available VRAM\n    available_vram = calculate_available_vram()\n    \n    # Sort the task queue by priority and dependencies\n    sorted_tasks = sort_task_queue()\n    \n    for task in sorted_tasks:\n        if can_schedule_task(task, available_vram):\n            # Schedule the task\n            start_task(task)\n            \n            # Update available VRAM\n            available_vram = calculate_available_vram()\n            \n            # Task queue is modified inside start_task\n            # Break and allow the loop to restart with updated state\n            break\n\ndef calculate_available_vram():\n    used_vram = base_model_size\n    for lora_name in loaded_loras:\n        used_vram += lora_model_size\n    \n    # Add some overhead and workspace for inference\n    used_vram += 0.5\n    \n    return vram_capacity - used_vram\n\ndef sort_task_queue():\n    # First sort by priority (highest first)\n    sorted_tasks = sorted(task_queue, key=lambda t: t.priority, reverse=True)\n    \n    # Then optimize for already loaded LoRAs\n    def score_task(task):\n        score = 0\n        # Highest score if primary LoRA is already loaded\n        if task.primary_lora in loaded_loras:\n            score += 100\n        # Bonus points if pre/post processing LoRAs are loaded\n        if task.preprocessing_lora in loaded_loras:\n            score += 50\n        if task.postprocessing_lora in loaded_loras:\n            score += 25\n        # Bonus points for I/O bound tasks (can be interleaved)\n        if task.io_bound:\n            score += 10\n        return score\n    \n    return sorted(sorted_tasks, key=score_task, reverse=True)\n\ndef can_schedule_task(task, available_vram):\n    # Check if we need to load a new LoRA\n    loras_to_load = []\n    if task.status == \"pending\" and task.preprocessing_lora:\n        if task.preprocessing_lora not in loaded_loras:\n            loras_to_load.append(task.preprocessing_lora)\n    elif task.status == \"preprocessing\" or task.status == \"pending\" and not task.preprocessing_lora:\n        if task.primary_lora not in loaded_loras:\n            loras_to_load.append(task.primary_lora)\n    elif task.status == \"processing\":\n        if task.postprocessing_lora and task.postprocessing_lora not in loaded_loras:\n            loras_to_load.append(task.postprocessing_lora)\n    \n    # Calculate VRAM needed\n    vram_needed = len(loras_to_load) * lora_model_size\n    \n    return vram_needed <= available_vram\n\ndef start_task(task):\n    # Remove from queue\n    task_queue.remove(task)\n    \n    # Add to active tasks\n    active_tasks[task.id] = task\n    \n    # Update task status based on current state\n    if task.status == \"pending\":\n        if task.preprocessing_lora:\n            load_lora(task.preprocessing_lora)\n            task.status = \"preprocessing\"\n            start_preprocessing(task)\n        else:\n            load_lora(task.primary_lora)\n            task.status = \"processing\"\n            start_processing(task)\n    elif task.status == \"preprocessing\":\n        load_lora(task.primary_lora)\n        task.status = \"processing\"\n        start_processing(task)\n    elif task.status == \"processing\":\n        load_lora(task.postprocessing_lora)\n        task.status = \"postprocessing\"\n        start_postprocessing(task)\n    \n    # Update LoRA model metadata\n    update_lora_metadata(task)\n\ndef load_lora(lora_name):\n    if lora_name in loaded_loras:\n        # Already loaded, just update last_used\n        lora_models[lora_name].last_used = datetime.now()\n        return\n    \n    # Check if we need to unload some LoRAs\n    while len(loaded_loras) * lora_model_size + base_model_size + 0.5 > vram_capacity:\n        unload_least_important_lora()\n    \n    # Load the LoRA\n    print(f\"Loading LoRA {lora_name}\")\n    # [Actual code to load the LoRA model would go here]\n    \n    lora_models[lora_name].loaded = True\n    lora_models[lora_name].last_used = datetime.now()\n    loaded_loras.append(lora_name)\n\ndef unload_least_important_lora():\n    # Don't unload LoRAs that are currently being used\n    active_loras = set()\n    for task in active_tasks.values():\n        if task.status == \"preprocessing\" and task.preprocessing_lora:\n            active_loras.add(task.preprocessing_lora)\n        elif task.status == \"processing\":\n            active_loras.add(task.primary_lora)\n        elif task.status == \"postprocessing\" and task.postprocessing_lora:\n            active_loras.add(task.postprocessing_lora)\n    \n    # Find candidates for unloading (loaded but not active)\n    candidates = [lora for lora in loaded_loras if lora not in active_loras]\n    \n    if not candidates:\n        # All loaded LoRAs are active, we can't unload any\n        # This is a resource constraint issue\n        raise Exception(\"Cannot free VRAM - all loaded LoRAs are in use\")\n    \n    # Find the least recently used LoRA\n    lru_lora = min(candidates, key=lambda l: lora_models[l].last_used)\n    \n    # Unload it\n    print(f\"Unloading LoRA {lru_lora}\")\n    # [Actual code to unload the LoRA model would go here]\n    \n    lora_models[lru_lora].loaded = False\n    loaded_loras.remove(lru_lora)\n\ndef manage_loaded_loras():\n    # Unload LoRAs that haven't been used recently and have no pending tasks\n    current_time = datetime.now()\n    inactive_threshold = timedelta(minutes=5)\n    \n    for lora_name in list(loaded_loras):\n        lora = lora_models[lora_name]\n        if (lora.task_count == 0 and \n            (current_time - lora.last_used) > inactive_threshold):\n            # Unload this LoRA\n            print(f\"Unloading inactive LoRA {lora_name}\")\n            # [Actual code to unload the LoRA model would go here]\n            \n            lora.loaded = False\n            loaded_loras.remove(lora_name)\n\ndef update_lora_metadata(task):\n    # Update task counts\n    if task.status == \"preprocessing\":\n        lora_models[task.preprocessing_lora].task_count += 1\n    elif task.status == \"processing\":\n        # If we just moved from preprocessing to processing\n        if task.preprocessing_lora:\n            lora_models[task.preprocessing_lora].task_count -= 1\n        lora_models[task.primary_lora].task_count += 1\n    elif task.status == \"postprocessing\":\n        lora_models[task.primary_lora].task_count -= 1\n        lora_models[task.postprocessing_lora].task_count += 1\n\n# Implementations for actual task execution\ndef start_preprocessing(task):\n    print(f\"Starting preprocessing for task {task.id} with LoRA {task.preprocessing_lora}\")\n    # [Actual code to start preprocessing would go here]\n    # This would typically be asynchronous\n    \n    # When preprocessing completes, the task should be placed back in the queue\n    # with status=\"preprocessing\" so it can advance to \"processing\"\n    \ndef start_processing(task):\n    print(f\"Starting processing for task {task.id} with LoRA {task.primary_lora}\")\n    # [Actual code to start main processing would go here]\n    # This would typically be asynchronous\n    \n    # If this is an I/O bound task, we should immediately look for other tasks\n    if task.io_bound:\n        # Schedule another task while this one is waiting for I/O\n        schedule_tasks()\n    \n    # When processing completes, if there's postprocessing, the task should be \n    # placed back in the queue with status=\"processing\" to advance to \"postprocessing\"\n    \ndef start_postprocessing(task):\n    print(f\"Starting postprocessing for task {task.id} with LoRA {task.postprocessing_lora}\")\n    # [Actual code to start postprocessing would go here]\n    # This would typically be asynchronous\n\n# Helper function to check if a task has completed its current stage\ndef is_task_completed(task_id):\n    # [Actual code to check task completion would go here]\n    # This would check if the async operation has completed\n    pass\n\n# Example usage\ndef example_main():\n    # Initialize the system\n    initialize_system()\n    \n    # Add some example tasks\n    task_queue.append(Task(\n        id=1,\n        primary_lora=\"A\",\n        preprocessing_lora=\"B\",\n        postprocessing_lora=\"C\",\n        status=\"pending\",\n        priority=10,\n        estimated_duration=5,\n        io_bound=True\n    ))\n    \n    task_queue.append(Task(\n        id=2,\n        primary_lora=\"D\",\n        preprocessing_lora=None,\n        postprocessing_lora=\"E\",\n        status=\"pending\",\n        priority=5,\n        estimated_duration=3,\n        io_bound=False\n    ))\n    \n    # Start the scheduler\n    scheduler_loop()\n\nif __name__ == \"__main__\":\n    example_main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_task(task):\n    # First, use the base LLM to analyze the task\n    software_needed, processing_stages = base_llm.analyze_task(task)\n    \n    if len(processing_stages) == 1:\n        # Simple case - just one software interaction needed\n        lora_name = software_to_lora_map[software_needed[0]]\n        load_lora(lora_name)\n        result = execute_with_lora(task, lora_name)\n        unload_lora(lora_name)\n        return result\n    else:\n        # Complex case - sequence of software interactions\n        results = []\n        for stage in processing_stages:\n            stage_software = stage[\"software\"]\n            stage_operation = stage[\"operation\"]\n            lora_name = software_to_lora_map[stage_software]\n            \n            # Load the appropriate LoRA for this stage\n            load_lora(lora_name)\n            \n            # Execute this stage of processing\n            stage_result = execute_stage_with_lora(\n                task, \n                lora_name, \n                stage_operation, \n                previous_results=results\n            )\n            \n            results.append(stage_result)\n            \n            # Unload this LoRA if it won't be needed soon\n            if not will_need_soon(lora_name):\n                unload_lora(lora_name)\n        \n        # Combine results from all stages\n        return combine_results(results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# LoRA Discovery and Analysis System\n\n# Part 1: Task Pattern Analysis\nclass TaskPatternAnalyzer:\n    def __init__(self, base_llm):\n        self.base_llm = base_llm\n        self.task_history = []\n        self.pattern_clusters = {}\n        self.software_interactions = {}\n        \n    def record_task(self, task, software_used, loras_used, execution_metrics):\n        \"\"\"Record a completed task with its metadata for pattern analysis\"\"\"\n        task_record = {\n            'task_description': task.description,\n            'software_used': software_used,\n            'loras_used': loras_used,\n            'execution_time': execution_metrics['execution_time'],\n            'tokens_generated': execution_metrics['tokens_generated'],\n            'success_rating': execution_metrics['success_rating'],\n            'timestamp': datetime.now()\n        }\n        self.task_history.append(task_record)\n        \n        # Update software interaction counter\n        for software in software_used:\n            if software in self.software_interactions:\n                self.software_interactions[software] += 1\n            else:\n                self.software_interactions[software] = 1\n        \n        # Periodically analyze patterns (e.g., every 100 tasks)\n        if len(self.task_history) % 100 == 0:\n            self.analyze_patterns()\n    \n    def analyze_patterns(self):\n        \"\"\"Identify common patterns in task history that might benefit from new LoRAs\"\"\"\n        # Extract task embeddings using the base LLM\n        task_embeddings = []\n        for task in self.task_history[-500:]:  # Analyze most recent 500 tasks\n            embedding = self.base_llm.get_embedding(task['task_description'])\n            task_embeddings.append({\n                'embedding': embedding,\n                'task': task\n            })\n        \n        # Cluster similar tasks\n        clusters = self.cluster_embeddings(task_embeddings)\n        \n        # Analyze each cluster for LoRA potential\n        for cluster_id, cluster in clusters.items():\n            if len(cluster) >= 50:  # Only consider clusters with significant samples\n                cluster_potential = self.analyze_cluster_for_lora_potential(cluster)\n                if cluster_potential['score'] > 0.7:  # Threshold for recommendation\n                    self.pattern_clusters[cluster_id] = cluster_potential\n    \n    def cluster_embeddings(self, task_embeddings, similarity_threshold=0.85):\n        \"\"\"Cluster task embeddings based on similarity\"\"\"\n        clusters = {}\n        cluster_id = 0\n        \n        for task_embedding in task_embeddings:\n            assigned = False\n            \n            # Check if this task belongs to an existing cluster\n            for cid, cluster in clusters.items():\n                centroid = self.calculate_centroid(cluster)\n                similarity = self.cosine_similarity(centroid, task_embedding['embedding'])\n                \n                if similarity > similarity_threshold:\n                    clusters[cid].append(task_embedding)\n                    assigned = True\n                    break\n            \n            # If not assigned to any cluster, create a new one\n            if not assigned:\n                cluster_id += 1\n                clusters[cluster_id] = [task_embedding]\n        \n        return clusters\n    \n    def analyze_cluster_for_lora_potential(self, cluster):\n        \"\"\"Analyze a cluster to determine if it would benefit from a dedicated LoRA\"\"\"\n        tasks = [item['task'] for item in cluster]\n        \n        # Extract common software interactions\n        software_counts = {}\n        for task in tasks:\n            for software in task['software_used']:\n                if software in software_counts:\n                    software_counts[software] += 1\n                else:\n                    software_counts[software] = 1\n        \n        # Calculate average execution metrics\n        avg_execution_time = sum(task['execution_time'] for task in tasks) / len(tasks)\n        avg_success_rating = sum(task['success_rating'] for task in tasks) / len(tasks)\n        \n        # Check if multiple LoRAs were typically used\n        multi_lora_count = sum(1 for task in tasks if len(task['loras_used']) > 1)\n        multi_lora_percentage = multi_lora_count / len(tasks)\n        \n        # Sample tasks for analysis\n        sample_tasks = random.sample(tasks, min(5, len(tasks)))\n        \n        # Calculate potential score based on criteria\n        potential_score = self.calculate_lora_potential_score(\n            avg_execution_time, \n            avg_success_rating,\n            multi_lora_percentage,\n            len(tasks),\n            software_counts\n        )\n        \n        return {\n            'primary_software': max(software_counts.items(), key=lambda x: x[1])[0],\n            'task_count': len(tasks),\n            'avg_execution_time': avg_execution_time,\n            'multi_lora_percentage': multi_lora_percentage,\n            'sample_tasks': sample_tasks,\n            'score': potential_score,\n            'recommendation': self.generate_lora_recommendation(potential_score, sample_tasks, software_counts)\n        }\n    \n    def calculate_lora_potential_score(self, avg_time, success_rate, multi_lora_pct, task_count, software_counts):\n        \"\"\"Calculate a score indicating how beneficial a new LoRA would be\"\"\"\n        # Higher score means more beneficial\n        score = 0\n        \n        # Frequency is important - more frequent patterns justify LoRA investment\n        frequency_score = min(task_count / 500, 1.0) * 0.3\n        score += frequency_score\n        \n        # Tasks that currently require multiple LoRAs could benefit from consolidation\n        multi_lora_score = multi_lora_pct * 0.2\n        score += multi_lora_score\n        \n        # Tasks with lower success rates might benefit from specialized training\n        improvement_potential = (1 - success_rate) * 0.2\n        score += improvement_potential\n        \n        # Tasks that take longer to execute might see better optimization\n        time_score = min(avg_time / 10, 1.0) * 0.15\n        score += time_score\n        \n        # Tasks involving multiple software interactions might benefit from integration\n        software_diversity = min(len(software_counts) / 3, 1.0) * 0.15\n        score += software_diversity\n        \n        return score\n    \n    def generate_lora_recommendation(self, score, sample_tasks, software_counts):\n        \"\"\"Generate a recommendation for a potential new LoRA\"\"\"\n        if score < 0.5:\n            return \"Not recommended for LoRA training\"\n        \n        primary_software = max(software_counts.items(), key=lambda x: x[1])[0]\n        secondary_software = []\n        \n        for software, count in sorted(software_counts.items(), key=lambda x: x[1], reverse=True)[1:3]:\n            if count > len(sample_tasks) * 0.3:  # Software used in at least 30% of tasks\n                secondary_software.append(software)\n        \n        recommendation = f\"Recommended new LoRA for {primary_software}\"\n        if secondary_software:\n            recommendation += f\" with integration for {', '.join(secondary_software)}\"\n        \n        return recommendation\n                \n    def get_lora_recommendations(self, limit=3):\n        \"\"\"Get top recommendations for new LoRAs\"\"\"\n        sorted_clusters = sorted(\n            self.pattern_clusters.items(),\n            key=lambda x: x[1]['score'],\n            reverse=True\n        )\n        \n        return [cluster_data for _, cluster_data in sorted_clusters[:limit]]\n\n# Part 2: LoRA Efficacy Evaluation\nclass LoRAEfficacyEvaluator:\n    def __init__(self):\n        self.lora_performance = {}  # Track performance metrics per LoRA\n        \n    def record_lora_usage(self, lora_name, task, execution_metrics):\n        \"\"\"Record performance metrics when a LoRA is used\"\"\"\n        if lora_name not in self.lora_performance:\n            self.lora_performance[lora_name] = {\n                'task_count': 0,\n                'success_ratings': [],\n                'execution_times': [],\n                'token_usages': [],\n                'software_usages': {}\n            }\n        \n        perf = self.lora_performance[lora_name]\n        perf['task_count'] += 1\n        perf['success_ratings'].append(execution_metrics['success_rating'])\n        perf['execution_times'].append(execution_metrics['execution_time'])\n        perf['token_usages'].append(execution_metrics['tokens_generated'])\n        \n        # Track software this LoRA is used with\n        for software in task.software_used:\n            if software in perf['software_usages']:\n                perf['software_usages'][software] += 1\n            else:\n                perf['software_usages'][software] = 1\n    \n    def evaluate_lora_effectiveness(self, lora_name):\n        \"\"\"Evaluate how effective a LoRA is based on collected metrics\"\"\"\n        if lora_name not in self.lora_performance:\n            return None\n        \n        perf = self.lora_performance[lora_name]\n        \n        # Ignore LoRAs with too few data points\n        if perf['task_count'] < 30:\n            return {\n                'name': lora_name,\n                'status': 'Insufficient data',\n                'task_count': perf['task_count']\n            }\n        \n        avg_success = sum(perf['success_ratings']) / len(perf['success_ratings'])\n        avg_time = sum(perf['execution_times']) / len(perf['execution_times'])\n        avg_tokens = sum(perf['token_usages']) / len(perf['token_usages'])\n        \n        # Determine primary software this LoRA is used for\n        primary_software = max(perf['software_usages'].items(), key=lambda x: x[1])[0] if perf['software_usages'] else \"Unknown\"\n        \n        # Calculate effectiveness score (0-100)\n        effectiveness_score = (\n            avg_success * 50 +  # Success rating (0-1) scaled to 0-50\n            (1 - min(avg_time / 10, 1)) * 25 +  # Time efficiency (25 points)\n            (1 - min(avg_tokens / 1000, 1)) * 25  # Token efficiency (25 points)\n        )\n        \n        status = \"High value\"\n        if effectiveness_score < 50:\n            status = \"Consider replacing with RAG\"\n        elif effectiveness_score < 70:\n            status = \"Needs improvement\"\n        \n        return {\n            'name': lora_name,\n            'effectiveness_score': effectiveness_score,\n            'status': status,\n            'task_count': perf['task_count'],\n            'avg_success_rating': avg_success,\n            'avg_execution_time': avg_time,\n            'primary_software': primary_software\n        }\n    \n    def get_all_lora_evaluations(self):\n        \"\"\"Get evaluations for all tracked LoRAs\"\"\"\n        return {\n            lora_name: self.evaluate_lora_effectiveness(lora_name)\n            for lora_name in self.lora_performance\n        }\n    \n    def identify_redundant_loras(self):\n        \"\"\"Identify LoRAs that might be redundant or could be merged\"\"\"\n        evaluations = self.get_all_lora_evaluations()\n        \n        # Group LoRAs by primary software\n        software_groups = {}\n        for lora_name, eval_data in evaluations.items():\n            if eval_data is None or 'primary_software' not in eval_data:\n                continue\n                \n            software = eval_data['primary_software']\n            if software in software_groups:\n                software_groups[software].append((lora_name, eval_data))\n            else:\n                software_groups[software] = [(lora_name, eval_data)]\n        \n        # Identify potential merges where multiple low-task-count LoRAs serve same software\n        merge_recommendations = []\n        for software, loras in software_groups.items():\n            if len(loras) > 1:\n                low_usage_loras = [l for l in loras if l[1]['task_count'] < 100]\n                if len(low_usage_loras) > 1:\n                    merge_recommendations.append({\n                        'software': software,\n                        'loras_to_merge': [l[0] for l in low_usage_loras],\n                        'combined_task_count': sum(l[1]['task_count'] for l in low_usage_loras)\n                    })\n        \n        return merge_recommendations\n\n# Example usage\ndef example():\n    # Initialize the analyzer\n    base_llm = BaseLLM()  # Placeholder for your actual LLM\n    analyzer = TaskPatternAnalyzer(base_llm)\n    evaluator = LoRAEfficacyEvaluator()\n    \n    # After a period of operation, check for recommendations\n    lora_recommendations = analyzer.get_lora_recommendations()\n    print(\"Recommended new LoRAs:\")\n    for rec in lora_recommendations:\n        print(f\"- {rec['recommendation']} (Score: {rec['score']:.2f})\")\n    \n    # Evaluate existing LoRAs\n    lora_evaluations = evaluator.get_all_lora_evaluations()\n    print(\"\\nExisting LoRA evaluations:\")\n    for lora_name, eval_data in lora_evaluations.items():\n        print(f\"- {lora_name}: {eval_data['status']} (Score: {eval_data['effectiveness_score']:.2f})\")\n    \n    # Check for redundant LoRAs\n    merge_recommendations = evaluator.identify_redundant_loras()\n    print(\"\\nRecommended LoRA merges:\")\n    for rec in merge_recommendations:\n        print(f\"- Merge {', '.join(rec['loras_to_merge'])} for {rec['software']}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}