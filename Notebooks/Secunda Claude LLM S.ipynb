{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport asyncio\nfrom typing import Dict, List, Optional, Union, Any\nfrom datetime import datetime\nfrom collections import deque\nfrom enum import Enum\nfrom pydantic import BaseModel, Field\nimport logging\nimport hashlib\nfrom pathlib import Path\nimport ctransformers\nfrom fastapi import FastAPI, WebSocket, HTTPException, BackgroundTasks\nimport uvicorn\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(\"agent_system.log\"),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(\"agent_system\")\n\n# ---- Models and Enums ----\n\nclass AgentType(str, Enum):\n    PROJECT_MANAGER = \"project_manager\"\n    PROGRAMMER = \"programmer\"\n\nclass ExpertiseArea(str, Enum):\n    PYTHON = \"python\"\n    JAVASCRIPT = \"javascript\"\n    DOCKER = \"docker\"\n    KUBERNETES = \"kubernetes\"\n    DATABASE = \"database\"\n    NETWORKING = \"networking\"\n    WEB = \"web\"\n    TESTING = \"testing\"\n    DEVOPS = \"devops\"\n\nclass TaskStatus(str, Enum):\n    PENDING = \"pending\"\n    IN_PROGRESS = \"in_progress\"\n    COMPLETED = \"completed\"\n    BLOCKED = \"blocked\"\n    FAILED = \"failed\"\n\nclass MessageType(str, Enum):\n    TASK_ASSIGNMENT = \"task_assignment\"\n    STATUS_UPDATE = \"status_update\"\n    TECHNICAL_QUERY = \"technical_query\"\n    FEEDBACK = \"feedback\"\n    COMMAND_RESULT = \"command_result\"\n    LEARNING = \"learning\"\n\nclass Thought(BaseModel):\n    id: str\n    timestamp: datetime = Field(default_factory=datetime.now)\n    category: str\n    content: str\n    confidence: float = 0.5\n    references: List[str] = []\n    context_hash: str = \"\"  # Hash of the context that generated this thought\n    \n    def generate_id(self) -> str:\n        \"\"\"Generate a unique ID for the thought based on content and timestamp\"\"\"\n        return hashlib.md5(f\"{self.content}_{self.timestamp}\".encode()).hexdigest()\n\nclass CommandExecutionRecord(BaseModel):\n    command: str\n    stdout: str\n    stderr: str\n    syslog: Optional[str] = None\n    exit_code: int\n    execution_time: float\n    system_state_before: Dict[str, Any] = {}\n    system_state_after: Dict[str, Any] = {}\n    interaction_history: List[Dict[str, str]] = []\n    agent_thoughts: List[Thought] = []\n    success: bool = False\n    annotations: Dict[str, Any] = {}\n\nclass Task(BaseModel):\n    id: str\n    title: str\n    description: str\n    status: TaskStatus = TaskStatus.PENDING\n    assigned_to: Optional[str] = None\n    due_date: Optional[datetime] = None\n    required_expertise: List[ExpertiseArea] = []\n    dependencies: List[str] = []\n    priority: int = 1\n    estimated_hours: float = 1.0\n    actual_hours: float = 0.0\n    progress: float = 0.0\n    commands_executed: List[str] = []\n    command_records: List[CommandExecutionRecord] = []\n    subtasks: List[str] = []\n    created_at: datetime = Field(default_factory=datetime.now)\n    updated_at: datetime = Field(default_factory=datetime.now)\n\nclass Message(BaseModel):\n    id: str = Field(default_factory=lambda: hashlib.md5(str(datetime.now().timestamp()).encode()).hexdigest())\n    sender: str\n    receiver: str\n    message_type: MessageType\n    content: Dict[str, Any]\n    timestamp: datetime = Field(default_factory=datetime.now)\n    read: bool = False\n\nclass MemoryEntry(BaseModel):\n    timestamp: datetime = Field(default_factory=datetime.now)\n    category: str\n    content: Any\n    importance: float = 1.0\n    references: List[str] = []\n    last_accessed: datetime = Field(default_factory=datetime.now)\n    access_count: int = 0\n\nclass ThoughtMemory(BaseModel):\n    capacity: int = 1000\n    thoughts: List[Thought] = []\n    thought_clusters: Dict[str, List[str]] = {}  # category -> thought_ids\n    pattern_index: Dict[str, List[str]] = {}  # pattern -> thought_ids\n    important_thoughts: List[str] = []  # Thought IDs deemed important\n    recent_thoughts: deque = Field(default_factory=lambda: deque(maxlen=100))\n    \n    def add_thought(self, thought: Thought) -> None:\n        # Ensure thought has an ID\n        if not thought.id:\n            thought.id = thought.generate_id()\n            \n        # Add to main thoughts list\n        self.thoughts.append(thought)\n        \n        # Update indexes\n        category = thought.category\n        if category not in self.thought_clusters:\n            self.thought_clusters[category] = []\n        self.thought_clusters[category].append(thought.id)\n        \n        # Add to recent thoughts\n        self.recent_thoughts.append(thought.id)\n        \n        # If we're over capacity, remove least important thoughts\n        if len(self.thoughts) > self.capacity:\n            self._prune_thoughts()\n    \n    def retrieve_relevant_thoughts(self, context: str, categories: List[str] = None, limit: int = 5) -> List[Thought]:\n        # Basic implementation - would be enhanced with embeddings in a real system\n        relevant_thoughts = []\n        context_words = set(context.lower().split())\n        \n        for thought in self.thoughts:\n            if categories and thought.category not in categories:\n                continue\n                \n            thought_words = set(thought.content.lower().split())\n            overlap = len(context_words.intersection(thought_words))\n            if overlap > 3:  # Simple relevance threshold\n                relevant_thoughts.append((thought, overlap))\n        \n        # Sort by relevance and return top results\n        relevant_thoughts.sort(key=lambda x: x[1], reverse=True)\n        return [t[0] for t in relevant_thoughts[:limit]]\n    \n    def _prune_thoughts(self) -> None:\n        # Remove least important thoughts while preserving important ones\n        thoughts_with_scores = []\n        important_ids = set(self.important_thoughts)\n        recent_ids = set(self.recent_thoughts)\n        \n        for thought in self.thoughts:\n            # Calculate importance score based on confidence, recency, and marked importance\n            score = thought.confidence\n            if thought.id in important_ids:\n                score += 10\n            if thought.id in recent_ids:\n                score += 5\n                \n            thoughts_with_scores.append((thought, score))\n        \n        # Sort by score ascending (least important first)\n        thoughts_with_scores.sort(key=lambda x: x[1])\n        \n        # Remove enough to get back to capacity\n        excess = len(self.thoughts) - self.capacity\n        thoughts_to_remove = thoughts_with_scores[:excess]\n        \n        # Update data structures\n        for thought, _ in thoughts_to_remove:\n            self.thoughts.remove(thought)\n            if thought.id in self.thought_clusters.get(thought.category, []):\n                self.thought_clusters[thought.category].remove(thought.id)\n\nclass LearningMemory(BaseModel):\n    experiences: List[CommandExecutionRecord] = []\n    pattern_frequency: Dict[str, int] = {}\n    success_patterns: Dict[str, int] = {}\n    error_patterns: Dict[str, int] = {}\n    learning_metrics: Dict[str, float] = {}\n    max_size: int = 1000\n    \n    def add_record(self, record: CommandExecutionRecord) -> None:\n        \"\"\"Add a new command execution record and update patterns\"\"\"\n        self.experiences.append(record)\n        \n        # Extract patterns\n        patterns = self._extract_patterns(record)\n        \n        # Update pattern frequencies\n        for pattern in patterns:\n            self.pattern_frequency[pattern] = self.pattern_frequency.get(pattern, 0) + 1\n            \n            # Update success or error patterns\n            if record.success:\n                self.success_patterns[pattern] = self.success_patterns.get(pattern, 0) + 1\n            else:\n                self.error_patterns[pattern] = self.error_patterns.get(pattern, 0) + 1\n                \n        # Prune if needed\n        if len(self.experiences) > self.max_size:\n            self.experiences.pop(0)  # Remove oldest\n    \n    def _extract_patterns(self, record: CommandExecutionRecord) -> List[str]:\n        \"\"\"Extract patterns from command execution record\"\"\"\n        patterns = []\n        \n        # Command patterns (simplified)\n        cmd_parts = record.command.split()\n        if len(cmd_parts) > 0:\n            patterns.append(f\"CMD:{cmd_parts[0]}\")\n            \n        # Error patterns\n        if record.stderr and len(record.stderr) > 0:\n            error_lines = record.stderr.split('\\n')\n            for line in error_lines:\n                if \"error\" in line.lower() or \"exception\" in line.lower():\n                    patterns.append(f\"ERR:{line[:50]}\")  # First 50 chars of error\n        \n        # Exit code patterns\n        patterns.append(f\"EXIT:{record.exit_code}\")\n        \n        # More complex patterns would be added here in a full implementation\n        \n        return patterns\n    \n    def get_relevant_experiences(self, command: str, limit: int = 5) -> List[CommandExecutionRecord]:\n        \"\"\"Retrieve relevant past experiences for a command\"\"\"\n        cmd_parts = command.split()\n        main_command = cmd_parts[0] if cmd_parts else \"\"\n        \n        # Find records with the same command\n        similar_records = [\n            rec for rec in self.experiences \n            if rec.command.split()[0] == main_command\n        ]\n        \n        # Sort by recency (newer first)\n        similar_records.sort(key=lambda x: x.execution_time, reverse=True)\n        \n        return similar_records[:limit]\n\n# Base Agent Class\nclass Agent(BaseModel):\n    id: str\n    name: str\n    agent_type: AgentType\n    expertise: List[ExpertiseArea] = []\n    model_path: str\n    manual_paths: List[str] = []\n    memory: List[MemoryEntry] = []\n    thought_memory: ThoughtMemory = Field(default_factory=ThoughtMemory)\n    learning_memory: LearningMemory = Field(default_factory=LearningMemory)\n    inbox: List[Message] = []\n    outbox: List[Message] = []\n    max_memory_size: int = 1000\n    llm: Any = None\n    \n    class Config:\n        arbitrary_types_allowed = True\n    \n    def initialize(self):\n        \"\"\"Initialize the agent, including loading the LLM\"\"\"\n        self.llm = self._load_llm()\n        self._load_manuals()\n        logger.info(f\"Agent {self.name} ({self.id}) initialized with type {self.agent_type}\")\n    \n    def _load_llm(self):\n        \"\"\"Load the LLM using ctransformers\"\"\"\n        logger.info(f\"Loading LLM from {self.model_path}\")\n        try:\n            # For Phi-3 Mini in GGUF format\n            model = ctransformers.AutoModelForCausalLM.from_pretrained(\n                self.model_path,\n                model_type=\"phi\",\n                gpu_layers=0  # Set higher for GPU acceleration\n            )\n            return model\n        except Exception as e:\n            logger.error(f\"Error loading LLM: {e}\")\n            raise\n    \n    def _load_manuals(self):\n        \"\"\"Load agent manuals into memory\"\"\"\n        for manual_path in self.manual_paths:\n            try:\n                with open(manual_path, 'r') as f:\n                    content = f.read()\n                    \n                manual_name = Path(manual_path).stem\n                \n                # Store in memory\n                self.add_to_memory(\n                    category=\"manual\",\n                    content=content,\n                    importance=2.0,\n                    references=[manual_path]\n                )\n                logger.info(f\"Loaded manual: {manual_name}\")\n            except Exception as e:\n                logger.error(f\"Error loading manual {manual_path}: {e}\")\n    \n    def add_to_memory(self, category: str, content: Any, importance: float = 1.0, references: List[str] = []):\n        \"\"\"Add a new entry to agent memory\"\"\"\n        entry = MemoryEntry(\n            category=category,\n            content=content,\n            importance=importance,\n            references=references\n        )\n        \n        self.memory.append(entry)\n        \n        # Prune memory if needed\n        if len(self.memory) > self.max_memory_size:\n            self._prune_memory()\n    \n    def _prune_memory(self):\n        \"\"\"Remove least important memories to stay within size limit\"\"\"\n        # Sort by importance and last_accessed (keep important and recent memories)\n        self.memory.sort(key=lambda x: (x.importance, x.last_accessed.timestamp()))\n        \n        # Remove oldest, least important memories\n        excess = len(self.memory) - self.max_memory_size\n        self.memory = self.memory[excess:]\n    \n    def retrieve_from_memory(self, category: str, query: str = None, limit: int = 5):\n        \"\"\"Retrieve entries from memory matching category and optionally containing query\"\"\"\n        results = [entry for entry in self.memory if entry.category == category]\n        \n        if query:\n            # Very simple matching - would use embeddings in production\n            results = [\n                entry for entry in results \n                if query.lower() in str(entry.content).lower()\n            ]\n        \n        # Update access stats\n        for entry in results:\n            entry.last_accessed = datetime.now()\n            entry.access_count += 1\n        \n        # Sort by importance and return top results\n        results.sort(key=lambda x: x.importance, reverse=True)\n        return results[:limit]\n    \n    def receive_message(self, message: Message):\n        \"\"\"Receive a message into inbox\"\"\"\n        self.inbox.append(message)\n        logger.info(f\"Agent {self.name} received message from {message.sender}: {message.message_type}\")\n    \n    def send_message(self, to: str, message_type: MessageType, content: Dict[str, Any]):\n        \"\"\"Create and send a message\"\"\"\n        message = Message(\n            sender=self.id,\n            receiver=to,\n            message_type=message_type,\n            content=content\n        )\n        self.outbox.append(message)\n        logger.info(f\"Agent {self.name} sent message to {to}: {message_type}\")\n        return message\n    \n    def process_messages(self):\n        \"\"\"Process all messages in inbox\"\"\"\n        messages_to_process = self.inbox.copy()\n        self.inbox = []\n        \n        for message in messages_to_process:\n            self._process_message(message)\n            # Mark as read\n            message.read = True\n    \n    def _process_message(self, message: Message):\n        \"\"\"Process a single message - to be implemented by subclasses\"\"\"\n        pass\n    \n    def generate_thought(self, category: str, context: str, confidence: float = 0.5) -> Thought:\n        \"\"\"Generate an internal thought based on the current context\"\"\"\n        # Create context hash\n        context_hash = hashlib.md5(context.encode()).hexdigest()\n        \n        # Build prompt for the LLM to generate a thought\n        prompt = self._build_thought_prompt(category, context)\n        \n        # Generate thought content using the LLM\n        thought_content = self._generate_with_llm(prompt, max_tokens=100)\n        \n        # Create the thought\n        thought = Thought(\n            id=\"\",  # Will be generated on add\n            category=category,\n            content=thought_content,\n            confidence=confidence,\n            context_hash=context_hash\n        )\n        \n        # Add to thought memory\n        self.thought_memory.add_thought(thought)\n        \n        return thought\n    \n    def _build_thought_prompt(self, category: str, context: str) -> str:\n        \"\"\"Build a prompt for generating a thought of a specific category\"\"\"\n        # Retrieve relevant past thoughts for context\n        relevant_thoughts = self.thought_memory.retrieve_relevant_thoughts(\n            context=context, \n            categories=[category], \n            limit=3\n        )\n        \n        relevant_thought_text = \"\\n\".join([\n            f\"- {thought.content}\" for thought in relevant_thoughts\n        ])\n        \n        prompts = {\n            \"observation\": f\"\"\"\nBased on the following context, generate a concise observation about what you notice:\nCONTEXT:\n{context}\n\nPAST OBSERVATIONS:\n{relevant_thought_text}\n\nOBSERVATION:\n\"\"\",\n            \"analysis\": f\"\"\"\nAnalyze the following context and extract key insights:\nCONTEXT:\n{context}\n\nPAST ANALYSIS:\n{relevant_thought_text}\n\nANALYSIS:\n\"\"\",\n            \"decision\": f\"\"\"\nBased on the following context, what decision would you make:\nCONTEXT:\n{context}\n\nPAST DECISIONS:\n{relevant_thought_text}\n\nDECISION:\n\"\"\",\n            \"learning\": f\"\"\"\nWhat can be learned from the following context:\nCONTEXT:\n{context}\n\nPAST LEARNINGS:\n{relevant_thought_text}\n\nLEARNING:\n\"\"\"\n        }\n        \n        return prompts.get(category, f\"Generate a {category} thought about: {context}\")\n    \n    def _generate_with_llm(self, prompt: str, max_tokens: int = 512) -> str:\n        \"\"\"Generate text using the loaded LLM\"\"\"\n        try:\n            # Generate text with the model\n            response = self.llm(\n                prompt,\n                max_new_tokens=max_tokens,\n                temperature=0.7,\n                top_p=0.95,\n                repetition_penalty=1.1\n            )\n            \n            # Clean and return the response\n            return response.strip()\n        except Exception as e:\n            logger.error(f\"Error generating with LLM: {e}\")\n            return f\"Error generating thought: {str(e)}\"\n    \n    async def step(self):\n        \"\"\"Process one step of the agent's loop - to be implemented by subclasses\"\"\"\n        pass\n    \n    def summarize_context(self, context: str, max_length: int = 200) -> str:\n        \"\"\"Summarize a long context to keep it lightweight\"\"\"\n        if len(context) <= max_length:\n            return context\n            \n        prompt = f\"Summarize the following text in a concise way:\\n\\n{context[:1000]}...\"\n        summary = self._generate_with_llm(prompt, max_tokens=200)\n        \n        return summary\n\n# Project Manager Agent\nclass ProjectManagerAgent(Agent):\n    current_tasks: List[Task] = []\n    completed_tasks: List[Task] = []\n    team: Dict[str, str] = {}  # programmer_id -> expertise\n    \n    async def step(self):\n        \"\"\"Run one step of the project manager's loop\"\"\"\n        # Process incoming messages\n        self.process_messages()\n        \n        # Check for tasks that need assignment\n        await self._assign_pending_tasks()\n        \n        # Check for status updates needed\n        await self._check_task_statuses()\n        \n        # Generate learning insights and feedback\n        await self._generate_insights()\n    \n    def _process_message(self, message: Message):\n        \"\"\"Process a single message\"\"\"\n        if message.message_type == MessageType.STATUS_UPDATE:\n            self._handle_status_update(message)\n        elif message.message_type == MessageType.TECHNICAL_QUERY:\n            self._handle_technical_query(message)\n        elif message.message_type == MessageType.COMMAND_RESULT:\n            self._handle_command_result(message)\n        \n        # Generate thoughts about the message\n        context = f\"Message from {message.sender} of type {message.message_type}: {json.dumps(message.content)}\"\n        self.generate_thought(\"observation\", context)\n    \n    def _handle_status_update(self, message: Message):\n        \"\"\"Handle a status update from a programmer\"\"\"\n        task_id = message.content.get(\"task_id\")\n        new_status = message.content.get(\"status\")\n        progress = message.content.get(\"progress\", 0.0)\n        \n        # Update task status\n        for task in self.current_tasks:\n            if task.id == task_id:\n                old_status = task.status\n                task.status = TaskStatus(new_status)\n                task.progress = progress\n                task.updated_at = datetime.now()\n                \n                # Move to completed if done\n                if task.status == TaskStatus.COMPLETED:\n                    self.completed_tasks.append(task)\n                    self.current_tasks.remove(task)\n                \n                # Generate thought about the status change\n                context = f\"Task {task.title} status changed from {old_status} to {new_status} with progress {progress}\"\n                self.generate_thought(\"analysis\", context)\n                \n                # Send feedback\n                self._send_feedback(task)\n                break\n    \n    def _handle_technical_query(self, message: Message):\n        \"\"\"Handle a technical query from a programmer\"\"\"\n        problem = message.content.get(\"problem\", {})\n        task_id = message.content.get(\"task_id\")\n        \n        # Generate thought about the problem\n        context = f\"Technical query from {message.sender} regarding task {task_id}: {json.dumps(problem)}\"\n        analysis_thought = self.generate_thought(\"analysis\", context)\n        \n        # Determine if this needs to be escalated to architect\n        complexity = self._assess_problem_complexity(problem)\n        \n        if complexity > 0.7:  # High complexity\n            # Forward to architect\n            self.send_message(\n                to=\"architect\",  # Assuming architect has this ID\n                message_type=MessageType.TECHNICAL_QUERY,\n                content={\n                    \"original_sender\": message.sender,\n                    \"problem\": problem,\n                    \"task_id\": task_id,\n                    \"pm_analysis\": analysis_thought.content\n                }\n            )\n            \n            # Let programmer know it's been escalated\n            self.send_message(\n                to=message.sender,\n                message_type=MessageType.FEEDBACK,\n                content={\n                    \"type\": \"technical_query_response\",\n                    \"message\": \"Your query has been escalated to the architect for expert assistance.\",\n                    \"task_id\": task_id\n                }\n            )\n        else:\n            # Try to handle it with the PM's knowledge\n            solution = self._generate_solution(problem)\n            \n            self.send_message(\n                to=message.sender,\n                message_type=MessageType.FEEDBACK,\n                content={\n                    \"type\": \"technical_query_response\",\n                    \"message\": solution,\n                    \"task_id\": task_id\n                }\n            )\n    \n    def _handle_command_result(self, message: Message):\n        \"\"\"Handle command execution results\"\"\"\n        record = CommandExecutionRecord(**message.content.get(\"record\", {}))\n        task_id = message.content.get(\"task_id\")\n        \n        # Find the task\n        for task in self.current_tasks:\n            if task.id == task_id:\n                # Add to task records\n                task.commands_executed.append(record.command)\n                task.command_records.append(record)\n                break\n        \n        # Add to learning memory\n        self.learning_memory.add_record(record)\n        \n        # Generate thoughts about the command execution\n        context = f\"Command execution: {record.command}\\nSuccess: {record.success}\\nExit code: {record.exit_code}\"\n        self.generate_thought(\"observation\", context)\n        \n        if not record.success:\n            # Generate analysis thought for failed command\n            error_context = f\"Command failed: {record.command}\\nError: {record.stderr}\\nExit code: {record.exit_code}\"\n            self.generate_thought(\"analysis\", error_context)\n    \n    async def _assign_pending_tasks(self):\n        \"\"\"Assign pending tasks to programmers\"\"\"\n        # Find unassigned tasks\n        pending_tasks = [task for task in self.current_tasks if task.status == TaskStatus.PENDING and not task.assigned_to]\n        \n        for task in pending_tasks:\n            # Find best programmer based on expertise\n            best_programmer = self._find_best_programmer(task)\n            \n            if best_programmer:\n                # Assign task\n                task.assigned_to = best_programmer\n                task.status = TaskStatus.IN_PROGRESS\n                task.updated_at = datetime.now()\n                \n                # Send assignment message\n                self.send_message(\n                    to=best_programmer,\n                    message_type=MessageType.TASK_ASSIGNMENT,\n                    content={\n                        \"task\": task.dict()\n                    }\n                )\n                \n                # Generate thought about assignment\n                context = f\"Assigned task {task.title} to {best_programmer} based on expertise match with {task.required_expertise}\"\n                self.generate_thought(\"decision\", context)\n    \n    async def _check_task_statuses(self):\n        \"\"\"Check status of ongoing tasks\"\"\"\n        in_progress_tasks = [task for task in self.current_tasks if task.status == TaskStatus.IN_PROGRESS]\n        \n        for task in in_progress_tasks:\n            # Check if task is overdue\n            if task.due_date and datetime.now() > task.due_date:\n                # Send reminder\n                self.send_message(\n                    to=task.assigned_to,\n                    message_type=MessageType.FEEDBACK,\n                    content={\n                        \"type\": \"reminder\",\n                        \"message\": f\"Task '{task.title}' is overdue. Please provide an update.\",\n                        \"task_id\": task.id\n                    }\n                )\n    \n    async def _generate_insights(self):\n        \"\"\"Generate learning insights from completed tasks\"\"\"\n        if len(self.completed_tasks) > 0:\n            # Analyze patterns from completed tasks\n            successful_patterns = {}\n            for task in self.completed_tasks:\n                for record in task.command_records:\n                    if record.success:\n                        patterns = self.learning_memory._extract_patterns(record)\n                        for pattern in patterns:\n                            successful_patterns[pattern] = successful_patterns.get(pattern, 0) + 1\n            \n            # Generate insights\n            top_patterns = sorted(successful_patterns.items(), key=lambda x: x[1], reverse=True)[:5]\n            \n            if top_patterns:\n                patterns_text = \"\\n\".join([f\"- {pattern} (observed {count} times)\" for pattern, count in top_patterns])\n                context = f\"Successful patterns from completed tasks:\\n{patterns_text}\"\n                insight = self.generate_thought(\"learning\", context)\n                \n                # Share insights with programmers\n                for programmer_id in self.team:\n                    self.send_message(\n                        to=programmer_id,\n                        message_type=MessageType.LEARNING,\n                        content={\n                            \"insight\": insight.content,\n                            \"patterns\": dict(top_patterns)\n                        }\n                    )\n    \n    def _find_best_programmer(self, task: Task) -> Optional[str]:\n        \"\"\"Find the best programmer for a task based on expertise and current workload\"\"\"\n        matches = []\n        \n        for programmer_id, expertise_areas in self.team.items():\n            # Check if programmer's expertise matches task requirements\n            expertise_match = any(exp in expertise_areas for exp in task.required_expertise)\n            \n            if expertise_match:\n                # Count current tasks assigned to this programmer\n                workload = sum(1 for t in self.current_tasks if t.assigned_to == programmer_id)\n                matches.append((programmer_id, workload))\n        \n        if not matches:\n            return None\n            \n        # Sort by workload (ascending)\n        matches.sort(key=lambda x: x[1])\n        \n        return matches[0][0]\n    \n    def _send_feedback(self, task: Task):\n        \"\"\"Send feedback on a task to the assigned programmer\"\"\"\n        # Generate feedback based on task progress and quality\n        if task.status == TaskStatus.COMPLETED:\n            prompt = f\"\"\"\nTask '{task.title}' has been completed.\nDescription: {task.description}\nCommands executed: {', '.join(task.commands_executed[:5])}\n\nGenerate positive feedback and constructive improvement suggestions:\n\"\"\"\n        else:\n            prompt = f\"\"\"\nTask '{task.title}' is at {task.progress}% progress with status {task.status}.\nDescription: {task.description}\nCommands executed: {', '.join(task.commands_executed[:5])}\n\nGenerate constructive feedback on work so far:\n\"\"\"\n        \n        feedback = self._generate_with_llm(prompt)\n        \n        self.send_message(\n            to=task.assigned_to,\n            message_type=MessageType.FEEDBACK,\n            content={\n                \"task_id\": task.id,\n                \"feedback\": feedback,\n                \"progress_assessment\": task.progress\n            }\n        )\n    \n    def _assess_problem_complexity(self, problem: Dict) -> float:\n        \"\"\"Assess the complexity of a technical problem (0-1 scale)\"\"\"\n        # This would be more sophisticated in a real implementation\n        description = problem.get(\"description\", \"\")\n        context = problem.get(\"context\", {})\n        \n        # Count indicators of complexity\n        complexity_indicators = [\n            \"complex\", \"challenging\", \"difficult\", \"unknown\", \"error\", \n            \"exception\", \"failing\", \"segmentation fault\", \"core dump\"\n        ]\n        \n        count = sum(1 for word in complexity_indicators if word in description.lower())\n        \n        # Normalize to 0-1\n        return min(1.0, count / 5)\n    \n    def _generate_solution(self, problem: Dict) -> str:\n        \"\"\"Generate a solution to a technical problem\"\"\"\n        description = problem.get(\"description\", \"\")\n        context = problem.get(\"context\", {})\n        \n        # Retrieve relevant memories\n        relevant_memories = self.retrieve_from_memory(\"command_record\", description, limit=3)\n        \n        # Format memory context\n        memory_context = \"\"\n        for memory in relevant_memories:\n            cmd_record = memory.content\n            memory_context += f\"Command: {cmd_record.get('command')}\\n\"\n            memory_context += f\"Result: {cmd_record.get('success', False)}\\n\"\n            if 'stderr' in cmd_record and cmd_record['stderr']:\n                memory_context += f\"Error: {cmd_record.get('stderr')}\\n\"\n            memory_context += \"\\n\"\n        \n        prompt = f\"\"\"\nTechnical problem: {description}\nContext: {json.dumps(context)}\n\nRelevant past experiences:\n{memory_context}\n\nGenerate a solution or advice for this problem:\n\"\"\"\n        \n        return self._generate_with_llm(prompt, max_tokens=300)\n\n# Programmer Agent\nclass ProgrammerAgent(Agent):\n    current_tasks: List[Task] = []\n    completed_tasks:","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}